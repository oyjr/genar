"""
Stage 1å¤šå°ºåº¦åŸºå› VQVAEæµ‹è¯•è„šæœ¬

æµ‹è¯•ä»¥ä¸‹åŠŸèƒ½ï¼š
1. MultiScaleGeneVQVAEæ¨¡å‹å®Œæ•´æ€§
2. ç¼–ç è§£ç æµç¨‹æ­£ç¡®æ€§
3. æŸå¤±è®¡ç®—å‡†ç¡®æ€§
4. checkpointä¿å­˜å’ŒåŠ è½½
5. Stage1Trainerè®­ç»ƒæµç¨‹
6. ä¸å…±äº«ç»„ä»¶çš„é›†æˆ

éªŒè¯Stage 1çš„å‡†å¤‡æƒ…å†µ
"""

import sys
import os
sys.path.append('src')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from model.VAR.multi_scale_gene_vqvae import MultiScaleGeneVQVAE, Stage1Trainer


def test_multi_scale_gene_vqvae():
    """æµ‹è¯•å¤šå°ºåº¦åŸºå› VQVAEæ¨¡å‹"""
    print("ğŸ§¬ æµ‹è¯• MultiScaleGeneVQVAE...")
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = MultiScaleGeneVQVAE(
        vocab_size=4096,
        embed_dim=128,
        beta=0.25,
        hierarchical_loss_weight=0.1,
        vq_loss_weight=0.25
    )
    
    print(f"   æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    B = 8
    gene_expression = torch.randn(B, 200)
    
    print(f"   è¾“å…¥åŸºå› è¡¨è¾¾: {gene_expression.shape}")
    
    # 1. æµ‹è¯•ç¼–ç 
    encode_result = model.encode(gene_expression)
    print(f"   ç¼–ç ç»“æœkeys: {list(encode_result.keys())}")
    
    # éªŒè¯tokenså½¢çŠ¶
    expected_token_shapes = {
        'global': (B, 1),
        'pathway': (B, 8),
        'module': (B, 32),
        'individual': (B, 200)
    }
    
    for scale, expected_shape in expected_token_shapes.items():
        actual_shape = encode_result['tokens'][scale].shape
        assert actual_shape == expected_shape, f"{scale} tokenså½¢çŠ¶é”™è¯¯: {actual_shape} != {expected_shape}"
        print(f"   âœ… {scale} tokens: {actual_shape}")
    
    # 2. æµ‹è¯•è§£ç 
    decode_result = model.decode(encode_result['quantized'])
    print(f"   è§£ç ç»“æœkeys: {list(decode_result.keys())}")
    
    final_recon = decode_result['final_reconstruction']
    assert final_recon.shape == gene_expression.shape, f"é‡å»ºå½¢çŠ¶é”™è¯¯: {final_recon.shape} != {gene_expression.shape}"
    print(f"   âœ… æœ€ç»ˆé‡å»º: {final_recon.shape}")
    
    # 3. æµ‹è¯•ä»tokensè§£ç 
    tokens_decode_result = model.decode_from_tokens(encode_result['tokens'])
    assert tokens_decode_result['final_reconstruction'].shape == gene_expression.shape
    print(f"   âœ… ä»tokensè§£ç : {tokens_decode_result['final_reconstruction'].shape}")
    
    # 4. æµ‹è¯•å®Œæ•´å‰å‘ä¼ æ’­
    forward_result = model(gene_expression)
    print(f"   å‰å‘ä¼ æ’­ç»“æœkeys: {list(forward_result.keys())}")
    
    # éªŒè¯æŸå¤±å­˜åœ¨
    required_losses = ['total_loss', 'total_reconstruction_loss', 'total_hierarchical_loss', 'total_vq_loss']
    for loss_name in required_losses:
        assert loss_name in forward_result, f"ç¼ºå°‘æŸå¤±: {loss_name}"
        loss_value = forward_result[loss_name]
        assert isinstance(loss_value, torch.Tensor) and loss_value.dim() == 0, f"{loss_name}åº”è¯¥æ˜¯æ ‡é‡"
        print(f"   âœ… {loss_name}: {loss_value.item():.4f}")
    
    # 5. æµ‹è¯•codebookåˆ©ç”¨ç‡
    utilization = model.update_codebook_usage(encode_result['tokens'])
    print(f"   âœ… Codebookåˆ©ç”¨ç‡: {utilization:.4f}")
    
    # 6. æµ‹è¯•éšæœºtokenç”Ÿæˆ
    random_tokens = model.generate_random_tokens(batch_size=4, device=torch.device('cpu'))
    for scale, expected_shape in [('global', (4, 1)), ('pathway', (4, 8)), ('module', (4, 32)), ('individual', (4, 200))]:
        assert random_tokens[scale].shape == expected_shape, f"éšæœº{scale} tokenså½¢çŠ¶é”™è¯¯"
        assert random_tokens[scale].min() >= 0 and random_tokens[scale].max() < 4096, f"éšæœº{scale} tokensèŒƒå›´é”™è¯¯"
    print(f"   âœ… éšæœºtokensç”Ÿæˆ: {[f'{k}:{v.shape}' for k, v in random_tokens.items()]}")
    
    print("   âœ… MultiScaleGeneVQVAEæµ‹è¯•é€šè¿‡ï¼")
    return model


def test_loss_computation():
    """æµ‹è¯•æŸå¤±è®¡ç®—çš„æ­£ç¡®æ€§"""
    print("\nğŸ§¬ æµ‹è¯•æŸå¤±è®¡ç®—...")
    
    model = MultiScaleGeneVQVAE()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    B = 4
    gene_expression = torch.randn(B, 200)
    
    # å‰å‘ä¼ æ’­
    result = model(gene_expression)
    
    # éªŒè¯æŸå¤±å…³ç³»
    total_loss = result['total_loss']
    recon_loss = result['total_reconstruction_loss']
    hier_loss = result['total_hierarchical_loss']
    vq_loss = result['total_vq_loss']
    
    # æ‰‹åŠ¨è®¡ç®—æœŸæœ›çš„æ€»æŸå¤±
    expected_total = recon_loss + 0.1 * hier_loss + 0.25 * vq_loss
    
    # éªŒè¯æŸå¤±è®¡ç®—æ­£ç¡®æ€§
    torch.testing.assert_close(total_loss, expected_total, rtol=1e-5, atol=1e-6)
    print(f"   âœ… æŸå¤±è®¡ç®—æ­£ç¡®: {total_loss.item():.4f} == {expected_total.item():.4f}")
    
    # éªŒè¯é‡å»ºæŸå¤±åˆç†æ€§
    manual_recon_loss = torch.nn.functional.mse_loss(result['final_reconstruction'], gene_expression)
    torch.testing.assert_close(recon_loss, manual_recon_loss, rtol=1e-5, atol=1e-6)
    print(f"   âœ… é‡å»ºæŸå¤±æ­£ç¡®: {recon_loss.item():.4f}")
    
    # éªŒè¯åˆ†å±‚æŸå¤±
    decomposed = result['decomposed']
    decoded = result['decoded']
    manual_hier_losses = []
    
    for scale in ['global', 'pathway', 'module', 'individual']:
        scale_loss = torch.nn.functional.mse_loss(decoded[scale], decomposed[scale])
        manual_hier_losses.append(scale_loss)
        
        # æ£€æŸ¥individual scale lossæ˜¯å¦å­˜åœ¨
        assert f'{scale}_recon_loss' in result, f"ç¼ºå°‘{scale}_recon_loss"
        torch.testing.assert_close(result[f'{scale}_recon_loss'], scale_loss, rtol=1e-5, atol=1e-6)
    
    manual_total_hier = sum(manual_hier_losses)
    torch.testing.assert_close(hier_loss, manual_total_hier, rtol=1e-5, atol=1e-6)
    print(f"   âœ… åˆ†å±‚æŸå¤±æ­£ç¡®: {hier_loss.item():.4f}")
    
    print("   âœ… æŸå¤±è®¡ç®—æµ‹è¯•é€šè¿‡ï¼")


def test_checkpoint_save_load():
    """æµ‹è¯•checkpointä¿å­˜å’ŒåŠ è½½"""
    print("\nğŸ§¬ æµ‹è¯•Checkpointä¿å­˜å’ŒåŠ è½½...")
    
    # åˆ›å»ºåŸå§‹æ¨¡å‹
    original_model = MultiScaleGeneVQVAE(vocab_size=512, embed_dim=64)  # ä½¿ç”¨è¾ƒå°å‚æ•°ä¾¿äºæµ‹è¯•
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®å¹¶è®­ç»ƒå‡ æ­¥
    gene_expression = torch.randn(4, 200)
    optimizer = optim.Adam(original_model.parameters(), lr=1e-3)
    
    # è®­ç»ƒå‡ æ­¥
    for step in range(3):
        optimizer.zero_grad()
        result = original_model(gene_expression)
        loss = result['total_loss']
        loss.backward()
        optimizer.step()
    
    # ä¿å­˜checkpoint
    checkpoint_path = "test_stage1_checkpoint.pth"
    original_model.save_stage1_checkpoint(
        path=checkpoint_path,
        epoch=10,
        optimizer_state=optimizer.state_dict()
    )
    
    # åŠ è½½checkpoint
    loaded_model, checkpoint_info = MultiScaleGeneVQVAE.load_stage1_checkpoint(
        path=checkpoint_path,
        device=torch.device('cpu')
    )
    
    # éªŒè¯åŠ è½½çš„æ¨¡å‹
    assert checkpoint_info['epoch'] == 10, f"epochåŠ è½½é”™è¯¯: {checkpoint_info['epoch']} != 10"
    assert checkpoint_info['stage'] == 'stage1_vqvae', f"stageä¿¡æ¯é”™è¯¯: {checkpoint_info['stage']}"
    assert checkpoint_info['optimizer_state_dict'] is not None, "ä¼˜åŒ–å™¨çŠ¶æ€æœªä¿å­˜"
    
    # éªŒè¯æ¨¡å‹å‚æ•°ä¸€è‡´æ€§
    original_result = original_model(gene_expression)
    loaded_result = loaded_model(gene_expression)
    
    torch.testing.assert_close(
        original_result['final_reconstruction'], 
        loaded_result['final_reconstruction'], 
        rtol=1e-6, atol=1e-7
    )
    
    print(f"   âœ… Checkpointä¿å­˜å’ŒåŠ è½½æ­£ç¡®")
    print(f"   âœ… æ¨¡å‹é…ç½®: {checkpoint_info}")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    if os.path.exists(checkpoint_path):
        os.remove(checkpoint_path)
    
    print("   âœ… Checkpointæµ‹è¯•é€šè¿‡ï¼")
    return loaded_model


def test_stage1_trainer():
    """æµ‹è¯•Stage1è®­ç»ƒå™¨"""
    print("\nğŸ§¬ æµ‹è¯•Stage1Trainer...")
    
    # åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
    model = MultiScaleGeneVQVAE(vocab_size=256, embed_dim=64)  # å°æ¨¡å‹ä¾¿äºæµ‹è¯•
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    device = torch.device('cpu')
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Stage1Trainer(
        model=model,
        optimizer=optimizer,
        device=device,
        print_freq=2
    )
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    num_samples = 32
    gene_expressions = torch.randn(num_samples, 200)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataset = TensorDataset(gene_expressions)
    train_loader = DataLoader(dataset, batch_size=8, shuffle=True)
    val_loader = DataLoader(dataset, batch_size=8, shuffle=False)
    
    print(f"   è®­ç»ƒæ•°æ®: {num_samples}ä¸ªæ ·æœ¬, æ‰¹æ¬¡å¤§å°: 8")
    
    # æµ‹è¯•è®­ç»ƒä¸€ä¸ªepoch
    print(f"   å¼€å§‹è®­ç»ƒepoch...")
    train_losses = trainer.train_epoch(train_loader, epoch=1)
    
    # éªŒè¯è®­ç»ƒæŸå¤±
    expected_keys = ['total_loss', 'reconstruction_loss', 'hierarchical_loss', 'vq_loss', 'codebook_utilization']
    for key in expected_keys:
        assert key in train_losses, f"ç¼ºå°‘è®­ç»ƒæŸå¤±: {key}"
        print(f"   âœ… è®­ç»ƒ{key}: {train_losses[key]:.4f}")
    
    # æµ‹è¯•éªŒè¯ä¸€ä¸ªepoch
    print(f"   å¼€å§‹éªŒè¯epoch...")
    val_losses = trainer.validate_epoch(val_loader, epoch=1)
    
    # éªŒè¯éªŒè¯æŸå¤±
    for key in expected_keys:
        assert key in val_losses, f"ç¼ºå°‘éªŒè¯æŸå¤±: {key}"
        print(f"   âœ… éªŒè¯{key}: {val_losses[key]:.4f}")
    
    # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
    stats = trainer.get_training_stats()
    assert 'epoch_losses' in stats, "ç¼ºå°‘epochæŸå¤±ç»Ÿè®¡"
    assert 'codebook_utilizations' in stats, "ç¼ºå°‘codebookåˆ©ç”¨ç‡ç»Ÿè®¡"
    assert stats['num_epochs_trained'] == 1, f"è®­ç»ƒepochæ•°é”™è¯¯: {stats['num_epochs_trained']} != 1"
    
    print(f"   âœ… è®­ç»ƒç»Ÿè®¡: {stats['num_epochs_trained']}ä¸ªepochå·²å®Œæˆ")
    print(f"   âœ… Codebookåˆ©ç”¨ç‡å†å²: {stats['codebook_utilizations']}")
    
    print("   âœ… Stage1Traineræµ‹è¯•é€šè¿‡ï¼")
    return trainer


def test_integration_with_shared_components():
    """æµ‹è¯•ä¸å…±äº«ç»„ä»¶çš„é›†æˆ"""
    print("\nğŸ§¬ æµ‹è¯•ä¸å…±äº«ç»„ä»¶é›†æˆ...")
    
    model = MultiScaleGeneVQVAE()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    B = 6
    gene_expression = torch.randn(B, 200)
    
    # æµ‹è¯•å®Œæ•´pipeline
    result = model(gene_expression)
    
    # éªŒè¯æ‰€æœ‰ç»„ä»¶éƒ½æ­£å¸¸å·¥ä½œ
    assert 'decomposed' in result, "å¤šå°ºåº¦åˆ†è§£å¤±è´¥"
    assert 'encoded' in result, "å¤šå°ºåº¦ç¼–ç å¤±è´¥"  
    assert 'tokens' in result, "å…±äº«é‡åŒ–å¤±è´¥"
    assert 'quantized' in result, "é‡åŒ–ç‰¹å¾ç¼ºå¤±"
    assert 'decoded' in result, "å¤šå°ºåº¦è§£ç å¤±è´¥"
    assert 'final_reconstruction' in result, "æ®‹å·®é‡å»ºå¤±è´¥"
    
    # éªŒè¯æ•°æ®æµä¸€è‡´æ€§
    decomposed = result['decomposed']
    tokens = result['tokens']
    final_recon = result['final_reconstruction']
    
    # éªŒè¯æ¯ä¸ªå°ºåº¦çš„å¤„ç†
    for scale in ['global', 'pathway', 'module', 'individual']:
        assert scale in decomposed, f"ç¼ºå°‘{scale}åˆ†è§£"
        assert scale in tokens, f"ç¼ºå°‘{scale}tokens"
        
        # éªŒè¯tokensèŒƒå›´
        scale_tokens = tokens[scale]
        assert scale_tokens.min() >= 0, f"{scale} tokensæœ€å°å€¼é”™è¯¯"
        assert scale_tokens.max() < 4096, f"{scale} tokensæœ€å¤§å€¼é”™è¯¯"
    
    # éªŒè¯é‡å»ºè´¨é‡
    reconstruction_error = torch.nn.functional.mse_loss(final_recon, gene_expression)
    print(f"   âœ… é‡å»ºè¯¯å·®: {reconstruction_error.item():.4f}")
    
    # éªŒè¯tokenæ€»æ•°
    total_tokens = sum(tokens[scale].numel() for scale in tokens.keys())
    expected_total = B * (1 + 8 + 32 + 200)  # B * (1+8+32+200) = B * 241
    assert total_tokens == expected_total, f"Tokenæ€»æ•°é”™è¯¯: {total_tokens} != {expected_total}"
    print(f"   âœ… Tokenæ€»æ•°: {total_tokens} (æœŸæœ›: {expected_total})")
    
    # éªŒè¯æ®‹å·®é‡å»ºçš„ç´¯ç§¯æ€§è´¨
    recon_result = result['reconstruction_result']
    cumulative = recon_result['cumulative_without_individual']
    individual = recon_result['individual_contribution']
    final = recon_result['final_reconstruction']
    
    expected_final = cumulative + individual
    torch.testing.assert_close(final, expected_final, rtol=1e-5, atol=1e-6)
    print(f"   âœ… æ®‹å·®é‡å»ºé€»è¾‘æ­£ç¡®")
    
    print("   âœ… ä¸å…±äº«ç»„ä»¶é›†æˆæµ‹è¯•é€šè¿‡ï¼")


def test_different_batch_sizes():
    """æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°çš„å…¼å®¹æ€§"""
    print("\nğŸ§¬ æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°...")
    
    model = MultiScaleGeneVQVAE()
    
    test_batch_sizes = [1, 4, 16, 32]
    
    for batch_size in test_batch_sizes:
        gene_expression = torch.randn(batch_size, 200)
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        result = model(gene_expression)
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        assert result['final_reconstruction'].shape == (batch_size, 200), f"æ‰¹æ¬¡{batch_size}é‡å»ºå½¢çŠ¶é”™è¯¯"
        
        # éªŒè¯tokenså½¢çŠ¶
        expected_token_shapes = {
            'global': (batch_size, 1),
            'pathway': (batch_size, 8),
            'module': (batch_size, 32),
            'individual': (batch_size, 200)
        }
        
        for scale, expected_shape in expected_token_shapes.items():
            actual_shape = result['tokens'][scale].shape
            assert actual_shape == expected_shape, f"æ‰¹æ¬¡{batch_size} {scale} tokenså½¢çŠ¶é”™è¯¯"
        
        print(f"   âœ… æ‰¹æ¬¡å¤§å° {batch_size}: æ‰€æœ‰æµ‹è¯•é€šè¿‡")
    
    print("   âœ… ä¸åŒæ‰¹æ¬¡å¤§å°æµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æµ‹è¯•Stage 1å¤šå°ºåº¦åŸºå› VQVAE...")
    
    try:
        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        model = test_multi_scale_gene_vqvae()
        test_loss_computation()
        loaded_model = test_checkpoint_save_load()
        trainer = test_stage1_trainer()
        test_integration_with_shared_components()
        test_different_batch_sizes()
        
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print(f"ğŸ“Š Stage 1 VQVAEæµ‹è¯•æ‘˜è¦:")
        print(f"   - æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
        print(f"   - è¯æ±‡è¡¨å¤§å°: {model.vocab_size}")
        print(f"   - åµŒå…¥ç»´åº¦: {model.embed_dim}")
        print(f"   - æ”¯æŒçš„æ‰¹æ¬¡å¤§å°: 1, 4, 16, 32+")
        print(f"   - è®­ç»ƒå™¨åŠŸèƒ½: âœ… æ­£å¸¸")
        print(f"   - CheckpointåŠŸèƒ½: âœ… æ­£å¸¸")
        
        print(f"\nâœ… Step 2 å¤šå°ºåº¦åŸºå› VQVAEåˆ›å»ºå®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1) 