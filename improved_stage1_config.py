#!/usr/bin/env python3
"""
æ”¹è¿›çš„Stage 1 VQVAEè®­ç»ƒé…ç½®

è§£å†³Codebook Collapseé—®é¢˜çš„å®Œæ•´æ–¹æ¡ˆï¼š
1. ä¼˜åŒ–è®­ç»ƒå‚æ•°
2. æ”¹è¿›é‡åŒ–ç­–ç•¥  
3. å¢å¼ºæ¶æ„è®¾è®¡
4. æ·»åŠ æ­£åˆ™åŒ–æŠ€æœ¯
"""

# ========================================
# æ–¹æ¡ˆAï¼šè®­ç»ƒå‚æ•°ä¼˜åŒ–
# ========================================

IMPROVED_TRAINING_CONFIG = {
    # åŸºç¡€è®­ç»ƒå‚æ•°
    'batch_size': 128,  # ğŸ”§ å‡å°batch sizeï¼Œå¢åŠ æ¢¯åº¦æ›´æ–°é¢‘ç‡
    'epochs': 200,
    'learning_rate': 3e-4,  # ğŸ”§ æé«˜å­¦ä¹ ç‡ï¼Œå¸®åŠ©codebookå¿«é€Ÿåˆ†åŒ–
    'weight_decay': 1e-5,
    
    # å­¦ä¹ ç‡è°ƒåº¦
    'lr_scheduler': {
        'type': 'cosine_with_warmup',
        'warmup_epochs': 20,  # ğŸ”§ é•¿é¢„çƒ­æœŸ
        'min_lr': 1e-5
    },
    
    # æ¢¯åº¦ç›¸å…³
    'gradient_clip_val': 1.0,  # ğŸ”§ æ¢¯åº¦è£å‰ªé˜²æ­¢è®­ç»ƒä¸ç¨³å®š
    'accumulate_grad_batches': 2,  # ğŸ”§ æ¢¯åº¦ç´¯ç§¯
}

IMPROVED_VQVAE_CONFIG = {
    'vocab_size': 4096,
    'embed_dim': 128,
    
    # ğŸ”§ å…³é”®æ”¹è¿›ï¼šcommitment lossæƒé‡
    'beta': 1.0,  # ğŸ”§ ä»0.25å¢åŠ åˆ°1.0ï¼Œå¼ºåŒ–commitment
    
    # ğŸ”§ å±‚çº§æŸå¤±æƒé‡è°ƒæ•´
    'hierarchical_loss_weight': 0.2,  # ä»0.1å¢åŠ åˆ°0.2
    'vq_loss_weight': 0.5,  # ä»0.25å¢åŠ åˆ°0.5
    
    # ğŸ”§ æ–°å¢ï¼šCodebookåˆ©ç”¨ç‡æ­£åˆ™åŒ–
    'utilization_loss_weight': 0.01,
    'target_utilization': 0.1,  # ç›®æ ‡10%åˆ©ç”¨ç‡
    
    # ğŸ”§ æ–°å¢ï¼šEMAæ›´æ–°ç­–ç•¥
    'use_ema_update': True,
    'ema_decay': 0.99,
    'ema_epsilon': 1e-5,
    
    # ğŸ”§ æ–°å¢ï¼šé‡å¯ä¸æ´»è·ƒcodes
    'restart_unused_codes': True,
    'restart_threshold': 1.0,  # epochçº§åˆ«çš„é‡å¯é˜ˆå€¼
    'restart_interval': 10,  # æ¯10ä¸ªepochæ£€æŸ¥ä¸€æ¬¡
}

# ========================================
# æ–¹æ¡ˆBï¼šæ”¹è¿›çš„VQVAEæ¶æ„
# ========================================

IMPROVED_ARCHITECTURE_CONFIG = {
    # ğŸ”§ æ›´æ·±çš„ç¼–ç å™¨
    'encoder_config': {
        'use_residual_blocks': True,
        'num_residual_blocks': 3,
        'hidden_dims': [256, 512, 256, 128],  # æ›´æ·±çš„ç½‘ç»œ
        'activation': 'gelu',
        'dropout': 0.1,
        'layer_norm': True
    },
    
    # ğŸ”§ æ›´æ·±çš„è§£ç å™¨
    'decoder_config': {
        'use_residual_blocks': True,
        'num_residual_blocks': 3,
        'hidden_dims': [128, 256, 512, 256],  # å¯¹ç§°è®¾è®¡
        'activation': 'gelu',
        'dropout': 0.1,
        'layer_norm': True
    },
    
    # ğŸ”§ æ”¹è¿›çš„é‡åŒ–ç­–ç•¥
    'quantizer_config': {
        'distance_metric': 'euclidean',  # æ¬§å‡ é‡Œå¾—è·ç¦»
        'normalize_embeddings': True,  # L2æ ‡å‡†åŒ–
        'temperature': 1.0,  # æ¸©åº¦å‚æ•°
        'use_cosine_similarity': False,  # ä¸ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
    }
}

# ========================================  
# æ–¹æ¡ˆCï¼šæ›¿ä»£é‡åŒ–æ–¹æ³• (FSQ)
# ========================================

FSQ_CONFIG = {
    'use_fsq': True,  # ğŸ”§ ä½¿ç”¨Finite Scalar Quantization
    'fsq_levels': [8, 5, 5, 5],  # å¯¹åº”1000ä¸ªcodesï¼Œæ¥è¿‘1024
    'fsq_dim': 4,  # FSQç»´åº¦
    'need_project': True,  # å¯ç”¨ç»´åº¦æŠ•å½±
    'project_dim': 128,  # æŠ•å½±åˆ°128ç»´
}

# ========================================
# æ–¹æ¡ˆDï¼šGroup/Product Quantization  
# ========================================

GROUP_QUANTIZATION_CONFIG = {
    'use_group_quantization': True,
    'num_groups': 4,  # ğŸ”§ å°†128ç»´åˆ†æˆ4ç»„ï¼Œæ¯ç»„32ç»´
    'group_vocab_size': 256,  # æ¯ç»„è¯æ±‡è¡¨å¤§å°256
    'total_combinations': 256**4,  # æ€»å…±4B+ç»„åˆ
}

# ========================================
# æ–¹æ¡ˆEï¼šResidual Vector Quantization
# ========================================

RVQ_CONFIG = {
    'use_rvq': True,
    'num_quantizers': 4,  # ğŸ”§ 4å±‚æ®‹å·®é‡åŒ–
    'vocab_size_per_layer': 1024,  # æ¯å±‚1024è¯æ±‡
    'shared_codebook': False,  # æ¯å±‚ç‹¬ç«‹codebook
}

# ========================================
# æ•°æ®é¢„å¤„ç†æ”¹è¿›
# ========================================

DATA_PREPROCESSING_CONFIG = {
    # ğŸ”§ å‡å°‘è¿‡åº¦æ ‡å‡†åŒ–
    'gene_normalization': {
        'method': 'log1p_only',  # åªåšlog1pï¼Œä¸åšz-score
        'clip_outliers': True,
        'clip_percentile': [1, 99],  # è£å‰ª1%å’Œ99%åˆ†ä½æ•°
    },
    
    # ğŸ”§ æ•°æ®å¢å¼º
    'augmentation': {
        'add_noise': True,
        'noise_std': 0.05,  # å°å¹…å™ªå£°å¢å¼ºå¤šæ ·æ€§
        'random_scale': True,
        'scale_range': [0.95, 1.05]
    }
}

# ========================================
# è®­ç»ƒç­–ç•¥
# ========================================

TRAINING_STRATEGY_CONFIG = {
    # ğŸ”§ åˆ†é˜¶æ®µè®­ç»ƒ
    'staged_training': {
        'stage1_epochs': 50,   # å…ˆè®­ç»ƒé‡å»º
        'stage1_vq_weight': 0.1,  # åˆæœŸVQæƒé‡è¾ƒå°
        
        'stage2_epochs': 100,  # å¢å¼ºVQè®­ç»ƒ  
        'stage2_vq_weight': 0.5,  # åæœŸVQæƒé‡è¾ƒå¤§
        
        'stage3_epochs': 50,   # ç²¾è°ƒé˜¶æ®µ
        'stage3_vq_weight': 1.0,  # æœ€å¤§VQæƒé‡
    },
    
    # ğŸ”§ è‡ªé€‚åº”æƒé‡è°ƒæ•´
    'adaptive_weights': {
        'monitor_utilization': True,
        'target_utilization': 0.1,
        'adjust_beta_dynamically': True,
        'beta_schedule': 'exponential'  # æŒ‡æ•°å¢é•¿
    },
    
    # ğŸ”§ å®šæœŸcodebooké‡å¯
    'codebook_maintenance': {
        'reset_unused_codes': True,
        'reset_interval': 1000,  # æ¯1000æ­¥
        'usage_threshold': 10,   # ä½¿ç”¨æ¬¡æ•°é˜ˆå€¼
        'reset_method': 'kmeans'  # ä½¿ç”¨k-meansé‡æ–°åˆå§‹åŒ–
    }
}

# ========================================
# ç›‘æ§å’Œè¯Šæ–­
# ========================================

MONITORING_CONFIG = {
    'log_interval': 100,
    'validate_interval': 500,
    
    # ğŸ”§ codebookç›‘æ§
    'codebook_metrics': {
        'track_utilization': True,
        'track_entropy': True,
        'track_dead_codes': True,
        'save_usage_heatmap': True
    },
    
    # ğŸ”§ é‡å»ºè´¨é‡ç›‘æ§
    'reconstruction_metrics': {
        'mse': True,
        'mae': True, 
        'pcc': True,  # Pearsonç›¸å…³ç³»æ•°
        'r2': True    # Ræ–¹
    }
}

# ========================================
# å®Œæ•´é…ç½®ç»„åˆ
# ========================================

def get_improved_config(method='enhanced_vqvae'):
    """
    è·å–æ”¹è¿›é…ç½®
    
    Args:
        method: æ”¹è¿›æ–¹æ³•
            - 'enhanced_vqvae': å¢å¼ºçš„VQVAE
            - 'fsq': Finite Scalar Quantization  
            - 'group_vq': Group Vector Quantization
            - 'rvq': Residual Vector Quantization
    """
    base_config = {
        'training': IMPROVED_TRAINING_CONFIG,
        'architecture': IMPROVED_ARCHITECTURE_CONFIG,
        'data': DATA_PREPROCESSING_CONFIG,
        'strategy': TRAINING_STRATEGY_CONFIG,
        'monitoring': MONITORING_CONFIG
    }
    
    if method == 'enhanced_vqvae':
        base_config['vqvae'] = IMPROVED_VQVAE_CONFIG
        
    elif method == 'fsq':
        base_config['vqvae'] = {**IMPROVED_VQVAE_CONFIG, **FSQ_CONFIG}
        
    elif method == 'group_vq':
        base_config['vqvae'] = {**IMPROVED_VQVAE_CONFIG, **GROUP_QUANTIZATION_CONFIG}
        
    elif method == 'rvq':
        base_config['vqvae'] = {**IMPROVED_VQVAE_CONFIG, **RVQ_CONFIG}
    
    return base_config

# ========================================
# ä½¿ç”¨ç¤ºä¾‹
# ========================================

if __name__ == "__main__":
    # è·å–ä¸åŒçš„é…ç½®
    enhanced_config = get_improved_config('enhanced_vqvae')
    fsq_config = get_improved_config('fsq')
    
    print("ğŸ”§ Enhanced VQVAEé…ç½®:")
    print(f"   Beta: {enhanced_config['vqvae']['beta']}")
    print(f"   åˆ©ç”¨ç‡æƒé‡: {enhanced_config['vqvae']['utilization_loss_weight']}")
    print(f"   EMAæ›´æ–°: {enhanced_config['vqvae']['use_ema_update']}")
    
    print("\nğŸ¯ FSQé…ç½®:")
    print(f"   FSQ levels: {fsq_config['vqvae']['fsq_levels']}")
    print(f"   æ€»codes: {8*5*5*5}") 