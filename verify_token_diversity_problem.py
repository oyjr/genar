#!/usr/bin/env python3
"""
éªŒè¯Tokenå¤šæ ·æ€§é—®é¢˜

å¯¹æ¯”ï¼š
1. Stage 1 VQVAEç¼–ç äº§ç”Ÿçš„tokensåˆ†å¸ƒ
2. Stage 2 VARç”Ÿæˆçš„tokensåˆ†å¸ƒ  
3. éªŒè¯è¿™æ˜¯å¦æ˜¯æ¨ç†å¼‚å¸¸çš„æ ¹æœ¬åŸå› 
"""

import sys
import os
sys.path.insert(0, 'src')

import torch
import numpy as np
from collections import Counter
from addict import Dict as AddictDict
from dataset.data_interface import DataInterface
from model.VAR.multi_scale_gene_vqvae import MultiScaleGeneVQVAE
from model.VAR.two_stage_var_st import TwoStageVARST
from main import DATASETS

def analyze_stage1_token_distribution():
    """åˆ†æStage 1äº§ç”Ÿçš„tokenåˆ†å¸ƒ"""
    print("ğŸ”¬ åˆ†æStage 1 VQVAEçš„Tokenåˆ†å¸ƒ")
    print("=" * 50)
    
    # åŠ è½½Stage 1æ¨¡å‹
    stage1_ckpt = "logs/PRAD/TWO_STAGE_VAR_ST/stage1-best-epoch=epoch=143-val_mse=val_mse=0.5353.ckpt"
    checkpoint = torch.load(stage1_ckpt, map_location='cpu')
    
    vqvae = MultiScaleGeneVQVAE()
    if 'state_dict' in checkpoint:
        state_dict = {}
        for key, value in checkpoint['state_dict'].items():
            if key.startswith('model.stage1_vqvae.'):
                new_key = key.replace('model.stage1_vqvae.', '')
                state_dict[new_key] = value
        vqvae.load_state_dict(state_dict, strict=False)
    
    vqvae.cuda()
    vqvae.eval()
    
    # å‡†å¤‡æ•°æ®
    dataset_info = DATASETS['PRAD']
    config = AddictDict({
        'data_path': dataset_info['path'],
        'slide_test': dataset_info['test_slides'],
        'encoder_name': dataset_info['recommended_encoder'],
        'use_augmented': False,
        'expr_name': 'PRAD',
        'MODEL': AddictDict({'model_name': 'TWO_STAGE_VAR_ST'}),
        'DATA': {
            'normalize': True,
            'test_dataloader': {
                'batch_size': 32,
                'num_workers': 0,
                'shuffle': False,
                'persistent_workers': False
            }
        }
    })
    
    data_interface = DataInterface(config)
    data_interface.setup(stage='test')
    dataloader = data_interface.test_dataloader()
    
    # æ”¶é›†æ‰€æœ‰tokens
    all_tokens = {scale: [] for scale in ['global', 'pathway', 'module', 'individual']}
    
    print("ğŸ”„ å¤„ç†æµ‹è¯•æ•°æ®...")
    with torch.no_grad():
        for i, batch in enumerate(dataloader):
            if i >= 20:  # å¤„ç†20ä¸ªæ‰¹æ¬¡ï¼Œè¶³å¤Ÿç»Ÿè®¡
                break
            
            gene_expression = batch['target_genes'].cuda()
            encode_result = vqvae.encode(gene_expression)
            tokens = encode_result['tokens']
            
            for scale in ['global', 'pathway', 'module', 'individual']:
                all_tokens[scale].append(tokens[scale].cpu())
            
            if i % 5 == 0:
                print(f"   å¤„ç†æ‰¹æ¬¡ {i}/20")
    
    # åˆ†ætokenåˆ†å¸ƒ
    print(f"\nğŸ“Š Stage 1 Tokenåˆ†å¸ƒç»Ÿè®¡:")
    for scale in ['global', 'pathway', 'module', 'individual']:
        all_scale_tokens = torch.cat(all_tokens[scale], dim=0).flatten().numpy()
        token_counts = Counter(all_scale_tokens)
        unique_tokens = len(token_counts)
        
        print(f"\n{scale.upper()}å°ºåº¦:")
        print(f"   æ€»tokenæ•°: {len(all_scale_tokens)}")
        print(f"   å”¯ä¸€tokenæ•°: {unique_tokens}/4096")
        print(f"   åˆ©ç”¨ç‡: {unique_tokens/4096:.4f}")
        
        # æ˜¾ç¤ºæœ€é¢‘ç¹çš„tokens
        most_common = token_counts.most_common(10)
        print(f"   æœ€é¢‘ç¹çš„10ä¸ªtokens:")
        for token, count in most_common:
            print(f"     Token {token}: {count}æ¬¡ ({count/len(all_scale_tokens)*100:.1f}%)")
        
        # è®¡ç®—é›†ä¸­åº¦ï¼ˆå‰10ä¸ªtokenå æ¯”ï¼‰
        top10_count = sum([count for _, count in most_common])
        concentration = top10_count / len(all_scale_tokens)
        print(f"   å‰10ä¸ªtokenå æ¯”: {concentration*100:.1f}%")
        
        if concentration > 0.8:
            print(f"   âŒ é«˜åº¦é›†ä¸­ï¼å¤§éƒ¨åˆ†è¾“å…¥è¢«æ˜ å°„åˆ°å°‘æ•°tokens")
        elif concentration > 0.5:
            print(f"   âš ï¸ è¾ƒä¸ºé›†ä¸­ï¼Œå¤šæ ·æ€§ä¸è¶³")
        else:
            print(f"   âœ… åˆ†å¸ƒç›¸å¯¹å‡åŒ€")

def compare_stage1_vs_var_tokens():
    """å¯¹æ¯”Stage 1ç¼–ç çš„tokens vs VARç”Ÿæˆçš„tokens"""
    print(f"\nğŸ†š å¯¹æ¯”Stage 1ç¼–ç  vs VARç”Ÿæˆçš„Tokenåˆ†å¸ƒ")
    print("=" * 60)
    
    # åŠ è½½å®Œæ•´çš„ä¸¤é˜¶æ®µæ¨¡å‹
    stage1_ckpt = "logs/PRAD/TWO_STAGE_VAR_ST/stage1-best-epoch=epoch=epoch=143-val_mse=val_mse=0.5353.ckpt"
    stage2_ckpt = "logs/PRAD/TWO_STAGE_VAR_ST/stage2-best-epoch=epoch=03-val_acc=val_accuracy=0.8263.ckpt"
    
    model_config = AddictDict({
        'MODEL': {
            'model_name': 'TWO_STAGE_VAR_ST',
            'stage1_ckpt': stage1_ckpt,
            'stage2_ckpt': stage2_ckpt,
            'feature_dim': 1024,
            'vocab_size': 4096,
            'embed_dim': 128,
            'depth': 16,
            'num_heads': 16,
            'num_genes': 200,
            'beta': 0.25,
            'hierarchical_loss_weight': 0.1,
            'vq_loss_weight': 0.25
        }
    })
    
    model = TwoStageVARST(model_config)
    model.cuda()
    model.eval()
    
    # å‡†å¤‡æ•°æ®
    dataset_info = DATASETS['PRAD']
    config = AddictDict({
        'data_path': dataset_info['path'],
        'slide_test': dataset_info['test_slides'],
        'encoder_name': dataset_info['recommended_encoder'],
        'use_augmented': False,
        'expr_name': 'PRAD',
        'MODEL': AddictDict({'model_name': 'TWO_STAGE_VAR_ST'}),
        'DATA': {
            'normalize': True,
            'test_dataloader': {
                'batch_size': 16,
                'num_workers': 0,
                'shuffle': False,
                'persistent_workers': False
            }
        }
    })
    
    data_interface = DataInterface(config)
    data_interface.setup(stage='test')
    dataloader = data_interface.test_dataloader()
    
    # æ”¶é›†tokens
    stage1_tokens = {scale: [] for scale in ['global', 'pathway', 'module', 'individual']}
    var_tokens = {scale: [] for scale in ['global', 'pathway', 'module', 'individual']}
    
    print("ğŸ”„ å¤„ç†å¯¹æ¯”æ•°æ®...")
    with torch.no_grad():
        for i, batch in enumerate(dataloader):
            if i >= 10:  # å¤„ç†10ä¸ªæ‰¹æ¬¡
                break
            
            histology_features = batch['feat'].cuda()
            spatial_coords = batch['coords'].cuda()
            gene_expression = batch['target_genes'].cuda()
            
            # 1. Stage 1ç›´æ¥ç¼–ç 
            encode_result = model.stage1_vqvae.encode(gene_expression)
            s1_tokens = encode_result['tokens']
            
            # 2. VARç”Ÿæˆtokens
            var_result = model.stage2_var.generate_tokens(histology_features, spatial_coords)
            v_tokens = var_result['tokens']
            
            # æ”¶é›†
            for scale in ['global', 'pathway', 'module', 'individual']:
                stage1_tokens[scale].append(s1_tokens[scale].cpu())
                var_tokens[scale].append(v_tokens[scale].cpu())
            
            if i % 2 == 0:
                print(f"   å¤„ç†æ‰¹æ¬¡ {i}/10")
    
    # å¯¹æ¯”åˆ†æ
    print(f"\nğŸ“Š Stage 1ç¼–ç  vs VARç”Ÿæˆ å¯¹æ¯”:")
    
    for scale in ['global', 'pathway', 'module', 'individual']:
        s1_all = torch.cat(stage1_tokens[scale], dim=0).flatten().numpy()
        var_all = torch.cat(var_tokens[scale], dim=0).flatten().numpy()
        
        s1_unique = len(set(s1_all))
        var_unique = len(set(var_all))
        
        print(f"\n{scale.upper()}å°ºåº¦:")
        print(f"   Stage 1å”¯ä¸€tokens: {s1_unique}/4096 ({s1_unique/4096:.4f})")
        print(f"   VARç”Ÿæˆå”¯ä¸€tokens: {var_unique}/4096 ({var_unique/4096:.4f})")
        
        # é‡å åˆ†æ
        s1_set = set(s1_all)
        var_set = set(var_all)
        overlap = len(s1_set.intersection(var_set))
        
        print(f"   Tokené‡å æ•°: {overlap}")
        print(f"   é‡å ç‡: {overlap/max(s1_unique, var_unique)*100:.1f}%")
        
        # åˆ†å¸ƒé›†ä¸­åº¦å¯¹æ¯”
        s1_counter = Counter(s1_all)
        var_counter = Counter(var_all)
        
        s1_top5 = [count for _, count in s1_counter.most_common(5)]
        var_top5 = [count for _, count in var_counter.most_common(5)]
        
        s1_concentration = sum(s1_top5) / len(s1_all)
        var_concentration = sum(var_top5) / len(var_all)
        
        print(f"   Stage 1å‰5ä¸ªtokenå æ¯”: {s1_concentration*100:.1f}%")
        print(f"   VARå‰5ä¸ªtokenå æ¯”: {var_concentration*100:.1f}%")
        
        if var_concentration > s1_concentration + 0.1:
            print(f"   âŒ VARç”Ÿæˆçš„tokensæ›´åŠ é›†ä¸­ï¼Œå¤šæ ·æ€§æŸå¤±ä¸¥é‡")
        elif var_concentration > s1_concentration:
            print(f"   âš ï¸ VARç•¥å¾®å¢åŠ äº†é›†ä¸­åº¦")
        else:
            print(f"   âœ… VARä¿æŒäº†tokenå¤šæ ·æ€§")

def test_codebook_utilization_during_training():
    """æµ‹è¯•è®­ç»ƒè¿‡ç¨‹ä¸­codebookçš„å®é™…åˆ©ç”¨æƒ…å†µ"""
    print(f"\nğŸ¯ æµ‹è¯•è®­ç»ƒæ•°æ®çš„Codebookåˆ©ç”¨æƒ…å†µ")
    print("=" * 50)
    
    # åŠ è½½Stage 1æ¨¡å‹
    stage1_ckpt = "logs/PRAD/TWO_STAGE_VAR_ST/stage1-best-epoch=epoch=epoch=143-val_mse=val_mse=0.5353.ckpt"
    checkpoint = torch.load(stage1_ckpt, map_location='cpu')
    
    vqvae = MultiScaleGeneVQVAE()
    if 'state_dict' in checkpoint:
        state_dict = {}
        for key, value in checkpoint['state_dict'].items():
            if key.startswith('model.stage1_vqvae.'):
                new_key = key.replace('model.stage1_vqvae.', '')
                state_dict[new_key] = value
        vqvae.load_state_dict(state_dict, strict=False)
    
    vqvae.cuda()
    vqvae.eval()
    
    # ä½¿ç”¨è®­ç»ƒé›†æ•°æ®æµ‹è¯•
    dataset_info = DATASETS['PRAD']
    config = AddictDict({
        'data_path': dataset_info['path'],
        'slide_val': dataset_info['val_slides'],  # ä½¿ç”¨è®­ç»ƒæ•°æ®
        'encoder_name': dataset_info['recommended_encoder'],
        'use_augmented': False,
        'expr_name': 'PRAD',
        'MODEL': AddictDict({'model_name': 'TWO_STAGE_VAR_ST'}),
        'DATA': {
            'normalize': True,
            'val_dataloader': {
                'batch_size': 32,
                'num_workers': 0,
                'shuffle': True,
                'persistent_workers': False
            }
        }
    })
    
    data_interface = DataInterface(config)
    data_interface.setup(stage='fit')
    train_dataloader = data_interface.val_dataloader()  # ä½¿ç”¨éªŒè¯é›†ä½œä¸ºè®­ç»ƒæ•°æ®çš„ä»£è¡¨
    
    print("ğŸ“ˆ åˆ†æè®­ç»ƒæ•°æ®çš„tokenåˆ†å¸ƒ...")
    
    all_tokens = {scale: [] for scale in ['global', 'pathway', 'module', 'individual']}
    
    with torch.no_grad():
        for i, batch in enumerate(train_dataloader):
            if i >= 30:  # æ›´å¤šæ‰¹æ¬¡
                break
            
            gene_expression = batch['target_genes'].cuda()
            encode_result = vqvae.encode(gene_expression)
            tokens = encode_result['tokens']
            
            for scale in ['global', 'pathway', 'module', 'individual']:
                all_tokens[scale].append(tokens[scale].cpu())
            
            if i % 10 == 0:
                print(f"   å¤„ç†è®­ç»ƒæ‰¹æ¬¡ {i}/30")
    
    print(f"\nğŸ“Š è®­ç»ƒæ•°æ®Tokenåˆ©ç”¨ç»Ÿè®¡:")
    for scale in ['global', 'pathway', 'module', 'individual']:
        all_scale_tokens = torch.cat(all_tokens[scale], dim=0).flatten().numpy()
        unique_tokens = len(set(all_scale_tokens))
        
        print(f"\n{scale.upper()}å°ºåº¦ (è®­ç»ƒæ•°æ®):")
        print(f"   å”¯ä¸€tokenæ•°: {unique_tokens}/4096")
        print(f"   åˆ©ç”¨ç‡: {unique_tokens/4096:.4f}")
        
        if unique_tokens < 100:
            print(f"   âŒ ä¸¥é‡æ¬ åˆ©ç”¨ï¼åªä½¿ç”¨äº†{unique_tokens}ä¸ªcodebookå‘é‡")
        elif unique_tokens < 500:
            print(f"   âš ï¸ åˆ©ç”¨ä¸è¶³ï¼Œå¤§é‡codebookå‘é‡æœªè¢«ä½¿ç”¨")
        else:
            print(f"   âœ… åˆç†åˆ©ç”¨codebook")

if __name__ == "__main__":
    analyze_stage1_token_distribution()
    compare_stage1_vs_var_tokens()
    test_codebook_utilization_during_training() 