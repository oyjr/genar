"""
Two-Stage VAR-ST Stage 2 æ¨ç†æ¼”ç¤ºè„šæœ¬

æ­¤è„šæœ¬å±•ç¤ºå¦‚ä½•ï¼š
1. åŠ è½½è®­ç»ƒå¥½çš„Stage 2æ¨¡å‹
2. è¿›è¡Œç«¯åˆ°ç«¯çš„åŸºå› è¡¨è¾¾é¢„æµ‹æ¨ç†
3. å¯è§†åŒ–æ¨ç†ç»“æœ

ä½¿ç”¨æ–¹æ³•ï¼š
python stage2_inference_demo.py --ckpt logs/PRAD/TWO_STAGE_VAR_ST/best-epoch=epoch=00-val_mse=0.0000.ckpt
"""

import sys
import os
sys.path.insert(0, 'src')

import torch
import numpy as np
import argparse
from typing import Dict, Tuple
import matplotlib.pyplot as plt

from model.VAR.two_stage_var_st import TwoStageVARST
from model.model_interface import ModelInterface
from dataset.data_interface import DataInterface


def load_stage2_model_from_lightning_ckpt(ckpt_path: str, device: str = 'cuda') -> TwoStageVARST:
    """
    ä»PyTorch Lightning checkpointåŠ è½½Stage 2æ¨¡å‹
    
    Args:
        ckpt_path: Lightning checkpointè·¯å¾„
        device: ç›®æ ‡è®¾å¤‡
    
    Returns:
        åŠ è½½å¥½çš„Two-Stage VAR-STæ¨¡å‹ï¼Œå¯ç”¨äºæ¨ç†
    """
    print(f"ğŸ”„ ä» {ckpt_path} åŠ è½½Stage 2æ¨¡å‹...")
    
    # åŠ è½½Lightning checkpoint
    checkpoint = torch.load(ckpt_path, map_location=device)
    state_dict = checkpoint['state_dict']
    hyper_params = checkpoint.get('hyper_parameters', {})
    
    print(f"   Checkpointä¿¡æ¯:")
    print(f"   - Epoch: {checkpoint.get('epoch', 'unknown')}")
    print(f"   - Global step: {checkpoint.get('global_step', 'unknown')}")
    print(f"   - Lightningç‰ˆæœ¬: {checkpoint.get('pytorch-lightning_version', 'unknown')}")
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«Stage 2æƒé‡
    stage1_keys = [k for k in state_dict.keys() if 'stage1_vqvae' in k]
    stage2_keys = [k for k in state_dict.keys() if 'stage2_var' in k]
    condition_keys = [k for k in state_dict.keys() if 'condition_processor' in k]
    
    print(f"   - Stage 1æƒé‡: {len(stage1_keys)}ä¸ª")
    print(f"   - Stage 2æƒé‡: {len(stage2_keys)}ä¸ª")
    print(f"   - æ¡ä»¶å¤„ç†å™¨æƒé‡: {len(condition_keys)}ä¸ª")
    
    if len(stage2_keys) == 0:
        raise ValueError("Checkpointä¸­æœªæ‰¾åˆ°Stage 2æƒé‡ï¼è¯·ç¡®ä¿ä½¿ç”¨Stage 2è®­ç»ƒçš„checkpointã€‚")
    
    # è·å–é…ç½®ä¿¡æ¯
    model_config = hyper_params.get('config', {}).get('MODEL', {})
    stage1_ckpt_path = model_config.get('stage1_ckpt_path')
    
    print(f"   - åŸå§‹Stage 1 checkpoint: {stage1_ckpt_path}")
    
    # åˆ›å»ºæ¨¡å‹å®ä¾‹ - ç”±äºLightning checkpointåŒ…å«å®Œæ•´çŠ¶æ€ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥åŠ è½½
    model = TwoStageVARST(
        num_genes=model_config.get('num_genes', 200),
        histology_feature_dim=model_config.get('histology_feature_dim', 1024),
        spatial_coord_dim=model_config.get('spatial_coord_dim', 2),
        current_stage=1,  # å…ˆè®¾ç½®ä¸ºStage 1ï¼Œé¿å…è¦æ±‚stage1_ckpt_path
        device=device
    )
    
    # æå–å¹¶åŠ è½½æ¨¡å‹æƒé‡ï¼ˆå»æ‰Lightningçš„å‰ç¼€ï¼‰
    model_state_dict = {}
    for key, value in state_dict.items():
        if key.startswith('model.'):
            # å»æ‰ 'model.' å‰ç¼€
            new_key = key[6:]
            model_state_dict[new_key] = value
        else:
            model_state_dict[key] = value
    
    # åŠ è½½æƒé‡
    model.load_state_dict(model_state_dict, strict=False)
    model = model.to(device)
    
    # ç°åœ¨è®¾ç½®ä¸ºStage 2æ¨¡å¼ï¼ˆæ¨ç†æ¨¡å¼ï¼Œä¸éœ€è¦é‡æ–°åŠ è½½checkpointï¼‰
    model.current_stage = 2
    model._set_vqvae_trainable(False)  # VQVAEå†»ç»“
    model._set_var_trainable(False)    # VARä¹Ÿå†»ç»“ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
    model.eval()
    
    print(f"âœ… Stage 2æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    return model


def demo_inference(model: TwoStageVARST, num_samples: int = 5, device: str = 'cuda'):
    """
    æ¼”ç¤ºStage 2æ¨ç†åŠŸèƒ½
    
    Args:
        model: åŠ è½½å¥½çš„Stage 2æ¨¡å‹
        num_samples: ç”Ÿæˆæ ·æœ¬æ•°é‡
        device: è®¾å¤‡
    """
    print(f"\nğŸ§¬ å¼€å§‹Stage 2æ¨ç†æ¼”ç¤º (ç”Ÿæˆ {num_samples} ä¸ªæ ·æœ¬)...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥æ•°æ®
    batch_size = num_samples
    histology_features = torch.randn(batch_size, 1024, device=device)  # æ¨¡æ‹Ÿç»„ç»‡å­¦ç‰¹å¾
    spatial_coords = torch.randn(batch_size, 2, device=device)         # æ¨¡æ‹Ÿç©ºé—´åæ ‡
    
    print(f"   è¾“å…¥æ•°æ®:")
    print(f"   - ç»„ç»‡å­¦ç‰¹å¾: {histology_features.shape}")
    print(f"   - ç©ºé—´åæ ‡: {spatial_coords.shape}")
    
    # è¿›è¡Œæ¨ç†
    with torch.no_grad():
        # åŸºç¡€æ¨ç†
        results = model.inference(
            histology_features=histology_features,
            spatial_coords=spatial_coords,
            temperature=1.0,  # æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§
            top_k=50,         # Top-ké‡‡æ ·
            top_p=0.9         # Nucleusé‡‡æ ·
        )
    
    # è§£æç»“æœ
    predicted_genes = results['predicted_gene_expression']  # [B, 200]
    generated_tokens = results['generated_tokens']          # [B, 241]
    multi_scale_tokens = results['multi_scale_tokens']      # Dict
    
    print(f"\nğŸ“Š æ¨ç†ç»“æœ:")
    print(f"   - é¢„æµ‹åŸºå› è¡¨è¾¾: {predicted_genes.shape}")
    print(f"   - ç”Ÿæˆçš„tokens: {generated_tokens.shape}")
    print(f"   - å¤šå°ºåº¦tokenç»“æ„:")
    for scale, tokens in multi_scale_tokens.items():
        print(f"     {scale}: {tokens.shape}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    gene_stats = {
        'mean': predicted_genes.mean(dim=1),     # [B]
        'std': predicted_genes.std(dim=1),       # [B]
        'min': predicted_genes.min(dim=1)[0],    # [B]
        'max': predicted_genes.max(dim=1)[0],    # [B]
    }
    
    print(f"\nğŸ“ˆ é¢„æµ‹åŸºå› è¡¨è¾¾ç»Ÿè®¡:")
    for i in range(batch_size):
        print(f"   æ ·æœ¬ {i+1}: mean={gene_stats['mean'][i]:.4f}, "
              f"std={gene_stats['std'][i]:.4f}, "
              f"range=[{gene_stats['min'][i]:.4f}, {gene_stats['max'][i]:.4f}]")
    
    return results


def compare_sampling_strategies(model: TwoStageVARST, device: str = 'cuda'):
    """
    æ¯”è¾ƒä¸åŒé‡‡æ ·ç­–ç•¥çš„æ•ˆæœ
    
    Args:
        model: Stage 2æ¨¡å‹
        device: è®¾å¤‡
    """
    print(f"\nğŸ¯ æ¯”è¾ƒä¸åŒé‡‡æ ·ç­–ç•¥...")
    
    # å›ºå®šè¾“å…¥
    histology_features = torch.randn(1, 1024, device=device)
    spatial_coords = torch.randn(1, 2, device=device)
    
    # ä¸åŒé‡‡æ ·ç­–ç•¥
    sampling_configs = [
        {'name': 'Greedy', 'temperature': 0.1, 'top_k': None, 'top_p': None},
        {'name': 'Low Temp', 'temperature': 0.7, 'top_k': None, 'top_p': None},
        {'name': 'High Temp', 'temperature': 1.5, 'top_k': None, 'top_p': None},
        {'name': 'Top-K', 'temperature': 1.0, 'top_k': 50, 'top_p': None},
        {'name': 'Nucleus', 'temperature': 1.0, 'top_k': None, 'top_p': 0.9},
    ]
    
    results_comparison = {}
    
    for config in sampling_configs:
        with torch.no_grad():
            result = model.inference(
                histology_features=histology_features,
                spatial_coords=spatial_coords,
                temperature=config['temperature'],
                top_k=config['top_k'],
                top_p=config['top_p']
            )
        
        pred_genes = result['predicted_gene_expression'][0]  # [200]
        results_comparison[config['name']] = {
            'predictions': pred_genes.cpu().numpy(),
            'mean': pred_genes.mean().item(),
            'std': pred_genes.std().item(),
            'entropy': -torch.sum(torch.softmax(pred_genes, dim=0) * 
                                 torch.log_softmax(pred_genes, dim=0)).item()
        }
    
    # æ‰“å°æ¯”è¾ƒç»“æœ
    print(f"\nğŸ“Š é‡‡æ ·ç­–ç•¥æ¯”è¾ƒ:")
    print(f"{'Strategy':<10} {'Mean':<8} {'Std':<8} {'Entropy':<10}")
    print("-" * 40)
    for name, stats in results_comparison.items():
        print(f"{name:<10} {stats['mean']:<8.4f} {stats['std']:<8.4f} {stats['entropy']:<10.4f}")
    
    return results_comparison


def save_inference_results(results: Dict, save_dir: str = './inference_results'):
    """
    ä¿å­˜æ¨ç†ç»“æœ
    
    Args:
        results: æ¨ç†ç»“æœå­—å…¸
        save_dir: ä¿å­˜ç›®å½•
    """
    os.makedirs(save_dir, exist_ok=True)
    
    # ä¿å­˜åŸºå› è¡¨è¾¾é¢„æµ‹
    predicted_genes = results['predicted_gene_expression'].cpu().numpy()
    np.save(os.path.join(save_dir, 'predicted_gene_expression.npy'), predicted_genes)
    
    # ä¿å­˜tokens
    generated_tokens = results['generated_tokens'].cpu().numpy()
    np.save(os.path.join(save_dir, 'generated_tokens.npy'), generated_tokens)
    
    # ä¿å­˜å¤šå°ºåº¦tokens
    for scale, tokens in results['multi_scale_tokens'].items():
        np.save(os.path.join(save_dir, f'tokens_{scale}.npy'), tokens.cpu().numpy())
    
    print(f"ğŸ’¾ æ¨ç†ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")


def main():
    parser = argparse.ArgumentParser(description='Two-Stage VAR-ST Stage 2 æ¨ç†æ¼”ç¤º')
    parser.add_argument('--ckpt', type=str, required=True,
                       help='PyTorch Lightning checkpointè·¯å¾„')
    parser.add_argument('--device', type=str, default='cuda',
                       choices=['cuda', 'cpu'], help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--num_samples', type=int, default=5,
                       help='ç”Ÿæˆæ ·æœ¬æ•°é‡')
    parser.add_argument('--save_results', action='store_true',
                       help='æ˜¯å¦ä¿å­˜æ¨ç†ç»“æœ')
    
    args = parser.parse_args()
    
    print("ğŸš€ Two-Stage VAR-ST Stage 2 æ¨ç†æ¼”ç¤º")
    print("=" * 50)
    
    # æ£€æŸ¥è®¾å¤‡
    device = args.device
    if device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        device = 'cpu'
    
    try:
        # 1. åŠ è½½æ¨¡å‹
        model = load_stage2_model_from_lightning_ckpt(args.ckpt, device)
        
        # 2. æ¼”ç¤ºåŸºç¡€æ¨ç†
        results = demo_inference(model, args.num_samples, device)
        
        # 3. æ¯”è¾ƒé‡‡æ ·ç­–ç•¥
        comparison = compare_sampling_strategies(model, device)
        
        # 4. ä¿å­˜ç»“æœï¼ˆå¯é€‰ï¼‰
        if args.save_results:
            save_inference_results(results)
        
        print(f"\nâœ… Stage 2æ¨ç†æ¼”ç¤ºå®Œæˆï¼")
        print(f"ğŸ’¡ Tips:")
        print(f"   - è°ƒæ•´temperatureå‚æ•°æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§")
        print(f"   - ä½¿ç”¨top_k/top_på‚æ•°æ”¹å–„ç”Ÿæˆè´¨é‡")
        print(f"   - ç»„ç»‡å­¦ç‰¹å¾å’Œç©ºé—´åæ ‡ä¼šå½±å“åŸºå› è¡¨è¾¾é¢„æµ‹")
        
    except Exception as e:
        print(f"âŒ æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 