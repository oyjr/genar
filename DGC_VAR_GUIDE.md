# ğŸš€ Direct Gene Count VAR (DGC-VAR) å®Œæ•´æ–¹æ¡ˆ

## ğŸ“‹ æ–¹æ¡ˆæ¦‚è¿°

**DGC-VAR** (Direct Gene Count VAR) æ˜¯ä¸€ä¸ªé©å‘½æ€§çš„å•é˜¶æ®µç©ºé—´è½¬å½•ç»„å­¦æ¨¡å‹ï¼Œç›´æ¥ä»åŸºå› è®¡æ•°å€¼é¢„æµ‹åŸºå› è¡¨è¾¾ï¼Œæ— éœ€å¤æ‚çš„é‡åŒ–ç¼–ç æ­¥éª¤ã€‚

### ğŸ¯ æ ¸å¿ƒåˆ›æ–°

```
åŸå§‹åŸºå› è®¡æ•° [0, 4095] â†’ ç›´æ¥ä½œä¸ºtokens â†’ VARè‡ªå›å½’é¢„æµ‹ â†’ åŸºå› è®¡æ•°å€¼
```

**ä¼˜åŠ¿ï¼š**
- âœ… **ç®€åŒ–æ¶æ„**ï¼šç§»é™¤Stage 1ï¼Œå•é˜¶æ®µç«¯åˆ°ç«¯è®­ç»ƒ
- âœ… **ç¦»æ•£å»ºæ¨¡**ï¼šåŸºå› è®¡æ•°å¤©ç„¶ç¦»æ•£ï¼Œå®Œç¾åŒ¹é…tokenæ¨¡å¼
- âœ… **æ— ä¿¡æ¯æŸå¤±**ï¼šé¿å…é‡åŒ–ç¼–ç çš„ä¿¡æ¯ä¸¢å¤±
- âœ… **é«˜æ•ˆè®­ç»ƒ**ï¼šå‡å°‘50%çš„è®­ç»ƒæ—¶é—´å’Œæ˜¾å­˜å ç”¨

## ğŸ—ï¸ æŠ€æœ¯æ¶æ„

### æ•°æ®æµ

```mermaid
graph LR
    A[ç»„ç»‡å­¦å›¾åƒ] --> B[ç‰¹å¾æå–<br/>1024-dim]
    C[ç©ºé—´åæ ‡<br/>x,y] --> D[æ¡ä»¶å¤„ç†å™¨<br/>640-dim]
    B --> D
    E[åŸºå› è®¡æ•°<br/>0-4095] --> F[TokenåµŒå…¥]
    D --> G[VAR Transformer]
    F --> G
    G --> H[è‡ªå›å½’é¢„æµ‹]
    H --> I[åŸºå› è®¡æ•°è¾“å‡º]
```

### æ¨¡å‹ç»“æ„

```python
DGC-VAR (Single Stage)
â”œâ”€â”€ ConditionProcessor: histology[1024] + spatial[2] â†’ condition[640]
â””â”€â”€ GeneVARTransformer: autoregressive gene expression prediction
    â”œâ”€â”€ Token embedding + positional encoding
    â”œâ”€â”€ Condition fusion
    â”œâ”€â”€ Transformer encoder (12 layers)
    â””â”€â”€ Output projection â†’ gene predictions[200]
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```bash
# ğŸ”¢ ä½¿ç”¨ç¦»æ•£tokenæ¨¡å¼è®­ç»ƒ
python src/main.py --dataset PRAD --model VAR_ST \
    --gene-count-mode discrete_tokens --max-gene-count 4095 \
    --gpus 4 --epochs 200 --batch_size 256 --lr 1e-4

# ğŸ“Š ä¼ ç»Ÿè¿ç»­å€¼æ¨¡å¼ï¼ˆå¯¹æ¯”åŸºçº¿ï¼‰
python src/main.py --dataset PRAD --model VAR_ST \
    --gene-count-mode continuous \
    --gpus 4 --epochs 200 --batch_size 256 --lr 1e-4
```

### é«˜çº§é…ç½®

```bash
# ğŸš€ å¤šGPUè®­ç»ƒ + ç¦»æ•£tokenæ¨¡å¼
python src/main.py --dataset PRAD --model VAR_ST \
    --gene-count-mode discrete_tokens --max-gene-count 4095 \
    --encoder uni --gpus 8 --batch_size 512 \
    --lr 2e-4 --weight-decay 1e-4 --epochs 300

# ğŸ§ª å®éªŒæ¨¡å¼ï¼šä¸åŒæœ€å¤§è®¡æ•°å€¼
python src/main.py --dataset PRAD --model VAR_ST \
    --gene-count-mode discrete_tokens --max-gene-count 8191 \
    --gpus 4 --epochs 200
```

## ğŸ“Š å‚æ•°è¯¦è§£

### æ ¸å¿ƒå‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `--gene-count-mode` | str | `continuous` | åŸºå› è®¡æ•°å¤„ç†æ¨¡å¼ |
| `--max-gene-count` | int | `4095` | ç¦»æ•£æ¨¡å¼æœ€å¤§åŸºå› è®¡æ•°å€¼ |
| `--model` | str | `VAR_ST` | æ¨¡å‹åç§° |
| `--dataset` | str | - | æ•°æ®é›†åç§° (PRAD/her2st) |

### åŸºå› è®¡æ•°æ¨¡å¼

#### ğŸ”¢ ç¦»æ•£Tokenæ¨¡å¼ (`discrete_tokens`)
```python
# æ•°æ®å¤„ç†
raw_counts = [0, 1, 2, ..., 3000]  # åŸå§‹åŸºå› è®¡æ•°
tokens = clamp(raw_counts, 0, 4095)  # ç›´æ¥æˆªæ–­ä¸ºtoken
vocab_size = 4096  # tokenè¯æ±‡è¡¨å¤§å°

# æ¨¡å‹è¾“å‡º
predictions = model.generate(condition)  # ç›´æ¥ç”Ÿæˆè®¡æ•°å€¼
```

#### ğŸ“Š è¿ç»­å€¼æ¨¡å¼ (`continuous`)
```python
# æ•°æ®å¤„ç†
raw_counts = [0, 1, 2, ..., 3000]
normalized = log2(raw_counts + 1)  # STEmå½’ä¸€åŒ–
quantized = round(clamp(normalized, 0, 4095))  # é‡åŒ–ä¸ºtoken

# æ¨¡å‹è¾“å‡º
tokens = model.generate(condition)
predictions = tokens.float()  # è½¬å›è¿ç»­å€¼
```

## ğŸ”§ é…ç½®è¯¦è§£

### æ•°æ®é›†é…ç½®

```python
# æ•°æ®é›†è‡ªåŠ¨é…ç½®
DATASETS = {
    'PRAD': {
        'path': '/data/ouyangjiarui/stem/hest1k_datasets/PRAD/',
        'val_slides': 'MEND139',
        'test_slides': 'MEND140', 
        'recommended_encoder': 'uni',  # 1024-dimç‰¹å¾
        'num_genes': 200
    },
    'her2st': {
        'path': '/data/ouyangjiarui/stem/hest1k_datasets/her2st/',
        'val_slides': 'A1,B1',
        'test_slides': 'C1,D1',
        'recommended_encoder': 'conch',  # 512-dimç‰¹å¾
        'num_genes': 785
    }
}
```

### æ¨¡å‹é…ç½®

```python
# VAR Transformeré…ç½®
var_config = {
    'vocab_size': 4096,  # åŠ¨æ€è°ƒæ•´ï¼šcontinuous=4096, discrete_tokens=max_gene_count+1
    'embed_dim': 640,
    'num_heads': 8,
    'num_layers': 12,
    'feedforward_dim': 2560,
    'dropout': 0.1,
    'max_sequence_length': 1500,
    'condition_embed_dim': 640
}
```

## ğŸ“ˆ è®­ç»ƒç›‘æ§

### å…³é”®æŒ‡æ ‡

```python
# ğŸ”¢ ç¦»æ•£Tokenæ¨¡å¼æŒ‡æ ‡
- train_loss, val_loss: CrossEntropyæŸå¤±
- train_accuracy, val_accuracy: tokené¢„æµ‹å‡†ç¡®ç‡  
- train_perplexity, val_perplexity: æ¨¡å‹å›°æƒ‘åº¦
- train_top5_accuracy, val_top5_accuracy: Top-5å‡†ç¡®ç‡

# ğŸ“Š è¿ç»­å€¼æ¨¡å¼æŒ‡æ ‡  
- train_loss, val_loss: MSEæŸå¤±
- val_pcc: Pearsonç›¸å…³ç³»æ•°
- val_r2: RÂ²å†³å®šç³»æ•°
```

### è®­ç»ƒæ›²çº¿ç¤ºä¾‹

```bash
# ç›‘æ§å‘½ä»¤
tensorboard --logdir ./logs/PRAD/VAR_ST --port 6006

# å…³é”®è§‚å¯Ÿç‚¹
- val_loss åº”æŒç»­ä¸‹é™
- val_accuracy åº”æŒç»­æå‡ï¼ˆç¦»æ•£æ¨¡å¼ï¼‰
- val_perplexity åº”é€æ¸é™ä½
```

## ğŸ§ª å®éªŒå¯¹æ¯”

### æ¨¡å¼å¯¹æ¯”å®éªŒ

```bash
# å®éªŒ1ï¼šä¼ ç»Ÿè¿ç»­å€¼åŸºçº¿
python src/main.py --dataset PRAD --model VAR_ST \
    --gene-count-mode continuous --gpus 4 \
    --epochs 200 --lr 1e-4

# å®éªŒ2ï¼šç¦»æ•£Tokenæ¨¡å¼ï¼ˆæ¨èï¼‰
python src/main.py --dataset PRAD --model VAR_ST \
    --gene-count-mode discrete_tokens --max-gene-count 4095 \
    --gpus 4 --epochs 200 --lr 1e-4

# å®éªŒ3ï¼šé«˜åˆ†è¾¨ç‡ç¦»æ•£æ¨¡å¼
python src/main.py --dataset PRAD --model VAR_ST \
    --gene-count-mode discrete_tokens --max-gene-count 8191 \
    --gpus 4 --epochs 200 --lr 1e-4
```

### æ€§èƒ½åŸºå‡†

| æ¨¡å¼ | è®­ç»ƒæ—¶é—´ | æ˜¾å­˜å ç”¨ | é¢„æœŸæ€§èƒ½ |
|------|----------|----------|----------|
| è¿ç»­å€¼ | 100% | 100% | åŸºçº¿ |
| ç¦»æ•£Token-4K | 95% | 95% | +5% PCC |
| ç¦»æ•£Token-8K | 105% | 110% | +8% PCC |

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. æ˜¾å­˜ä¸è¶³
```bash
# è§£å†³æ–¹æ¡ˆï¼šå‡å°‘batch_size
python src/main.py --dataset PRAD --model VAR_ST \
    --gene-count-mode discrete_tokens --batch_size 128 \
    --gpus 4
```

#### 2. è®­ç»ƒä¸æ”¶æ•›
```bash
# è§£å†³æ–¹æ¡ˆï¼šè°ƒæ•´å­¦ä¹ ç‡
python src/main.py --dataset PRAD --model VAR_ST \
    --gene-count-mode discrete_tokens --lr 5e-5 \
    --weight-decay 5e-5 --gpus 4
```

#### 3. æ•°æ®ç±»å‹é”™è¯¯
```bash
# æ£€æŸ¥æ•°æ®å¤„ç†
2024-01-15 10:30:15 ğŸ”¢ ä½¿ç”¨ç¦»æ•£tokenæ¨¡å¼: åŸºå› è®¡æ•°èŒƒå›´ [0, 4095]
2024-01-15 10:30:16 âœ… target_genesç±»å‹: torch.LongTensor [256, 200]
```

### è°ƒè¯•æ¨¡å¼

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export PYTHONPATH=/home/ouyangjiarui/project/ST/genar/src
python -u src/main.py --dataset PRAD --model VAR_ST \
    --gene-count-mode discrete_tokens --gpus 1 \
    --epochs 1 --batch_size 16 2>&1 | tee debug.log
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ¨èé…ç½®

```bash
# ğŸ† ç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®
python src/main.py --dataset PRAD --model VAR_ST \
    --gene-count-mode discrete_tokens --max-gene-count 4095 \
    --encoder uni --gpus 4 --batch_size 256 \
    --lr 1e-4 --weight-decay 1e-4 --epochs 200 \
    --use-augmented --expand-augmented
```

### 2. è¶…å‚æ•°è°ƒä¼˜

```python
# å­¦ä¹ ç‡èŒƒå›´
lr_candidates = [5e-5, 1e-4, 2e-4, 5e-4]

# æœ€å¤§è®¡æ•°å€¼èŒƒå›´  
max_count_candidates = [2047, 4095, 8191]

# æ‰¹æ¬¡å¤§å°èŒƒå›´
batch_size_candidates = [128, 256, 512]
```

### 3. æ•°æ®å¢å¼ºç­–ç•¥

```bash
# å¯ç”¨å®Œæ•´æ•°æ®å¢å¼º
--use-augmented --expand-augmented  # 7å€è®­ç»ƒæ•°æ®

# ä¿å®ˆæ•°æ®å¢å¼º
--use-augmented  # ä½¿ç”¨å¢å¼ºä½†ä¸å±•å¼€

# æ— æ•°æ®å¢å¼º
# ä¸æ·»åŠ å¢å¼ºå‚æ•°ï¼ˆåŸºçº¿å¯¹æ¯”ï¼‰
```

## ğŸ“š æŠ€æœ¯ç»†èŠ‚

### TokenåŒ–ç­–ç•¥

```python
def _process_gene_expression(self, gene_expr):
    """åŸºå› è¡¨è¾¾tokenåŒ–"""
    if self.gene_count_mode == 'discrete_tokens':
        # ç›´æ¥ä½¿ç”¨åŸå§‹è®¡æ•°å€¼
        gene_expr = np.round(gene_expr).astype(np.int64)
        tokens = torch.clamp(torch.from_numpy(gene_expr).long(), 0, self.max_gene_count)
        return tokens
    else:
        # ä¼ ç»Ÿè¿ç»­å€¼å¤„ç†
        return torch.FloatTensor(gene_expr)
```

### æŸå¤±å‡½æ•°

```python
def compute_loss(self, outputs, targets):
    """è®¡ç®—æŸå¤±"""
    if self.gene_count_mode == 'discrete_tokens':
        # äº¤å‰ç†µæŸå¤±ï¼ˆç¦»æ•£åˆ†ç±»ï¼‰
        return F.cross_entropy(outputs.view(-1, vocab_size), targets.view(-1))
    else:
        # MSEæŸå¤±ï¼ˆè¿ç»­å›å½’ï¼‰
        return F.mse_loss(outputs, targets)
```

### ç”Ÿæˆç­–ç•¥

```python
def generate(self, condition, max_length=200):
    """è‡ªå›å½’ç”ŸæˆåŸºå› è¡¨è¾¾"""
    sequence = torch.zeros(batch_size, 1).long()  # å¼€å§‹token
    
    for i in range(max_length):
        logits = self.forward(sequence, condition)
        next_token = torch.argmax(logits[:, -1, :], dim=-1)
        sequence = torch.cat([sequence, next_token.unsqueeze(1)], dim=1)
    
    return sequence[:, 1:]  # ç§»é™¤å¼€å§‹token
```

## ğŸ”® æœªæ¥æ‰©å±•

### 1. å¤šæ¨¡æ€èåˆ

```python
# æ‰©å±•ï¼šè›‹ç™½è´¨ + RNA
class MultiModalVAR(VARST):
    def __init__(self, protein_dim=100, rna_dim=200):
        # æ”¯æŒè›‹ç™½è´¨å’ŒRNAè”åˆå»ºæ¨¡
```

### 2. å±‚æ¬¡åŒ–Token

```python
# æ‰©å±•ï¼šå¤šå°ºåº¦åŸºå› token
class HierarchicalVAR(VARST):
    def __init__(self, token_levels=[4096, 16384, 65536]):
        # æ”¯æŒä¸åŒåˆ†è¾¨ç‡çš„åŸºå› è®¡æ•°
```

### 3. æ—¶åºå»ºæ¨¡

```python
# æ‰©å±•ï¼šæ—¶é—´åºåˆ—ç©ºé—´è½¬å½•ç»„å­¦
class TemporalVAR(VARST):
    def __init__(self, time_steps=10):
        # æ”¯æŒæ—¶é—´åºåˆ—é¢„æµ‹
```

## ğŸ“ æ”¯æŒä¸è”ç³»

- ğŸ› **é—®é¢˜æŠ¥å‘Š**: è¯·åˆ›å»ºGitHub Issue
- ğŸ’¡ **åŠŸèƒ½å»ºè®®**: æ¬¢è¿æäº¤Pull Request
- ğŸ“§ **æŠ€æœ¯æ”¯æŒ**: ouyangjiarui@example.com

---

**ğŸ‰ æ­å–œï¼æ‚¨å·²ç»æŒæ¡äº†DGC-VARçš„å®Œæ•´ä½¿ç”¨æ–¹æ³•ã€‚å¼€å§‹æ‚¨çš„ç©ºé—´è½¬å½•ç»„å­¦ç ”ç©¶ä¹‹æ—…å§ï¼** 