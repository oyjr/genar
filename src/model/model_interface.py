"""
VAR-STæ¨¡å‹çš„PyTorch Lightningæ¥å£
ç²¾ç®€ç‰ˆæœ¬ï¼šä¿ç•™æ ¸å¿ƒåŠŸèƒ½ï¼Œåˆ é™¤å†—ä½™ä»£ç 
"""

# æ ‡å‡†åº“å¯¼å…¥
import os
import inspect
import importlib
import logging
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

# PyTorch Lightningç›¸å…³
import pytorch_lightning as pl

# Metrics
import torchmetrics
from torchmetrics.regression import (
    PearsonCorrCoef,
    MeanAbsoluteError,
    MeanSquaredError,
    ConcordanceCorrCoef,   
    R2Score,
)

# é¡¹ç›®å†…éƒ¨å¯¼å…¥
from addict import Dict as AddictDict

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

# é»˜è®¤å¸¸é‡
DEFAULT_NUM_GENES = 200
DEFAULT_LEARNING_RATE = 1e-4
DEFAULT_WEIGHT_DECAY = 0.0
DEFAULT_GRADIENT_CLIP = 1.0
MAX_SAVED_SAMPLES = 10000
LOG_FREQUENCY = 100
TOP_GENE_RATIO = 0.3
MIN_VARIANCE_THRESHOLD = 1e-8


class ModelInterface(pl.LightningModule):
    """VAR-STæ¨¡å‹çš„PyTorch Lightningæ¥å£"""

    def __init__(self, config):
        super().__init__()
        
        # åˆ›å»ºä¸“ç”¨logger
        self._logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # ä¿å­˜é…ç½®
        self.config = config
        self.save_hyperparameters()

        # åŠ è½½æ¨¡å‹é…ç½®å’Œæ¨¡å‹
        self.model_config = config.MODEL
        self._logger.info("åˆå§‹åŒ–VAR-STæ¨¡å‹æ¥å£")
        self.model = self._load_model()
        
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°ï¼ˆå®é™…åœ¨_compute_lossä¸­å®ç°æœŸæœ›å€¼æŸå¤±ï¼‰
        self.criterion = torch.nn.MSELoss()  # ä¿ç•™ä½œä¸ºå¤‡ç”¨
        self._logger.info("ä½¿ç”¨æœŸæœ›å€¼å›å½’æŸå¤±ï¼ˆåœ¨_compute_lossä¸­å®ç°ï¼‰")

        # åˆå§‹åŒ–è¾“å‡ºç¼“å­˜ï¼ˆåªç”¨äºéªŒè¯å’Œæµ‹è¯•ï¼‰
        self.val_outputs = []
        self.test_outputs = []
        self.validation_step_outputs = []

        # åˆå§‹åŒ–æŒ‡æ ‡
        self._init_metrics()

        # è·å–æ ‡å‡†åŒ–è®¾ç½®
        self.normalize = self._get_config('DATA.normalize', True)

    def _get_config(self, path: str, default=None):
        """å®‰å…¨åœ°è·å–é…ç½®å€¼"""
        parts = path.split('.')
        value = self.config
        
        try:
            for part in parts:
                if isinstance(value, dict):
                    value = value.get(part, default)
                elif hasattr(value, part):
                    value = getattr(value, part)
                else:
                    return default
            return value
        except Exception:
            return default

    def _load_model(self):
        """åŠ è½½Multi-Scale Gene VARæ¨¡å‹"""
        try:
            self._logger.info("åŠ è½½Multi-Scale Gene VARæ¨¡å‹...")
            Model = getattr(importlib.import_module(
                'model.VAR.two_stage_var_st'), 'MultiScaleGeneVAR')
            
            # å®ä¾‹åŒ–æ¨¡å‹
            model = self._instancialize(Model)
            self._logger.info("Multi-Scale Gene VARæ¨¡å‹åŠ è½½æˆåŠŸ")
            
            return model
            
        except Exception as e:
            self._logger.error(f"åŠ è½½Multi-Scale Gene VARæ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}")
            raise ValueError(f'Multi-Scale Gene VARæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}')

    def _instancialize(self, Model, **other_args):
        """å®ä¾‹åŒ–æ¨¡å‹"""
        try:
            # è·å–æ¨¡å‹åˆå§‹åŒ–å‚æ•°
            class_args = inspect.getfullargspec(Model.__init__).args[1:]
            
            # å¤„ç†ä¸åŒç±»å‹çš„é…ç½®å¯¹è±¡
            if isinstance(self.model_config, AddictDict):
                model_config_dict = dict(self.model_config)
            elif hasattr(self.model_config, '__dict__'):
                model_config_dict = vars(self.model_config)
            else:
                model_config_dict = self.model_config
            
            args = {}
            
            # ä»é…ç½®ä¸­è·å–å‚æ•°
            for arg in class_args:
                if arg in model_config_dict:
                    args[arg] = model_config_dict[arg]
                elif arg == 'config':
                    args[arg] = self.config
                elif arg == 'histology_feature_dim' and 'feature_dim' in model_config_dict:
                    args[arg] = model_config_dict['feature_dim']
                    
            # æ·»åŠ å…¶ä»–å‚æ•°
            args.update(other_args)
            
            return Model(**args)
            
        except Exception as e:
            self._logger.error(f"æ¨¡å‹å®ä¾‹åŒ–å¤±è´¥ï¼š{str(e)}")
            raise

    def _init_metrics(self):
        """åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡"""
        num_genes = self._get_config('MODEL.num_genes', DEFAULT_NUM_GENES)
        self._logger.info(f"VAR_STæ¨¡å‹ä½¿ç”¨åŸºå› æ•°é‡: {num_genes}")
        
        # åˆ›å»ºæŒ‡æ ‡é›†åˆ
        metrics = {
            'mse': MeanSquaredError(),
            'mae': MeanAbsoluteError(),
            'pearson': PearsonCorrCoef(num_outputs=num_genes),
            'concordance': ConcordanceCorrCoef(num_outputs=num_genes),
            'r2': R2Score(multioutput='uniform_average')
        }

        self.train_metrics = torchmetrics.MetricCollection(metrics.copy())
        self.val_metrics = torchmetrics.MetricCollection(metrics.copy())
        self.test_metrics = torchmetrics.MetricCollection(metrics.copy())

        # åˆ›å»ºè¯¦ç»†æŒ‡æ ‡
        self.val_detailed_metrics = self._create_detailed_metrics(num_genes)
        self.test_detailed_metrics = self._create_detailed_metrics(num_genes)

    def _common_step(self, batch, batch_idx, phase: str) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """é€šç”¨çš„stepå¤„ç†é€»è¾‘"""
        # é¢„å¤„ç†
        original_batch = batch.copy() if isinstance(batch, dict) else batch
        processed_batch = self._preprocess_inputs(batch)
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šéªŒè¯å’Œæµ‹è¯•æ—¶ç§»é™¤target_genesä»¥å¯ç”¨çœŸæ­£çš„æ¨ç†æ¨¡å¼
        if phase in ['val', 'test'] and 'target_genes' in processed_batch:
            # ä¿å­˜target_genesç”¨äºæŸå¤±è®¡ç®—ï¼Œä½†ä»æ¨¡å‹è¾“å…¥ä¸­ç§»é™¤
            _ = processed_batch.pop('target_genes')
        
        # å‰å‘ä¼ æ’­
        results_dict = self.model(**processed_batch)
        # è®¡ç®—æŸå¤±
        loss = self._compute_loss(results_dict, original_batch)
        # æå–é¢„æµ‹å’Œç›®æ ‡
        logits, target_genes = self._extract_predictions_and_targets(results_dict, original_batch)
        # ğŸ”§ è°¨æ…å¤„ç†æŒ‡æ ‡æ›´æ–°ï¼Œé¿å…éªŒè¯æ—¶çš„å¤šGPUåŒæ­¥é—®é¢˜
        should_log = (phase == 'train' and batch_idx % LOG_FREQUENCY == 0)
        if should_log:
            # åªåœ¨è®­ç»ƒæ—¶æ›´æ–°æŒ‡æ ‡ï¼ŒéªŒè¯æ—¶é¿å…è°ƒç”¨å¤æ‚çš„æŒ‡æ ‡è®¡ç®—
            self._update_metrics(phase, logits, target_genes)
        
        # è®°å½•æŸå¤± - é¿å…éªŒè¯æ—¶åŒæ­¥
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šéªŒè¯æ—¶ä¸åŒæ­¥ï¼Œé¿å…æ­»é”
        sync_dist = False  # å®Œå…¨ç¦ç”¨åŒæ­¥ï¼Œè®©Lightningåœ¨epoch endå¤„ç†
        batch_size = original_batch.get('target_genes', torch.empty(1)).size(0) if isinstance(original_batch, dict) else 1
        
        # åªåœ¨training_stepä¸­è®°å½•ï¼Œvalidation_stepè‡ªå·±å¤„ç†
        if phase == 'train':
            self.log(f'{phase}_loss', loss, 
                    on_step=True, 
                    on_epoch=True, 
                    prog_bar=True,
                    batch_size=batch_size,
                    sync_dist=sync_dist)
        
        # è®°å½•æ¨¡å‹ç‰¹å®šæŒ‡æ ‡
        self._log_model_specific_metrics(phase, results_dict)
        
        return loss, logits, target_genes

    def training_step(self, batch, batch_idx):
        """è®­ç»ƒæ­¥éª¤"""
        loss, _, _ = self._common_step(batch, batch_idx, 'train')
        return loss

    def validation_step(self, batch, batch_idx):
        """éªŒè¯æ­¥éª¤ - ä¿®å¤å¤šGPUåŒæ­¥é—®é¢˜"""
        # æ‰§è¡Œå®Œæ•´çš„éªŒè¯æ­¥éª¤
        loss, predictions, targets = self._common_step(batch, batch_idx, 'val')
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šåœ¨å¤šGPUç¯å¢ƒä¸‹æ­£ç¡®åŒæ­¥val_loss
        # è®°å½•éªŒè¯æŸå¤±ï¼ˆå¯ç”¨åŒæ­¥ä»¥ç¡®ä¿ModelCheckpointèƒ½è·å–æ­£ç¡®çš„å€¼ï¼‰
        self.log('val_loss', loss, 
                on_step=False, 
                on_epoch=True, 
                prog_bar=True,
                batch_size=targets.size(0) if hasattr(targets, 'size') else 1,
                sync_dist=True,  # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¯ç”¨åŒæ­¥ç¡®ä¿ModelCheckpointæ­£ç¡®å·¥ä½œ
                reduce_fx='mean')  # æ˜ç¡®æŒ‡å®šreduceå‡½æ•°
        
        # ğŸ”§ æ”¶é›†éªŒè¯è¾“å‡ºç”¨äºPCCè®¡ç®—
        output = {
            'val_loss': loss,
            'predictions': predictions.detach().cpu(),  # ç§»åˆ°CPUå‡å°‘GPUå†…å­˜
            'targets': targets.detach().cpu()
        }
        
        # æ·»åŠ åˆ°éªŒè¯è¾“å‡ºåˆ—è¡¨
        self.validation_step_outputs.append(output)
        
        return output

    def test_step(self, batch, batch_idx):
        """æµ‹è¯•æ­¥éª¤"""
        loss, logits, target_genes = self._common_step(batch, batch_idx, 'test')
        # ä¿å­˜è¾“å‡º
        self._save_step_outputs('test', loss, logits, target_genes, batch_idx)
        return {'logits': logits, 'target_genes': target_genes}
    
    def _preprocess_inputs(self, inputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """é¢„å¤„ç†è¾“å…¥æ•°æ®"""
        # éªŒè¯è¾“å…¥
        self._validate_inputs(inputs)
        processed = {}
        # ç»„ç»‡å­¦ç‰¹å¾å¤„ç†
        if 'img' in inputs:
            processed['histology_features'] = inputs['img']
        # ç©ºé—´åæ ‡å¤„ç†
        if 'positions' in inputs:
            processed['spatial_coords'] = inputs['positions']
        # åŸºå› è¡¨è¾¾æ•°æ®å¤„ç† - ä¿ç•™åŸå§‹é€»è¾‘ï¼Œè®©_common_stepå¤„ç†æ¨ç†é€»è¾‘
        if 'target_genes' in inputs:
            processed['target_genes'] = inputs['target_genes']
        return processed

    def _validate_inputs(self, inputs: Dict[str, torch.Tensor]):
        """éªŒè¯è¾“å…¥æ•°æ®"""
        # æ£€æŸ¥å¿…éœ€çš„é”®
        if 'img' not in inputs:
            raise ValueError("ç¼ºå°‘å¿…éœ€çš„è¾“å…¥: img")
        
        # å®šä¹‰ä¸åŒå­—æ®µçš„æœŸæœ›ç»´åº¦
        expected_dims = {
            'img': [2, 3],           # å›¾åƒç‰¹å¾: (batch, features) æˆ– (batch, seq, features)
            'target_genes': [2, 3],   # åŸºå› è¡¨è¾¾: (batch, genes) æˆ– (batch, seq, genes)
            'positions': [2, 3],      # ç©ºé—´åæ ‡: (batch, coords) æˆ– (batch, seq, coords)
            'spot_idx': [1, 2],       # spotç´¢å¼•: (batch,) æˆ– (batch, seq)
            'slide_id': [1],          # slideæ ‡è¯†: (batch,)
            'gene_ids': [1, 2],       # åŸºå› ID: (batch,) æˆ– (batch, genes)
        }
        
        # éªŒè¯å¼ é‡å½¢çŠ¶
        for key, tensor in inputs.items():
            if isinstance(tensor, torch.Tensor):
                # è·å–è¯¥å­—æ®µçš„æœŸæœ›ç»´åº¦ï¼Œå¦‚æœæœªå®šä¹‰åˆ™å…è®¸1-3ç»´
                allowed_dims = expected_dims.get(key, [1, 2, 3])
                
                if tensor.dim() not in allowed_dims:
                    raise ValueError(f"{key}ç»´åº¦é”™è¯¯: {tensor.shape}ï¼ŒæœŸæœ›ç»´åº¦: {allowed_dims}")
        
        # éªŒè¯æ•°å€¼èŒƒå›´
        if 'target_genes' in inputs:
            targets = inputs['target_genes']
            if (targets < 0).any():
                raise ValueError("ç›®æ ‡åŸºå› è¡¨è¾¾å€¼åŒ…å«è´Ÿæ•°")

    def _compute_loss(self, outputs: Dict[str, torch.Tensor], batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """ä½¿ç”¨æœŸæœ›å€¼å›å½’æŸå¤±æ›¿ä»£äº¤å‰ç†µæŸå¤±"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰logitsè¾“å‡ºï¼ˆç”¨äºæœŸæœ›å€¼æŸå¤±ï¼‰
            if 'logits' in outputs and ('full_target' in outputs or 'target_genes' in batch):
                # ä½¿ç”¨æœŸæœ›å€¼å›å½’æŸå¤±
                logits = outputs['logits']  # [B, seq_len, vocab_size] æˆ– [B*seq_len, vocab_size]
                
                # ä¼˜å…ˆä½¿ç”¨full_targetï¼ˆVARæ¨¡å‹çš„å¤šå°ºåº¦ç›®æ ‡ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨target_genes
                if 'full_target' in outputs:
                    targets = outputs['full_target']  # [B, seq_len] - VARæ¨¡å‹çš„å¤šå°ºåº¦ç›®æ ‡
                else:
                    targets = batch['target_genes']   # [B, seq_len] - æ ‡å‡†ç›®æ ‡
                
                # ç¡®ä¿ç»´åº¦åŒ¹é…
                if logits.dim() == 3:
                    B, seq_len, V = logits.shape
                    logits = logits.view(-1, V)
                    targets = targets.view(-1)
                
                # åˆ›å»ºtokenåˆ°log2è¿ç»­å€¼çš„æ˜ å°„
                vocab_size = logits.shape[-1]
                token_values = torch.log2(torch.arange(vocab_size, dtype=torch.float32, device=logits.device) + 1.0)
                
                # è®¡ç®—çœŸå®è¿ç»­å€¼
                true_continuous = token_values[targets]
                
                # è®¡ç®—æœŸæœ›è¿ç»­å€¼
                probs = F.softmax(logits, dim=-1)
                expected_continuous = torch.sum(probs * token_values[None, :], dim=-1)
                
                # ä½¿ç”¨MSEæŸå¤±ï¼ˆä¸è¯„ä¼°æŒ‡æ ‡ä¸€è‡´ï¼‰
                total_loss = F.mse_loss(expected_continuous, true_continuous)
                
                # è®°å½•é¢å¤–æŒ‡æ ‡
                with torch.no_grad():
                    token_acc = (logits.argmax(dim=-1) == targets).float().mean()
                    self.log('train_token_accuracy', token_acc, prog_bar=False, sync_dist=False)
                    self.log('train_expected_mse', total_loss.detach(), prog_bar=True, sync_dist=False)
                
                self._logger.debug(f"æœŸæœ›å€¼å›å½’æŸå¤±={total_loss:.4f}")
                
            elif 'predictions' in outputs and 'target_genes' in batch:
                # æ¨ç†æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨é¢„æµ‹çš„token IDs
                predictions = outputs['predictions']  # [B, 200] token IDs
                targets = batch['target_genes']       # [B, 200] token IDs
                predictions_log2 = torch.log2(predictions.float() + 1.0)
                targets_log2 = torch.log2(targets.float() + 1.0)
                total_loss = F.mse_loss(predictions_log2, targets_log2)
                self._logger.debug(f"æ¨ç†æ¨¡å¼ï¼šä½¿ç”¨log2(x+1)å˜æ¢åçš„MSEæŸå¤±={total_loss:.4f}")
                
            elif 'loss' in outputs:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ¨¡å‹å†…éƒ¨æŸå¤±
                total_loss = outputs['loss']
                self._logger.warning("ä½¿ç”¨æ¨¡å‹å†…éƒ¨äº¤å‰ç†µæŸå¤±ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰")
                
            else:
                raise KeyError("æ— æ³•è®¡ç®—æŸå¤±ï¼šç¼ºå°‘å¿…è¦çš„è¾“å‡º")
            
            # éªŒè¯æŸå¤±å€¼
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                self._logger.error(f"æŸå¤±å€¼å¼‚å¸¸: {total_loss.item()}")
                raise ValueError("æŸå¤±å€¼ä¸ºNaNæˆ–Inf")
                
            return total_loss
            
        except Exception as e:
            self._logger.error(f"è®¡ç®—æŸå¤±æ—¶å‡ºé”™: {str(e)}")
            self._logger.error(f"è¾“å‡ºé”®: {list(outputs.keys())}")
            raise

    def _extract_predictions_and_targets(self, results_dict: Dict[str, torch.Tensor], 
                                       batch: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:
        """æå–é¢„æµ‹å’Œç›®æ ‡"""
        # è·å–é¢„æµ‹
        if 'predictions' in results_dict:
            logits = results_dict['predictions']
        else:
            logits = results_dict.get('generated_sequence', None)
            if logits is None:
                raise ValueError("Multi-Scale Gene VARæ¨¡å‹åº”è¯¥æœ‰predictionsæˆ–generated_sequenceè¾“å‡º")
        
        # è·å–ç›®æ ‡
        if 'target_genes' not in batch:
            raise ValueError("æ‰¹æ¬¡æ•°æ®ä¸­æ‰¾ä¸åˆ°target_genes")
        target_genes = batch['target_genes']
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šVAR_STæ¨¡å‹åº”è¯¥ç›´æ¥è¿”å›200ä¸ªåŸºå› çš„é¢„æµ‹
        # å¦‚æœå½¢çŠ¶ä¸åŒ¹é…ï¼Œè¯´æ˜æ¨¡å‹å®ç°æœ‰é—®é¢˜ï¼Œç›´æ¥æŠ¥é”™
        num_genes = target_genes.shape[-1]  # é€šå¸¸æ˜¯200
        
        if logits.shape[-1] != num_genes:
            raise ValueError(
                f"æ¨¡å‹é¢„æµ‹ç»´åº¦({logits.shape[-1]})ä¸ç›®æ ‡åŸºå› æ•°é‡({num_genes})ä¸åŒ¹é…ï¼"
                f"è¿™è¡¨æ˜è®­ç»ƒå’Œæ¨ç†çš„æ¨¡å‹é…ç½®ä¸ä¸€è‡´ã€‚"
                f"é¢„æµ‹å½¢çŠ¶: {logits.shape}, ç›®æ ‡å½¢çŠ¶: {target_genes.shape}"
            )
        
        # è®­ç»ƒæ—¶ç›´æ¥ä½¿ç”¨åŸå§‹è®¡æ•°å€¼ï¼Œä¸è¿›è¡Œlog2å˜æ¢
        # è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ—¶ä¼šåœ¨éœ€è¦çš„åœ°æ–¹è¿›è¡Œlog2å˜æ¢
        return logits.float(), target_genes.float()

    def _apply_log2_normalization_for_evaluation(self, predictions: torch.Tensor, 
                                                targets: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """ä¸ºè¯„ä¼°æŒ‡æ ‡åº”ç”¨log2(x+1)æ ‡å‡†åŒ– - åªåœ¨è¯„ä¼°æ—¶ä½¿ç”¨"""
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat
        predictions = predictions.float()
        targets = targets.float()
        
        # åº”ç”¨log2(x+1)æ ‡å‡†åŒ–
        predictions_log2 = torch.log2(predictions + 1.0)
        targets_log2 = torch.log2(targets + 1.0)
        
        # éªŒè¯ç»“æœ
        if torch.isnan(predictions_log2).any() or torch.isnan(targets_log2).any():
            self._logger.warning("Log2æ ‡å‡†åŒ–åå‘ç°NaNå€¼")
        
        return predictions_log2, targets_log2

    def _update_metrics(self, stage: str, predictions: torch.Tensor, targets: torch.Tensor):
        """æ›´æ–°è¯„ä¼°æŒ‡æ ‡"""
        try:
            # è·å–å¯¹åº”é˜¶æ®µçš„æŒ‡æ ‡é›†åˆ
            metrics = getattr(self, f'{stage}_metrics')
            
            # ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®
            if predictions.dim() == 3:
                B, N, G = predictions.shape
                predictions = predictions.reshape(-1, G)
            if targets.dim() == 3:
                B, N, G = targets.shape
                targets = targets.reshape(-1, G)
            
            # ä¸ºäº†ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”ï¼Œåœ¨è®¡ç®—è¯„ä¼°æŒ‡æ ‡æ—¶åº”ç”¨log2å˜æ¢
            predictions_log2, targets_log2 = self._apply_log2_normalization_for_evaluation(predictions, targets)
            
            # è®­ç»ƒé˜¶æ®µæ¯æ¬¡éƒ½é‡ç½®æŒ‡æ ‡
            if stage == 'train':
                metrics.reset()
            
            # ä½¿ç”¨log2å˜æ¢åçš„å€¼æ›´æ–°æŒ‡æ ‡
            metrics.update(predictions_log2, targets_log2)

            # è®¡ç®—å¹¶è®°å½•æŒ‡æ ‡
            if stage == 'train' or self.trainer.global_step % LOG_FREQUENCY == 0:
                self._log_metrics(stage, metrics, predictions.size(0))

        except Exception as e:
            self._logger.error(f"æ›´æ–°æŒ‡æ ‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise

    def _log_metrics(self, stage: str, metrics: torchmetrics.MetricCollection, batch_size: int):
        """è®°å½•æŒ‡æ ‡"""
        metric_dict = metrics.compute()
        
        for name, value in metric_dict.items():
            if isinstance(value, torch.Tensor):
                if value.numel() > 1:
                    # å¤šå…ƒç´ å¼ é‡
                    values = torch.nan_to_num(value, nan=0.0, posinf=1e6, neginf=-1e6)
                    mean_value = values.mean()
                    std_value = values.std() if values.numel() > 1 else torch.tensor(0.0, device=value.device)
                    
                    # åªåœ¨è¿›åº¦æ¡æ˜¾ç¤ºé‡è¦æŒ‡æ ‡
                    show_in_prog_bar = name in ['mse', 'mae']
                    
                    self.log(f'{stage}_{name}', mean_value, 
                            prog_bar=show_in_prog_bar, 
                            batch_size=batch_size, 
                            sync_dist=True)
                    self.log(f'{stage}_{name}_std', std_value, 
                            prog_bar=False, 
                            batch_size=batch_size, 
                            sync_dist=True)
                    
                    # è®°å½•é«˜ç›¸å…³æ€§åŸºå› 
                    if name in ['pearson', 'concordance']:
                        self._log_high_correlation_genes(stage, name, values, batch_size)
                else:
                    # å•å…ƒç´ å¼ é‡
                    self.log(f'{stage}_{name}', value.item(), 
                            prog_bar=(name in ['mse', 'mae']), 
                            batch_size=batch_size, 
                            sync_dist=True)

    def _log_high_correlation_genes(self, stage: str, metric_name: str, 
                                   values: torch.Tensor, batch_size: int):
        """è®°å½•é«˜ç›¸å…³æ€§åŸºå› çš„ç»Ÿè®¡"""
        top_k = max(1, int(len(values) * TOP_GENE_RATIO))
        high_values = torch.topk(values, top_k)[0]
        high_mean = high_values.mean()
        high_std = high_values.std() if high_values.numel() > 1 else torch.tensor(0.0, device=values.device)
        
        self.log(f'{stage}_{metric_name}_high_mean', high_mean, 
                prog_bar=False, 
                batch_size=batch_size, 
                sync_dist=True)
        self.log(f'{stage}_{metric_name}_high_std', high_std, 
                prog_bar=False, 
                batch_size=batch_size, 
                sync_dist=True)

    def _log_model_specific_metrics(self, phase: str, results_dict: Dict[str, Any]):
        """è®°å½•æ¨¡å‹ç‰¹å®šçš„æŒ‡æ ‡"""
        # ğŸ”§ å‡å°‘åŒæ­¥ï¼Œåªåœ¨è®­ç»ƒæ—¶è®°å½•è¯¦ç»†æŒ‡æ ‡
        if phase == 'train':
            # VAR-STçš„ç‰¹å®šæŒ‡æ ‡
            if 'accuracy' in results_dict:
                self.log(f'{phase}_accuracy', results_dict['accuracy'], 
                        on_epoch=True, 
                        sync_dist=False)
            
            if 'perplexity' in results_dict:
                self.log(f'{phase}_perplexity', results_dict['perplexity'], 
                        on_epoch=True, 
                        sync_dist=False)
            
            if 'top5_accuracy' in results_dict:
                self.log(f'{phase}_top5_accuracy', results_dict['top5_accuracy'], 
                        on_epoch=True, 
                        sync_dist=False)

    def _save_step_outputs(self, phase: str, loss: torch.Tensor, 
                          preds: torch.Tensor, targets: torch.Tensor, 
                          batch_idx: Optional[int] = None):
        """ä¿å­˜æ­¥éª¤è¾“å‡º"""
        if phase == 'train':
            return  # è®­ç»ƒé˜¶æ®µä¸ä¿å­˜
        
        # æ£€æŸ¥å†…å­˜é™åˆ¶
        current_samples = sum(out['preds'].shape[0] for out in getattr(self, f'{phase}_outputs'))
        if current_samples >= MAX_SAVED_SAMPLES:
            if batch_idx == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªbatchæ—¶è­¦å‘Š
                self._logger.warning(f"{phase}é˜¶æ®µå·²ä¿å­˜{current_samples}ä¸ªæ ·æœ¬ï¼Œè¾¾åˆ°ä¸Šé™")
            return
        
        output_dict = {
            'loss': loss.detach().cpu().item(),
            'preds': preds.detach().cpu(),
            'targets': targets.detach().cpu(),
        }
        if batch_idx is not None:
            output_dict['batch_idx'] = batch_idx

        getattr(self, f'{phase}_outputs').append(output_dict)

    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        weight_decay = float(self._get_config('TRAINING.weight_decay', DEFAULT_WEIGHT_DECAY))
        learning_rate = float(self._get_config('TRAINING.learning_rate', DEFAULT_LEARNING_RATE))
        
        # å¤šGPUå­¦ä¹ ç‡ç¼©æ”¾
        learning_rate = self._scale_learning_rate(learning_rate)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        # è®¾ç½®æ¢¯åº¦è£å‰ª
        self.trainer.gradient_clip_val = self._get_config('TRAINING.gradient_clip_val', DEFAULT_GRADIENT_CLIP)
        
        # é…ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_config = self._get_scheduler_config(optimizer)
        
        if scheduler_config:
            return {'optimizer': optimizer, 'lr_scheduler': scheduler_config}
        else:
            return {'optimizer': optimizer}

    def _scale_learning_rate(self, base_lr: float) -> float:
        """æ ¹æ®GPUæ•°é‡ç¼©æ”¾å­¦ä¹ ç‡"""
        num_devices = self._get_config('devices', 1)
        if num_devices <= 1:
            return base_lr
        
        scaling_strategy = self._get_config('MULTI_GPU.lr_scaling', 'none')
        
        if scaling_strategy == 'linear':
            scaled_lr = base_lr * num_devices
            self._logger.info(f"çº¿æ€§ç¼©æ”¾å­¦ä¹ ç‡: {scaled_lr} (åŸå§‹: {base_lr}, è®¾å¤‡æ•°: {num_devices})")
        elif scaling_strategy == 'sqrt':
            scaled_lr = base_lr * (num_devices ** 0.5)
            self._logger.info(f"å¹³æ–¹æ ¹ç¼©æ”¾å­¦ä¹ ç‡: {scaled_lr} (åŸå§‹: {base_lr}, è®¾å¤‡æ•°: {num_devices})")
        else:
            scaled_lr = base_lr
            self._logger.info(f"ä¸ç¼©æ”¾å­¦ä¹ ç‡: {scaled_lr}")
        
        return scaled_lr

    def _get_scheduler_config(self, optimizer):
        """è·å–å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®"""
        # è·å–é…ç½®å‚æ•°
        factor = self._get_config('TRAINING.lr_scheduler.factor', 0.5)
        patience = self._get_config('TRAINING.lr_scheduler.patience', 5)
        mode = self._get_config('TRAINING.lr_scheduler.mode', 'min')
        
        if patience == 0:
            self._logger.info("å­¦ä¹ ç‡è°ƒåº¦å™¨å·²ç¦ç”¨")
            return None
        
        return {
            'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode=mode,
                factor=factor,
                patience=patience,
                verbose=True
            ),
            'monitor': self._get_config('TRAINING.monitor', 'val_loss'),
            'interval': 'epoch',
            'frequency': 1
        }

    def on_train_epoch_end(self):
        """è®­ç»ƒepochç»“æŸæ—¶çš„å›è°ƒ"""
        pass  # è®­ç»ƒæ•°æ®ä¸å†ç´¯ç§¯
    
    def on_validation_epoch_end(self):
        """éªŒè¯epochç»“æŸæ—¶çš„å›è°ƒ - è®¡ç®—å’Œæ‰“å°PCCæŒ‡æ ‡"""
        
        # ğŸ”§ æ”¶é›†éªŒè¯æ•°æ®å¹¶è®¡ç®—PCCæŒ‡æ ‡
        if hasattr(self, 'validation_step_outputs') and self.validation_step_outputs:
            try:
                # æ”¶é›†æ‰€æœ‰éªŒè¯æ•°æ®
                all_predictions = []
                all_targets = []
                
                for output in self.validation_step_outputs:
                    all_predictions.append(output['predictions'])
                    all_targets.append(output['targets'])
                
                # åˆå¹¶æ•°æ®
                predictions = torch.cat(all_predictions, dim=0)  # [N, genes]
                targets = torch.cat(all_targets, dim=0)  # [N, genes]
                
                # è®¡ç®—PCCæŒ‡æ ‡
                pcc_metrics = self._calculate_comprehensive_pcc_metrics(predictions, targets)
                
                # è®°å½•PCCæŒ‡æ ‡åˆ°wandb
                for metric_name, value in pcc_metrics.items():
                    self.log(f'val_{metric_name}', value, on_epoch=True, prog_bar=False, sync_dist=True)
                
                # åœ¨ä¸»è¿›ç¨‹æ‰“å°è¯¦ç»†ç»“æœ
                if self.trainer.is_global_zero:
                    val_loss = self.trainer.callback_metrics.get('val_loss', 0.0)
                    print(f"\nğŸ¯ Epoch {self.current_epoch} éªŒè¯ç»“æœ:")
                    print(f"   Loss: {val_loss:.6f}")
                    print(f"   PCC-10:  {pcc_metrics['pcc_10']:.4f}")
                    print(f"   PCC-50:  {pcc_metrics['pcc_50']:.4f}")
                    print(f"   PCC-200: {pcc_metrics['pcc_200']:.4f}")
                    print(f"   MSE:     {pcc_metrics['mse']:.6f}")
                    print(f"   MAE:     {pcc_metrics['mae']:.6f}")
                    print(f"   RVD:     {pcc_metrics['rvd']:.6f}")
                    print()
                
                # æ¸…ç†éªŒè¯è¾“å‡ºæ•°æ®
                self.validation_step_outputs.clear()
                
            except Exception as e:
                self._logger.error(f"è®¡ç®—PCCæŒ‡æ ‡æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
        
        # æ¸…ç†éªŒè¯æ•°æ®ï¼ˆå®‰å…¨æ“ä½œï¼‰
        self._cleanup_validation_data()
            
        # ğŸ”§ é‡ç½®éªŒè¯æŒ‡æ ‡ä»¥é‡Šæ”¾å†…å­˜
    
    def _cleanup_validation_data(self):
        """å®‰å…¨åœ°æ¸…ç†éªŒè¯ç›¸å…³æ•°æ®"""
        # æ¸…ç©ºéªŒè¯è¾“å‡º
        if hasattr(self, 'val_outputs'):
            self.val_outputs.clear()
        if hasattr(self, '_collected_predictions'):
            self._collected_predictions.clear()
        if hasattr(self, '_collected_targets'):
            self._collected_targets.clear()
            
        # é‡ç½®éªŒè¯æŒ‡æ ‡ï¼ˆè¿™ä¸ªæ“ä½œæ˜¯å®‰å…¨çš„ï¼‰
        if hasattr(self, 'val_metrics'):
            try:
                self.val_metrics.reset()
            except Exception:
                pass  # å¦‚æœé‡ç½®å¤±è´¥å°±å¿½ç•¥
        
        # ğŸ”§ ç¡®ä¿éªŒè¯æŒ‡æ ‡æ­£ç¡®é‡ç½®
        try:
            if hasattr(self, 'val_metrics'):
                self.val_metrics.reset()
        except Exception:
            pass  # å¿½ç•¥é‡ç½®é”™è¯¯

    def on_test_epoch_end(self):
        """æµ‹è¯•epochç»“æŸæ—¶çš„å›è°ƒ"""
        self._compute_and_log_evaluation_metrics('test')
        
        # æ¸…ç©ºæµ‹è¯•è¾“å‡º
        if self.current_epoch < self.trainer.max_epochs - 1:
            self.test_outputs.clear()
    
    def on_fit_end(self):
        """è®­ç»ƒå®Œæˆæ—¶çš„å›è°ƒ"""
        if not self.trainer.is_global_zero:
            self._logger.info(f"GPUè¿›ç¨‹ {self.trainer.global_rank}: è®­ç»ƒå®Œæˆ")
            return
        
        self._logger.info("è®­ç»ƒå®Œæˆï¼")

    def _compute_and_log_evaluation_metrics(self, phase: str):
        """è®¡ç®—å¹¶è®°å½•è¯„ä¼°æŒ‡æ ‡"""
        outputs = getattr(self, f'{phase}_outputs', [])
        
        if not outputs:
            self._logger.warning(f"æ²¡æœ‰{phase}é˜¶æ®µçš„è¾“å‡ºæ•°æ®")
            return
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºsanity check
        if hasattr(self.trainer, 'sanity_checking') and self.trainer.sanity_checking:
            self._logger.info("è·³è¿‡sanity checké˜¶æ®µçš„è¯¦ç»†è¯„ä¼°")
            return
       
        self._logger.info(f"å¼€å§‹è®¡ç®—{phase}é˜¶æ®µçš„è¯„ä¼°æŒ‡æ ‡...")
        
        # æ”¶é›†æ‰€æœ‰è¾“å‡º
        all_predictions, all_targets = self._collect_outputs(outputs)
        
        # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        predictions = all_predictions.to(self.device)
        targets = all_targets.to(self.device)
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæµ‹è¯•é˜¶æ®µä¹Ÿéœ€è¦åº”ç”¨log2å˜æ¢æ¥è®¡ç®—æŒ‡æ ‡
        # ä¸ºäº†ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”ï¼Œåœ¨è®¡ç®—è¯„ä¼°æŒ‡æ ‡æ—¶åº”ç”¨log2å˜æ¢
        predictions_log2, targets_log2 = self._apply_log2_normalization_for_evaluation(predictions, targets)
        
        # ä½¿ç”¨TorchMetricsè®¡ç®—æ ‡å‡†æŒ‡æ ‡ï¼ˆä½¿ç”¨log2å˜æ¢åçš„å€¼ï¼‰
        metrics = getattr(self, f'{phase}_metrics')
        metrics.reset()
        metrics.update(predictions_log2, targets_log2)
        metric_dict = metrics.compute()
        
        # è®°å½•æ ‡å‡†æŒ‡æ ‡
        self._log_evaluation_metrics(phase, metric_dict)
        
        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡ï¼ˆä¹Ÿä½¿ç”¨log2å˜æ¢åçš„å€¼ï¼‰
        if hasattr(self, f'{phase}_detailed_metrics'):
            detailed_metrics = getattr(self, f'{phase}_detailed_metrics')
            detailed_metrics.reset()
            detailed_metrics.update(predictions_log2, targets_log2)
            detailed_dict = detailed_metrics.compute()
            self._log_detailed_metrics(phase, detailed_dict)
        
        # åœ¨ä¸»è¿›ç¨‹ä¸Šç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        if self.trainer.is_global_zero:
            self._generate_simple_evaluation_report(phase, metric_dict)

    def _log_evaluation_metrics(self, phase: str, metric_dict: Dict[str, torch.Tensor]):
        """è®°å½•è¯„ä¼°æŒ‡æ ‡"""
        processed_metrics = {}
        
        for name, value in metric_dict.items():
            if isinstance(value, torch.Tensor):
                if value.numel() > 1:
                    # å¤šå…ƒç´ å¼ é‡
                    values = torch.nan_to_num(value, nan=0.0, posinf=1e6, neginf=-1e6)
                    mean_value = values.mean()
                    std_value = values.std() if values.numel() > 1 else torch.tensor(0.0, device=value.device)
                    
                    # è®°å½•æŒ‡æ ‡ï¼Œç¡®ä¿å¤šGPUåŒæ­¥
                    show_in_prog_bar = name in ['mse', 'mae']
                    self.log(f'{phase}_{name}', mean_value, on_epoch=True, prog_bar=show_in_prog_bar, sync_dist=True, reduce_fx='mean')
                    self.log(f'{phase}_{name}_std', std_value, on_epoch=True, prog_bar=False, sync_dist=True, reduce_fx='mean')
                    
                    processed_metrics[name] = mean_value.item()
                    
                    # é«˜ç›¸å…³æ€§åŸºå› ç»Ÿè®¡
                    if name in ['pearson', 'concordance']:
                        self._log_high_correlation_genes(phase, name, values, 1)
                else:
                    # å•å…ƒç´ å¼ é‡
                    scalar_value = value.item()
                    self.log(f'{phase}_{name}', scalar_value, on_epoch=True, 
                            prog_bar=(name in ['mse', 'mae']), sync_dist=True, reduce_fx='mean')
                    processed_metrics[name] = scalar_value
        
        return processed_metrics

    def _log_detailed_metrics(self, phase: str, detailed_dict: Dict[str, torch.Tensor]):
        """è®°å½•è¯¦ç»†æŒ‡æ ‡"""
        for name, value in detailed_dict.items():
            if isinstance(value, torch.Tensor):
                scalar_value = value.item() if value.numel() == 1 else value.mean().item()
                self.log(f'{phase}_{name}', scalar_value, on_epoch=True, prog_bar=False, sync_dist=True)

    def _generate_simple_evaluation_report(self, phase: str, metric_dict: Dict[str, torch.Tensor]):
        """ç”Ÿæˆç®€åŒ–çš„è¯„ä¼°æŠ¥å‘Š"""
        self._logger.info("=" * 40)
        self._logger.info(f"{phase.upper()} è¯„ä¼°ç»“æœ")
        self._logger.info("=" * 40)
        
        for name, value in metric_dict.items():
            if isinstance(value, torch.Tensor):
                if value.numel() > 1:
                    mean_val = torch.nan_to_num(value, nan=0.0).mean().item()
                    self._logger.info(f"  {name}: {mean_val:.4f}")
                else:
                    self._logger.info(f"  {name}: {value.item():.4f}")
        
        self._logger.info("=" * 40)

    def _collect_outputs(self, outputs: List[Dict]) -> Tuple[torch.Tensor, torch.Tensor]:
        """æ”¶é›†å¹¶åˆå¹¶è¾“å‡º"""
        all_preds = []
        all_targets = []
        
        for output in outputs:
            preds = output['preds']
            targets = output['targets']
            
            # ç¡®ä¿ç»´åº¦æ­£ç¡®
            if preds.dim() == 3:
                preds = preds.reshape(-1, preds.size(-1))
            if targets.dim() == 3:
                targets = targets.reshape(-1, targets.size(-1))
                
            all_preds.append(preds)
            all_targets.append(targets)
        
        return torch.cat(all_preds, dim=0), torch.cat(all_targets, dim=0)

    def _create_detailed_metrics(self, num_genes: int):
        """åˆ›å»ºè¯¦ç»†æŒ‡æ ‡è®¡ç®—å™¨"""
        from torchmetrics import Metric
        
        class DetailedMetrics(Metric):
            def __init__(self, num_genes):
                super().__init__()
                self.num_genes = num_genes
                # æ·»åŠ çŠ¶æ€å¼ é‡
                self.add_state("preds_sum", default=torch.zeros(num_genes), dist_reduce_fx="sum")
                self.add_state("targets_sum", default=torch.zeros(num_genes), dist_reduce_fx="sum")
                self.add_state("preds_sq_sum", default=torch.zeros(num_genes), dist_reduce_fx="sum")
                self.add_state("targets_sq_sum", default=torch.zeros(num_genes), dist_reduce_fx="sum")
                self.add_state("preds_targets_sum", default=torch.zeros(num_genes), dist_reduce_fx="sum")
                self.add_state("total", default=torch.tensor(0), dist_reduce_fx="sum")
            
            def update(self, preds: torch.Tensor, targets: torch.Tensor):
                # ç¡®ä¿ç»´åº¦æ­£ç¡®
                if preds.dim() == 3:
                    preds = preds.reshape(-1, preds.size(-1))
                if targets.dim() == 3:
                    targets = targets.reshape(-1, targets.size(-1))
                
                batch_size = preds.size(0)
                
                # ç´¯ç§¯ç»Ÿè®¡é‡
                self.preds_sum += preds.sum(dim=0)
                self.targets_sum += targets.sum(dim=0)
                self.preds_sq_sum += (preds ** 2).sum(dim=0)
                self.targets_sq_sum += (targets ** 2).sum(dim=0)
                self.preds_targets_sum += (preds * targets).sum(dim=0)
                self.total += batch_size
            
            def compute(self):
                # è®¡ç®—æ¯ä¸ªåŸºå› çš„ç›¸å…³ç³»æ•°
                n = self.total.float()
                
                # è®¡ç®—å‡å€¼
                preds_mean = self.preds_sum / n
                targets_mean = self.targets_sum / n
                
                # è®¡ç®—åæ–¹å·®å’Œæ–¹å·®
                covariance = (self.preds_targets_sum / n) - (preds_mean * targets_mean)
                preds_var = (self.preds_sq_sum / n) - (preds_mean ** 2)
                targets_var = (self.targets_sq_sum / n) - (targets_mean ** 2)
                
                # è®¡ç®—ç›¸å…³ç³»æ•°
                correlations = covariance / torch.sqrt(preds_var * targets_var + 1e-8)
                correlations = torch.nan_to_num(correlations, nan=0.0)
                
                # è®¡ç®—PCCæŒ‡æ ‡
                sorted_corr, _ = torch.sort(correlations, descending=True)
                
                pcc_10 = sorted_corr[:10].mean() if self.num_genes >= 10 else sorted_corr.mean()
                pcc_50 = sorted_corr[:50].mean() if self.num_genes >= 50 else sorted_corr.mean()
                pcc_200 = sorted_corr[:200].mean() if self.num_genes >= 200 else sorted_corr.mean()
                
                return {
                    'pcc_10': pcc_10,
                    'pcc_50': pcc_50,
                    'pcc_200': pcc_200,
                    'correlations_mean': correlations.mean()
                }
        
        return DetailedMetrics(num_genes)

    def calculate_gene_correlations(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        """è®¡ç®—åŸºå› çº§åˆ«çš„ç›¸å…³ç³»æ•°"""
        num_genes = y_true.shape[1]
        correlations = np.zeros(num_genes)
        
        for i in range(num_genes):
            true_gene = y_true[:, i]
            pred_gene = y_pred[:, i]
            
            # å¤„ç†å¸¸æ•°å€¼
            if np.std(true_gene) == 0 or np.std(pred_gene) == 0:
                correlations[i] = 0.0
            else:
                corr = np.corrcoef(true_gene, pred_gene)[0, 1]
                correlations[i] = 0.0 if np.isnan(corr) else corr
        
        return correlations

    def calculate_evaluation_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—ç»¼åˆè¯„ä¼°æŒ‡æ ‡"""
        # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„
        if torch.is_tensor(y_true):
            y_true = y_true.cpu().numpy()
        if torch.is_tensor(y_pred):
            y_pred = y_pred.cpu().numpy()
        
        # è®¡ç®—åŸºå› ç›¸å…³æ€§
        correlations = self.calculate_gene_correlations(y_true, y_pred)
        
        # æ’åºç›¸å…³æ€§
        sorted_corr = np.sort(correlations)[::-1]
        
        # è®¡ç®—PCCæŒ‡æ ‡
        pcc_10 = np.mean(sorted_corr[:10]) if len(sorted_corr) >= 10 else np.mean(sorted_corr)
        pcc_50 = np.mean(sorted_corr[:50]) if len(sorted_corr) >= 50 else np.mean(sorted_corr)
        pcc_200 = np.mean(sorted_corr[:200]) if len(sorted_corr) >= 200 else np.mean(sorted_corr)
        
        # è®¡ç®—MSEå’ŒMAE
        mse = np.mean((y_true - y_pred) ** 2)
        mae = np.mean(np.abs(y_true - y_pred))
        
        # è®¡ç®—RVD
        pred_var = np.var(y_pred, axis=0)
        true_var = np.var(y_true, axis=0)
        
        valid_mask = true_var > MIN_VARIANCE_THRESHOLD
        if np.sum(valid_mask) > 0:
            rvd = np.mean(((pred_var[valid_mask] - true_var[valid_mask]) ** 2) / (true_var[valid_mask] ** 2))
        else:
            rvd = 0.0
        
        return {
            'PCC-10': float(pcc_10),
            'PCC-50': float(pcc_50), 
            'PCC-200': float(pcc_200),
            'MSE': float(mse),
            'MAE': float(mae),
            'RVD': float(rvd),
            'correlations': correlations
        }

    def on_before_optimizer_step(self, optimizer):
        """ä¼˜åŒ–å™¨æ­¥éª¤å‰çš„å›è°ƒ"""
        grad_norm = torch.nn.utils.clip_grad_norm_(
            self.parameters(), 
            self.trainer.gradient_clip_val
        )
        
        self.log('grad_norm', grad_norm, sync_dist=True)

    def _calculate_pcc_metrics(self, val_metrics):
        """ä»éªŒè¯æœŸé—´æ”¶é›†çš„æ•°æ®è®¡ç®—PCC-10, PCC-50, PCC-200 - å®‰å…¨ç‰ˆæœ¬"""
        pcc_metrics = {}
        
        # ğŸ”§ æš‚æ—¶ç¦ç”¨PCCè®¡ç®—ä»¥é¿å…æ­»é”
        # åœ¨å¤šGPUç¯å¢ƒä¸‹torch.corrcoefå¯èƒ½å¯¼è‡´æ­»é”
        self._logger.info("PCCè®¡ç®—å·²æš‚æ—¶ç¦ç”¨ä»¥é¿å…æ­»é”é—®é¢˜")
        
        # æ¸…ç†æ”¶é›†çš„æ•°æ®
        if hasattr(self, '_collected_predictions'):
            self._collected_predictions.clear()
        if hasattr(self, '_collected_targets'):
            self._collected_targets.clear()
        
        return pcc_metrics

    def _print_simple_validation_summary(self):
        """æ‰“å°ç®€åŒ–çš„éªŒè¯ç»“æœæ‘˜è¦"""
        if not self.trainer.is_global_zero:
            return
            
        try:
            val_metrics = self.val_metrics.compute()
            
            # è®¡ç®—PCCæŒ‡æ ‡
            pcc_metrics = self._calculate_pcc_metrics(val_metrics)
            
            # æå–å…³é”®æŒ‡æ ‡
            key_metrics = {}
            for name, value in val_metrics.items():
                if isinstance(value, torch.Tensor):
                    if value.numel() > 1:
                        mean_val = torch.nan_to_num(value, nan=0.0).mean().item()
                        key_metrics[name] = mean_val
                    else:
                        key_metrics[name] = value.item()
            
            # ç®€æ´æ ¼å¼æ‰“å°
            print(f"\nğŸ¯ Epoch {self.current_epoch} éªŒè¯ç»“æœ:")
            
            # ä¼˜å…ˆæ˜¾ç¤ºPCCæŒ‡æ ‡
            if pcc_metrics:
                print("   ğŸ“Š PCCæŒ‡æ ‡:")
                for pcc_name, pcc_value in pcc_metrics.items():
                    print(f"      {pcc_name}: {pcc_value:.4f}")
            
            # æ˜¾ç¤ºåŸºç¡€æŒ‡æ ‡
            basic_metrics = ['mse', 'mae', 'r2']
            print("   ğŸ“ˆ åŸºç¡€æŒ‡æ ‡:")
            for metric in basic_metrics:
                if metric in key_metrics:
                    print(f"      {metric.upper()}: {key_metrics[metric]:.4f}")
            
            print()  # ç©ºè¡Œåˆ†éš”
            
        except Exception as e:
            print(f"âŒ éªŒè¯ç»“æœæ‰“å°å¤±è´¥: {e}")
            self._logger.error(f"ç®€åŒ–éªŒè¯æ‘˜è¦æ‰“å°å‡ºé”™: {e}")

    def on_validation_epoch_start(self):
        """éªŒè¯epochå¼€å§‹æ—¶é‡ç½®æŒ‡æ ‡"""
        try:
            # å®‰å…¨åœ°é‡ç½®éªŒè¯æŒ‡æ ‡
            self.val_metrics.reset()
            self._logger.debug(f"å¼€å§‹éªŒè¯Epoch {self.current_epoch}")
        except Exception as e:
            self._logger.warning(f"é‡ç½®éªŒè¯æŒ‡æ ‡æ—¶å‡ºç°è­¦å‘Š: {e}")
        
        # ğŸ”§ æ¸…ç†ä¹‹å‰å¯èƒ½æ®‹ç•™çš„æ•°æ®
        self._cleanup_validation_data()

    def _print_simple_validation_summary_safe(self):
        """å®‰å…¨çš„éªŒè¯ç»“æœæ‘˜è¦æ‰“å° - é¿å…æ­»é”"""
        if not self.trainer.is_global_zero:
            return
            
        try:
            val_metrics = self.val_metrics.compute()
            
            # æå–å…³é”®æŒ‡æ ‡
            key_metrics = {}
            for name, value in val_metrics.items():
                if isinstance(value, torch.Tensor):
                    if value.numel() > 1:
                        mean_val = torch.nan_to_num(value, nan=0.0).mean().item()
                        key_metrics[name] = mean_val
                    else:
                        key_metrics[name] = value.item()
            
            # ç®€æ´æ ¼å¼æ‰“å° - é¿å…å¤æ‚çš„PCCè®¡ç®—
            print(f"\nğŸ¯ Epoch {self.current_epoch} éªŒè¯ç»“æœ:")
            
            # åªæ˜¾ç¤ºåŸºç¡€æŒ‡æ ‡
            basic_metrics = ['mse', 'mae', 'r2']
            print("   ğŸ“ˆ åŸºç¡€æŒ‡æ ‡:")
            for metric in basic_metrics:
                if metric in key_metrics:
                    print(f"      {metric.upper()}: {key_metrics[metric]:.4f}")
            
            print()  # ç©ºè¡Œåˆ†éš”
            
        except Exception as e:
            print(f"âŒ å®‰å…¨éªŒè¯ç»“æœæ‰“å°å¤±è´¥: {e}")
            self._logger.error(f"å®‰å…¨éªŒè¯æ‘˜è¦æ‰“å°å‡ºé”™: {e}")

    def _calculate_comprehensive_pcc_metrics(self, predictions: torch.Tensor, targets: torch.Tensor) -> Dict[str, float]:
        """è®¡ç®—ç»¼åˆPCCæŒ‡æ ‡ - ä¸æ¨ç†è„šæœ¬ä¿æŒä¸€è‡´"""
        import numpy as np
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if torch.is_tensor(predictions):
            predictions = predictions.cpu().numpy()
        if torch.is_tensor(targets):
            targets = targets.cpu().numpy()
        
        # åº”ç”¨log2(x+1)å˜æ¢ç”¨äºè¯„ä¼°æŒ‡æ ‡è®¡ç®—ï¼ˆä¸æ¨ç†è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
        y_true_log2 = np.log2(targets + 1.0)
        y_pred_log2 = np.log2(predictions + 1.0)
        
        # æ£€æŸ¥NaNå€¼
        if np.isnan(y_true_log2).any() or np.isnan(y_pred_log2).any():
            self._logger.warning("âš ï¸ Log2å˜æ¢åå‘ç°NaNå€¼ï¼Œå°†ä½¿ç”¨åŸå§‹å€¼")
            y_true_log2 = targets
            y_pred_log2 = predictions
        
        # è®¡ç®—åŸºå› çº§åˆ«çš„ç›¸å…³æ€§
        num_genes = y_true_log2.shape[1]
        correlations = np.zeros(num_genes)
        
        for i in range(num_genes):
            true_gene = y_true_log2[:, i]
            pred_gene = y_pred_log2[:, i]
            
            # å¤„ç†å¸¸æ•°å€¼
            if np.std(true_gene) == 0 or np.std(pred_gene) == 0:
                correlations[i] = 0.0
            else:
                corr = np.corrcoef(true_gene, pred_gene)[0, 1]
                correlations[i] = 0.0 if np.isnan(corr) else corr
        
        # æ’åºç›¸å…³æ€§
        sorted_corr = np.sort(correlations)[::-1]
        
        # è®¡ç®—PCCæŒ‡æ ‡
        pcc_10 = np.mean(sorted_corr[:10]) if len(sorted_corr) >= 10 else np.mean(sorted_corr)
        pcc_50 = np.mean(sorted_corr[:50]) if len(sorted_corr) >= 50 else np.mean(sorted_corr)
        pcc_200 = np.mean(sorted_corr[:200]) if len(sorted_corr) >= 200 else np.mean(sorted_corr)
        
        # è®¡ç®—MSEå’ŒMAEï¼ˆä½¿ç”¨log2å˜æ¢åçš„å€¼ï¼‰
        mse = np.mean((y_true_log2 - y_pred_log2) ** 2)
        mae = np.mean(np.abs(y_true_log2 - y_pred_log2))
        
        # è®¡ç®—RVD (Relative Variance Difference)ï¼ˆä½¿ç”¨log2å˜æ¢åçš„å€¼ï¼‰
        MIN_VARIANCE_THRESHOLD = 1e-8
        pred_var = np.var(y_pred_log2, axis=0)
        true_var = np.var(y_true_log2, axis=0)
        
        valid_mask = true_var > MIN_VARIANCE_THRESHOLD
        if np.sum(valid_mask) > 0:
            rvd = np.mean(((pred_var[valid_mask] - true_var[valid_mask]) ** 2) / (true_var[valid_mask] ** 2))
        else:
            rvd = 0.0
        
        return {
            'pcc_10': float(pcc_10),
            'pcc_50': float(pcc_50), 
            'pcc_200': float(pcc_200),
            'mse': float(mse),
            'mae': float(mae),
            'rvd': float(rvd)
        }