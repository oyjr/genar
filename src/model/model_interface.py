"""
VAR-STæ¨¡å‹çš„PyTorch Lightningæ¥å£
é‡æ„ç‰ˆæœ¬ï¼šæ ¸å¿ƒLightningæ¥å£ï¼Œå§”æ‰˜å…·ä½“åŠŸèƒ½ç»™ä¸“é—¨çš„å·¥å…·ç±»
"""

import logging
from typing import Dict, Any, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torch.optim import Optimizer
from torch.optim.lr_scheduler import _LRScheduler

# å¯¼å…¥å·¥å…·ç±»
from .model_metrics import ModelMetrics
from .model_utils import ModelUtils

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logging.basicConfig(level=logging.INFO)

# é»˜è®¤è¶…å‚æ•°
DEFAULT_LEARNING_RATE = 1e-4
DEFAULT_WEIGHT_DECAY = 0.05
DEFAULT_GRADIENT_CLIP = 1.0



class ModelInterface(pl.LightningModule):
    """VAR-STæ¨¡å‹çš„PyTorch Lightningæ¥å£"""

    def __init__(self, config):
        super().__init__()
        
        # åˆ›å»ºä¸“ç”¨logger
        self._logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # ä¿å­˜é…ç½®
        self.config = config
        self.save_hyperparameters()

        # åˆå§‹åŒ–å·¥å…·ç±»
        self.model_utils = ModelUtils(config, self)
        self.model_metrics = ModelMetrics(config, self)
        
        # åŠ è½½æ¨¡å‹
        self._logger.info("åˆå§‹åŒ–VAR-STæ¨¡å‹æ¥å£")
        self.model = self.model_utils.load_model()
        
        # åˆå§‹åŒ–éªŒè¯å’Œæµ‹è¯•è¾“å‡ºç¼“å­˜
        self.validation_step_outputs = []
        self.test_step_outputs = []
        
        # ä»é…ç½®ä¸­è·å–æ¨ç†å‚æ•°
        self.inference_top_k = self.model_utils.get_config('INFERENCE.top_k', 1)

    def _common_step(self, batch, batch_idx, phase: str) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """é€šç”¨çš„stepå¤„ç†é€»è¾‘"""
        original_batch = batch.copy() if isinstance(batch, dict) else batch
        processed_batch = self.model_utils.preprocess_inputs(batch)
        
        loss_final = None

        # Validation and Test steps require special handling to avoid data leakage from teacher forcing
        if phase in ['val', 'test']:
            # Pass 1: Get realistic predictions without teacher forcing for metrics
            inference_batch = processed_batch.copy()
            if 'target_genes' in inference_batch:
                _ = inference_batch.pop('target_genes')
            
            with torch.no_grad():
                # Manually set model to eval mode for this pass
                self.model.eval()
                # Use top-k sampling for realistic, non-deterministic predictions
                inference_results = self.model(**inference_batch, top_k=self.inference_top_k)
            
            # Pass 2: Get the loss using ground truth targets
            # The model is already in eval mode from the trainer, so forward_training will be called
            # This pass will use teacher forcing, but only for loss calculation
            loss_results = self.model(**processed_batch)
            loss = self._compute_loss(loss_results, original_batch)
            loss_final = loss_results.get('loss_final', loss)

            # Extract predictions from the inference pass and targets from the original batch
            predictions, targets = self._extract_predictions_and_targets(inference_results, original_batch)

        else: # Training step
            # Single pass for training
            results_dict = self.model(**processed_batch)
            loss = self._compute_loss(results_dict, original_batch)
            loss_final = results_dict.get('loss_final', loss)
            predictions, targets = self._extract_predictions_and_targets(results_dict, original_batch)
        
        # è®°å½•æ¨¡å‹ç‰¹å®šæŒ‡æ ‡
        # For val/test, this uses loss_results which contains more metrics than inference_results
        # final_results_for_logging = loss_results if phase in ['val', 'test'] else results_dict
        # self.model_metrics.log_model_specific_metrics(phase, final_results_for_logging) # âœ… FIX: ç¦ç”¨è¾…åŠ©æ¨¡å—çš„è‡ªåŠ¨æ—¥å¿—ï¼Œé¿å…é‡å¤è®°å½•
        
        return loss, loss_final, predictions, targets

    def training_step(self, batch, batch_idx):
        """è®­ç»ƒæ­¥éª¤"""
        loss, loss_final, _, _ = self._common_step(batch, batch_idx, 'train')
        self.log('train_loss_final', loss_final, on_step=True, on_epoch=True, prog_bar=False, sync_dist=True)
        return loss

    def validation_step(self, batch, batch_idx):
        """éªŒè¯æ­¥éª¤"""
        # æ‰§è¡Œå®Œæ•´çš„éªŒè¯æ­¥éª¤
        loss, loss_final, predictions, targets = self._common_step(batch, batch_idx, 'val')
        
        # è·å–å®é™…çš„batch_size
        batch_size = targets.size(0) if hasattr(targets, 'size') else 1
        
        # è®°å½•å¤åˆéªŒè¯æŸå¤± (ä¿¡æ¯æ€§)
        self.log('val_loss', loss, 
                on_step=False, 
                on_epoch=True, 
                prog_bar=True,
                batch_size=batch_size,
                sync_dist=True,
                reduce_fx='mean')
        
        # è®°å½•æœ€ç»ˆå°ºåº¦æŸå¤± (æ–°çš„å…³é”®ç›‘æ§æŒ‡æ ‡)
        self.log('val_loss_final', loss_final,
                on_step=False, 
                on_epoch=True, 
                prog_bar=True,
                batch_size=batch_size,
                sync_dist=True,
                reduce_fx='mean')
        
        # æš‚æ—¶ç§»é™¤å³æ—¶PCCè®¡ç®—ï¼Œå› ä¸ºç°åœ¨ä½¿ç”¨val_lossä½œä¸ºç›‘æ§æŒ‡æ ‡
        # TODO: å¦‚æœåç»­éœ€è¦ä½¿ç”¨val_pcc_50ä½œä¸ºç›‘æ§æŒ‡æ ‡ï¼Œå¯ä»¥é‡æ–°å¯ç”¨è¿™éƒ¨åˆ†ä»£ç 
        
        # æ”¶é›†éªŒè¯è¾“å‡ºç”¨äºè¯¦ç»†PCCè®¡ç®— - ä½†è¦é¿å…sanity checké˜¶æ®µ
        if not (hasattr(self.trainer, 'sanity_checking') and self.trainer.sanity_checking):
            output = {
                'val_loss': loss_final,  # ä½¿ç”¨æœ€ç»ˆå°ºåº¦æŸå¤±
                'predictions': predictions.detach().cpu(),  # ç§»åˆ°CPUå‡å°‘GPUå†…å­˜
                'targets': targets.detach().cpu()
            }
            
            # æ·»åŠ åˆ°éªŒè¯è¾“å‡ºåˆ—è¡¨
            self.validation_step_outputs.append(output)
        
    def test_step(self, batch, batch_idx):
        """æµ‹è¯•æ­¥éª¤"""
        loss, loss_final, predictions, targets = self._common_step(batch, batch_idx, 'test')
        
        # è®°å½•æœ€ç»ˆå°ºåº¦æµ‹è¯•æŸå¤±
        self.log('test_loss_final', loss_final, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)

        # æ”¶é›†æµ‹è¯•è¾“å‡ºç”¨äºPCCè®¡ç®—
        output = {
            'test_loss': loss_final, # ä½¿ç”¨æœ€ç»ˆå°ºåº¦æŸå¤±
            'predictions': predictions.detach().cpu(),  # ç§»åˆ°CPUå‡å°‘GPUå†…å­˜
            'targets': targets.detach().cpu()
        }
        
        # æ·»åŠ åˆ°æµ‹è¯•è¾“å‡ºåˆ—è¡¨
        if not hasattr(self, 'test_step_outputs'):
            self.test_step_outputs = []
        self.test_step_outputs.append(output)
        
        return output

    def _compute_loss(self, outputs: Dict[str, torch.Tensor], batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """è®¡ç®—æŸå¤±å‡½æ•°"""
        try:
            # ç›´æ¥ä½¿ç”¨æ¨¡å‹è¿”å›çš„æŸå¤±ï¼Œä¸å†é‡å¤è®¡ç®—
            if 'loss' in outputs:
                total_loss = outputs['loss']
                
                # è®°å½•é¢å¤–æŒ‡æ ‡ï¼ˆä»…åœ¨è®­ç»ƒæ—¶ï¼‰
                if self.training and 'logits' in outputs:
                    with torch.no_grad():
                        logits = outputs['logits']
                        if 'full_target' in outputs:
                            targets = outputs['full_target']
                        elif 'target_genes' in batch:
                            targets = batch['target_genes']
                        else:
                            targets = None
                        
                        if targets is not None:
                            # è®¡ç®—tokenå‡†ç¡®ç‡
                            if logits.dim() == 3:
                                pred_tokens = logits.argmax(dim=-1)
                                targets_flat = targets.view(-1)
                                pred_flat = pred_tokens.view(-1)
                            else:
                                pred_tokens = logits.argmax(dim=-1)
                                targets_flat = targets.view(-1)
                                pred_flat = pred_tokens
                            
                            token_acc = (pred_flat == targets_flat).float().mean()
                            self.log('train_token_accuracy', token_acc, prog_bar=False, sync_dist=False)
                
                self._logger.debug(f"ä½¿ç”¨æ¨¡å‹å†…éƒ¨æŸå¤±={total_loss:.4f}")
                
            else:
                # Fallback for models that don't return 'loss' but 'logits'
                # This part is now less likely to be used with the hierarchical model
                logits = outputs.get('logits')
                if logits is None:
                    raise KeyError("æ¨¡å‹è¾“å‡ºä¸­ç¼ºå°‘'loss'æˆ–'logits'é”®")
                
                targets = batch.get('target_genes')
                if targets is None:
                    raise KeyError("æ‰¹æ¬¡æ•°æ®ä¸­ç¼ºå°‘'target_genes'é”®")

                total_loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))
                self._logger.debug(f"æ‰‹åŠ¨è®¡ç®—æŸå¤±={total_loss:.4f}")
            
            # éªŒè¯æŸå¤±å€¼
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                self._logger.error(f"æŸå¤±å€¼å¼‚å¸¸: {total_loss.item()}")
                raise ValueError("æŸå¤±å€¼ä¸ºNaNæˆ–Inf")
                
            return total_loss
            
        except Exception as e:
            self._logger.error(f"è®¡ç®—æŸå¤±æ—¶å‡ºé”™: {str(e)}")
            self._logger.error(f"è¾“å‡ºé”®: {list(outputs.keys())}")
            raise

    def _extract_predictions_and_targets(self, results_dict: Dict[str, torch.Tensor], 
                                       batch: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:
        """æå–é¢„æµ‹å’Œç›®æ ‡"""
        # The hierarchical model returns 'predictions' for the final scale during training/validation,
        # and 'generated_sequence' during pure inference.
        if 'predictions' in results_dict:
            predictions = results_dict['predictions']
        elif 'generated_sequence' in results_dict:
            predictions = results_dict['generated_sequence']
        else:
            raise ValueError("æ¨¡å‹è¾“å‡ºä¸­å¿…é¡»åŒ…å« 'predictions' æˆ– 'generated_sequence'")
        
        # è·å–ç›®æ ‡
        if 'target_genes' not in batch:
            raise ValueError("æ‰¹æ¬¡æ•°æ®ä¸­æ‰¾ä¸åˆ°target_genes")
        targets = batch['target_genes']
        
        # éªŒè¯æœ€ç»ˆé¢„æµ‹çš„ç»´åº¦æ˜¯å¦ä¸º200
        num_genes = self.model.num_genes
        if predictions.shape[-1] != num_genes:
            raise ValueError(
                f"æœ€ç»ˆé¢„æµ‹ç»´åº¦({predictions.shape[-1]})ä¸ç›®æ ‡åŸºå› æ•°é‡({num_genes})ä¸åŒ¹é…ï¼"
            )
        
        return predictions.float(), targets.float()

    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        weight_decay = float(self.model_utils.get_config('TRAINING.weight_decay', DEFAULT_WEIGHT_DECAY))
        learning_rate = float(self.model_utils.get_config('TRAINING.learning_rate', DEFAULT_LEARNING_RATE))
        
        # å¤šGPUå­¦ä¹ ç‡ç¼©æ”¾
        learning_rate = self.model_utils.scale_learning_rate(learning_rate)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        # è®¾ç½®æ¢¯åº¦è£å‰ª
        self.trainer.gradient_clip_val = self.model_utils.get_config('TRAINING.gradient_clip_val', DEFAULT_GRADIENT_CLIP)
        
        # é…ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_config = self.model_utils.get_scheduler_config(optimizer)
        
        if scheduler_config:
            return {'optimizer': optimizer, 'lr_scheduler': scheduler_config}
        else:
            return {'optimizer': optimizer}

    def on_train_epoch_end(self):
        """è®­ç»ƒepochç»“æŸæ—¶çš„å›è°ƒ"""
        pass  # è®­ç»ƒæ•°æ®ä¸å†ç´¯ç§¯
    
    def on_validation_epoch_end(self):
        """éªŒè¯epochç»“æŸæ—¶çš„å›è°ƒ"""
        self._compute_and_log_pcc_metrics('val')
    
    def on_test_epoch_end(self):
        """æµ‹è¯•epochç»“æŸæ—¶çš„å›è°ƒ"""
        self._compute_and_log_pcc_metrics('test')
    
    def _compute_and_log_pcc_metrics(self, phase: str):
        """ç»Ÿä¸€çš„PCCæŒ‡æ ‡è®¡ç®—å’Œè®°å½•æ–¹æ³•"""
        # ä¿®å¤å±æ€§åæ˜ å°„
        if phase == 'val':
            outputs_attr = 'validation_step_outputs'
        elif phase == 'test':
            outputs_attr = 'test_step_outputs'
        else:
            if self.trainer.is_global_zero:
                print(f"âš ï¸ ä¸æ”¯æŒçš„é˜¶æ®µ: {phase}")
            return
        
        if not hasattr(self, outputs_attr):
            if self.trainer.is_global_zero:
                print(f"âš ï¸ æ²¡æœ‰{phase}é˜¶æ®µçš„è¾“å‡ºæ•°æ®å±æ€§: {outputs_attr}")
            return
            
        outputs = getattr(self, outputs_attr)
        if not outputs:
            if self.trainer.is_global_zero:
                print(f"âš ï¸ {phase}é˜¶æ®µè¾“å‡ºåˆ—è¡¨ä¸ºç©º (å¯èƒ½æ˜¯sanity checké˜¶æ®µ)")
            return
        
        try:
            # æ”¶é›†æ‰€æœ‰æ•°æ®
            all_predictions = []
            all_targets = []
            
            for output in outputs:
                all_predictions.append(output['predictions'])
                all_targets.append(output['targets'])
            
            # åˆå¹¶æ•°æ®
            predictions = torch.cat(all_predictions, dim=0)  # [N, genes]
            targets = torch.cat(all_targets, dim=0)  # [N, genes]
            
            self._logger.info(f"{phase}é˜¶æ®µæ”¶é›†åˆ° {predictions.shape[0]} ä¸ªæ ·æœ¬ï¼Œ{predictions.shape[1]} ä¸ªåŸºå› ")
            
            # è®¡ç®—PCCæŒ‡æ ‡ - æ•°æ®æ˜¯åŸå§‹tokenè®¡æ•°å€¼ï¼Œéœ€è¦åº”ç”¨log2å˜æ¢
            pcc_metrics = self.model_metrics.calculate_comprehensive_pcc_metrics(predictions, targets, apply_log2=True)
            
            # è®°å½•PCCæŒ‡æ ‡åˆ°wandb
            total_samples = predictions.shape[0]
            for metric_name, value in pcc_metrics.items():
                self.log(f'{phase}_{metric_name}', value, 
                        on_epoch=True, 
                        prog_bar=False, 
                        batch_size=total_samples,
                        sync_dist=True)
            
            # åœ¨ä¸»è¿›ç¨‹æ‰“å°è¯¦ç»†ç»“æœ
            if self.trainer.is_global_zero:
                phase_loss = self.trainer.callback_metrics.get(f'{phase}_loss', 0.0)
                
                print(f"\nğŸ¯ Epoch {self.current_epoch} {phase.upper()}ç»“æœ:")
                print(f"   Loss: {phase_loss:.6f}")
                print(f"   PCC-10:  {pcc_metrics['pcc_10']:.4f}")
                print(f"   PCC-50:  {pcc_metrics['pcc_50']:.4f}")
                print(f"   PCC-200: {pcc_metrics['pcc_200']:.4f}")
                print(f"   MSE:     {pcc_metrics['mse']:.6f}")
                print(f"   MAE:     {pcc_metrics['mae']:.6f}")
                print(f"   RVD:     {pcc_metrics['rvd']:.6f}")
                print()
            
            # æ¸…ç†è¾“å‡ºæ•°æ®
            outputs.clear()
            
        except Exception as e:
            self._logger.error(f"è®¡ç®—{phase}é˜¶æ®µPCCæŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    def on_fit_end(self):
        """è®­ç»ƒå®Œæˆæ—¶çš„å›è°ƒ"""
        if not self.trainer.is_global_zero:
            self._logger.info(f"GPUè¿›ç¨‹ {self.trainer.global_rank}: è®­ç»ƒå®Œæˆ")
            return
        
        self._logger.info("è®­ç»ƒå®Œæˆï¼")





    def on_before_optimizer_step(self, optimizer):
        """ä¼˜åŒ–å™¨æ­¥éª¤å‰çš„å›è°ƒ"""
        grad_norm = torch.nn.utils.clip_grad_norm_(
            self.parameters(), 
            self.trainer.gradient_clip_val
        )
        
        self.log('grad_norm', grad_norm, sync_dist=True)