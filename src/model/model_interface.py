import os
import inspect
import importlib
import logging
import numpy as np
import torch
import torchmetrics
from torchmetrics.regression import (
    PearsonCorrCoef,
    MeanAbsoluteError,
    MeanSquaredError,
    ConcordanceCorrCoef,   
    R2Score,
)

from scipy.stats import pearsonr
from datetime import datetime
from typing import Dict, Any

import pytorch_lightning as pl
from pytorch_lightning.callbacks import BasePredictionWriter
import torch.nn.functional as F
from addict import Dict as AddictDict
import torch.nn as nn

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

# Import visualization module
try:
    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ç»å¯¹å¯¼å…¥é¿å…ç›¸å¯¹å¯¼å…¥é”™è¯¯
    from visualization import GeneVisualizer
    VISUALIZATION_AVAILABLE = True
except ImportError:
    # ğŸ”§ åˆ é™¤å›é€€æœºåˆ¶ï¼Œå¯¼å…¥å¤±è´¥ç›´æ¥æŠ¥é”™
    raise ImportError(
        "æ— æ³•å¯¼å…¥å¯è§†åŒ–æ¨¡å—ã€‚è¯·ç¡®ä¿å®‰è£…äº†matplotlib, seaborn, å’ŒPILä¾èµ–ã€‚"
        "å¦‚æœä¸éœ€è¦å¯è§†åŒ–åŠŸèƒ½ï¼Œè¯·ä¿®æ”¹ä»£ç ç§»é™¤å¯è§†åŒ–ç›¸å…³å¯¼å…¥ã€‚"
    )


class ModelInterface(pl.LightningModule):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.save_hyperparameters()

        self.model_name = config.MODEL.model_name if hasattr(config.MODEL, 'model_name') else None
        self.model_config = config.MODEL

        logger.debug(f"Model config: {self.model_config}")
        self.model = self.load_model()

        logger.debug(f"Model: {self.model}")

        self.criterion = torch.nn.MSELoss()

        self.train_outputs = []
        self.val_outputs = []
        self.test_outputs = []

        self._init_metrics()

        self.config = config

        if hasattr(config.DATA, 'normalize'):
            self.normalize = config.DATA.normalize
        else:
            self.normalize = True

        self.avg_pcc = None


    def training_step(self, batch, batch_idx):
        logger.debug(f"Training step {batch_idx} started")
        
        # é¢„å¤„ç†è¾“å…¥
        original_batch = batch.copy() if isinstance(batch, dict) else batch
        processed_batch = self._preprocess_inputs(batch)
        
        # å‰å‘ä¼ æ’­
        results_dict = self.model(**processed_batch)
        
        # è®¡ç®—æŸå¤± (ä»ç„¶ä½¿ç”¨åŸå§‹è®¡æ•°å€¼è®¡ç®—äº¤å‰ç†µæŸå¤±)
        loss = self._compute_loss(results_dict, original_batch)
        
        # ğŸ†• å¦‚æœéœ€è¦è®°å½•æŒ‡æ ‡ï¼Œåˆ™ä½¿ç”¨log2æ ‡å‡†åŒ–å€¼
        if batch_idx % 1000 == 0:  # æ¯1000ä¸ªbatchè®°å½•ä¸€æ¬¡è®­ç»ƒæŒ‡æ ‡
            # æå–é¢„æµ‹å’Œç›®æ ‡ (log2æ ‡å‡†åŒ–)
            logits, target_genes = self._extract_predictions_and_targets(results_dict, original_batch)
            
            # æ›´æ–°è®­ç»ƒæŒ‡æ ‡ (ä½¿ç”¨æ ‡å‡†åŒ–å€¼)
            self._update_metrics('train', logits, target_genes)
            
            # è®°å½•åŸå§‹è®¡æ•°å€¼ç»Ÿè®¡
            if 'predictions' in results_dict:
                predictions_raw = results_dict['predictions']
            else:
                predictions_raw = results_dict.get('generated_sequence', logits)
            targets_raw = original_batch['target_genes']
            
            pred_raw_mean = predictions_raw.float().mean().item()
            target_raw_mean = targets_raw.float().mean().item()
            pred_log2_mean = logits.mean().item()
            target_log2_mean = target_genes.mean().item()
            
            logger.info(f"ğŸƒ è®­ç»ƒ Batch {batch_idx}:")
            logger.info(f"   åŸå§‹è®¡æ•°å€¼ - é¢„æµ‹å‡å€¼: {pred_raw_mean:.2f}, ç›®æ ‡å‡å€¼: {target_raw_mean:.2f}")
            logger.info(f"   Log2æ ‡å‡†åŒ– - é¢„æµ‹å‡å€¼: {pred_log2_mean:.3f}, ç›®æ ‡å‡å€¼: {target_log2_mean:.3f}")
        
        # è®°å½•æŸå¤±
        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)
        
        # ğŸ”§ è®°å½•VAR-STçš„è®­ç»ƒæŒ‡æ ‡
        if hasattr(results_dict, 'accuracy'):
            self.log('train_accuracy_step', results_dict['accuracy'], on_step=True)
        if hasattr(results_dict, 'perplexity'):
            self.log('train_perplexity_step', results_dict['perplexity'], on_step=True)
        
        return loss


    def validation_step(self, batch, batch_idx):
        logger.debug(f"Validation step {batch_idx} started")
        
        # é¢„å¤„ç†è¾“å…¥
        original_batch = batch.copy() if isinstance(batch, dict) else batch
        processed_batch = self._preprocess_inputs(batch)
        
        # å‰å‘ä¼ æ’­
        results_dict = self.model(**processed_batch)
        
        # è®¡ç®—æŸå¤±
        loss = self._compute_loss(results_dict, original_batch)
        self.log('val_loss', loss, on_epoch=True, prog_bar=True, sync_dist=True)
        
        # æå–é¢„æµ‹å’Œç›®æ ‡ (å·²ç»è¿‡log2æ ‡å‡†åŒ–)
        logits, target_genes = self._extract_predictions_and_targets(results_dict, original_batch)
        
        # ğŸ†• åŒæ—¶è®°å½•åŸå§‹è®¡æ•°å€¼ç”¨äºåˆ†æ
        if 'predictions' in results_dict:
            predictions_raw = results_dict['predictions']
        else:
            predictions_raw = results_dict.get('generated_sequence', logits)
        targets_raw = original_batch['target_genes']
        
        # æ›´æ–°æ ‡å‡†åŒ–å€¼çš„æŒ‡æ ‡
        self._update_metrics('val', logits, target_genes)
        
        # ä¿å­˜è¾“å‡ºç”¨äºepochç»“æŸæ—¶çš„è¯„ä¼° (ä¿å­˜æ ‡å‡†åŒ–å€¼)
        self._save_step_outputs('val', loss, logits, target_genes, batch_idx)
        
        # ğŸ†• è®°å½•åŸå§‹è®¡æ•°å€¼ç»Ÿè®¡ä¿¡æ¯
        if batch_idx % 100 == 0:  # æ¯100ä¸ªbatchè®°å½•ä¸€æ¬¡
            pred_raw_mean = predictions_raw.float().mean().item()
            target_raw_mean = targets_raw.float().mean().item()
            pred_log2_mean = logits.mean().item()
            target_log2_mean = target_genes.mean().item()
            
            logger.info(f"ğŸ“Š Batch {batch_idx} ç»Ÿè®¡:")
            logger.info(f"   åŸå§‹è®¡æ•°å€¼ - é¢„æµ‹å‡å€¼: {pred_raw_mean:.2f}, ç›®æ ‡å‡å€¼: {target_raw_mean:.2f}")
            logger.info(f"   Log2æ ‡å‡†åŒ– - é¢„æµ‹å‡å€¼: {pred_log2_mean:.3f}, ç›®æ ‡å‡å€¼: {target_log2_mean:.3f}")
        
        # ğŸ”§ è®°å½•VAR-STçš„éªŒè¯æŒ‡æ ‡
        if hasattr(self, 'model_name') and self.model_name == 'VAR_ST':
            # è®°å½•VAR Transformerçš„ä¸“ç”¨æŒ‡æ ‡
            if 'accuracy' in results_dict:
                self.log('val_accuracy', results_dict['accuracy'], on_epoch=True, logger=True, sync_dist=True, prog_bar=True)
            
            if 'perplexity' in results_dict:
                self.log('val_perplexity', results_dict['perplexity'], on_epoch=True, logger=True, sync_dist=True, prog_bar=True)
            
            if 'top5_accuracy' in results_dict:
                self.log('val_top5_accuracy', results_dict['top5_accuracy'], on_epoch=True, logger=True, sync_dist=True)
        
        return loss

    def test_step(self, batch, batch_idx):
        self._log_tensor_shapes(batch, "Test batch")
        
        original_batch = batch.copy()  # ä¿å­˜åŸå§‹batchç”¨äºåå¤„ç†
        batch = self._preprocess_inputs(batch)

        results_dict = self.model(**batch)
        
        # è·å–é¢„æµ‹å’Œç›®æ ‡
        logits, target_genes = self._extract_predictions_and_targets(results_dict, original_batch)
        
        # è®¡ç®—æŸå¤±å’ŒæŒ‡æ ‡
        loss = self._compute_loss(results_dict, original_batch)
        logger.debug(f"Test loss: {loss.item():.4f}")
        
        # æ›´æ–°æŒ‡æ ‡
        self._update_metrics('test', logits, target_genes)
        
        # ä¿å­˜è¾“å‡º
        self._save_step_outputs('test', loss, logits, target_genes, batch_idx)
        
        return {'logits': logits, 'target_genes': target_genes}
    
    def configure_optimizers(self):
        """
        Configure optimizer and learning rate scheduler with multi-GPU support.
        
        This method implements learning rate scaling strategies for multi-GPU training
        and sets up the AdamW optimizer with ReduceLROnPlateau scheduler.
        
        Returns:
            dict: Dictionary containing optimizer and lr_scheduler configurations
        """
        weight_decay = float(getattr(self.config.TRAINING, 'weight_decay', 0.0))
        learning_rate = float(self.config.TRAINING.learning_rate)
        
        # Apply learning rate scaling for multi-GPU training
        # When training with multiple GPUs, the effective batch size increases proportionally
        # Different scaling strategies help maintain training stability and convergence
        if hasattr(self.config, 'devices') and self.config.devices > 1:
            if hasattr(self.config, 'MULTI_GPU') and hasattr(self.config.MULTI_GPU, 'lr_scaling'):
                lr_scaling = self.config.MULTI_GPU.lr_scaling
                if lr_scaling == 'linear':
                    # Linear scaling: lr = base_lr * num_gpus
                    # Commonly used rule: scale learning rate linearly with batch size
                    learning_rate = learning_rate * self.config.devices
                    logger.info(f"å¤šå¡è®­ç»ƒçº¿æ€§ç¼©æ”¾å­¦ä¹ ç‡: {learning_rate} (åŸå§‹: {self.config.TRAINING.learning_rate}, è®¾å¤‡æ•°: {self.config.devices})")
                elif lr_scaling == 'sqrt':
                    # Square root scaling: lr = base_lr * sqrt(num_gpus)
                    # More conservative scaling, often used for very large batch sizes
                    learning_rate = learning_rate * (self.config.devices ** 0.5)
                    logger.info(f"å¤šå¡è®­ç»ƒå¹³æ–¹æ ¹ç¼©æ”¾å­¦ä¹ ç‡: {learning_rate} (åŸå§‹: {self.config.TRAINING.learning_rate}, è®¾å¤‡æ•°: {self.config.devices})")
                else:
                    # No scaling: keep original learning rate
                    # Useful when batch size scaling is handled elsewhere or not needed
                    logger.info(f"å¤šå¡è®­ç»ƒä¸ç¼©æ”¾å­¦ä¹ ç‡: {learning_rate}")
        
        # Initialize AdamW optimizer with weight decay for regularization
        # AdamW decouples weight decay from gradient-based update, improving generalization
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        # Configure ReduceLROnPlateau scheduler for adaptive learning rate adjustment
        # Reduces learning rate when validation loss plateaus, helping fine-tune convergence
        lr_scheduler_config = self.config.TRAINING.lr_scheduler
        
        # Handle both dict and Namespace types for lr_scheduler configuration
        if isinstance(lr_scheduler_config, dict):
            factor = lr_scheduler_config.get('factor', 0.5)
            patience = lr_scheduler_config.get('patience', 5)
            mode = lr_scheduler_config.get('mode', 'min')
        else:
            factor = getattr(lr_scheduler_config, 'factor', 0.5)
            patience = getattr(lr_scheduler_config, 'patience', 5)
            mode = getattr(lr_scheduler_config, 'mode', 'min')
        
        # Set gradient clipping value for training stability
        # Prevents exploding gradients by clipping gradient norms above threshold
        self.trainer.gradient_clip_val = getattr(self.config.TRAINING, 'gradient_clip_val', 1.0)
        
        # Check if learning rate scheduler should be disabled
        if patience == 0:
            logger.info("å­¦ä¹ ç‡è°ƒåº¦å™¨å·²ç¦ç”¨ (patience=0)ï¼Œå°†ä½¿ç”¨å›ºå®šå­¦ä¹ ç‡")
            return {'optimizer': optimizer}
        
        scheduler = {
            'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode=self.config.TRAINING.mode,  # 'min' for loss, 'max' for accuracy
                factor=factor,  # Reduction factor
                patience=patience,  # Epochs to wait before reduction
                verbose=True  # Log learning rate changes
            ),
            'monitor': self.config.TRAINING.monitor,  # Metric to monitor (e.g., 'val_loss')
            'interval': 'epoch',  # Check at the end of each epoch
            'frequency': 1  # Check every epoch
        }
        
        return {
            'optimizer': optimizer,
            'lr_scheduler': scheduler
        }
    

    def _init_metrics(self):
        # æ£€æµ‹æ¨¡å‹ç±»å‹
        model_name = getattr(self.config, 'model_name', '') or getattr(self.config.MODEL, 'model_name', '') if hasattr(self.config, 'MODEL') else ''
        
        if model_name.upper() == 'TWO_STAGE_VAR_ST':
            # Two-stage VAR_STæ¨¡å‹ä½¿ç”¨200ä¸ªåŸºå› 
            num_outputs = 200
            logger.info(f"Two-stage VAR_STæ¨¡å‹ä½¿ç”¨åŸºå› æ•°é‡: {num_outputs}")
        else:
            # å…¶ä»–æ¨¡å‹ä»åŸºå› åˆ—è¡¨æ–‡ä»¶è·å–åŸºå› æ•°é‡
            if hasattr(self.config, 'data_path'):
                gene_file = f"{self.config.data_path}processed_data/selected_gene_list.txt"
                try:
                    with open(gene_file, 'r') as f:
                        genes = [line.strip() for line in f.readlines() if line.strip()]
                    num_outputs = len(genes)
                    logger.info(f"ä»åŸºå› åˆ—è¡¨æ–‡ä»¶è·å–åŸºå› æ•°é‡: {num_outputs}")
                except FileNotFoundError:
                    num_outputs = 200  # é»˜è®¤å€¼
                    logger.warning(f"æ— æ³•è¯»å–åŸºå› åˆ—è¡¨æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤åŸºå› æ•°é‡: {num_outputs}")
                except Exception as e:
                    num_outputs = 200  # é»˜è®¤å€¼
                    logger.error(f"è¯»å–åŸºå› åˆ—è¡¨æ–‡ä»¶æ—¶å‡ºé”™: {e}ï¼Œä½¿ç”¨é»˜è®¤åŸºå› æ•°é‡: {num_outputs}")
            else:
                num_outputs = 200  # é»˜è®¤å€¼
                logger.warning(f"é…ç½®ä¸­æ— æ•°æ®è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤åŸºå› æ•°é‡: {num_outputs}")

        # ğŸ”§ VARæ¨¡å‹ç‰¹æ®Šå¤„ç†ï¼šä¿®å¤concordanceæŒ‡æ ‡çš„ç»´åº¦åŒ¹é…
        if self.config.MODEL.name == 'VARSTModel':
            # VAR-STè¾“å‡ºåŸºå› è¡¨è¾¾é¢„æµ‹ï¼Œéœ€è¦ä¸åŸºå› æ•°é‡åŒ¹é…
            num_genes = getattr(self.config.MODEL, 'num_genes', 196)
            print(f"ğŸ”§ VARæ¨¡å‹é…ç½®concordanceæŒ‡æ ‡ï¼ŒåŸºå› æ•°é‡: {num_genes}")
            
            # ä¸ºVARæ¨¡å‹åˆ›å»ºæ­£ç¡®çš„æŒ‡æ ‡
            metrics = {
                'mse': MeanSquaredError(),
                'mae': MeanAbsoluteError(),
                'pearson': PearsonCorrCoef(num_outputs=num_genes),
                'concordance': ConcordanceCorrCoef(num_outputs=num_genes),
                'r2': R2Score(multioutput='uniform_average')
            }
        else:
            # å…¶ä»–æ¨¡å‹ä¿æŒåŸæœ‰é€»è¾‘
            metrics = {
                'mse': MeanSquaredError(),
                'mae': MeanAbsoluteError(),
                'pearson': PearsonCorrCoef(num_outputs=num_outputs),
                'concordance': ConcordanceCorrCoef(num_outputs=num_outputs),
                'r2': R2Score(multioutput='uniform_average')
            }

        self.train_metrics = torchmetrics.MetricCollection(metrics.copy())
        self.val_metrics = torchmetrics.MetricCollection(metrics.copy())
        self.test_metrics = torchmetrics.MetricCollection(metrics.copy())

        self.train_history = {
            'loss': [],
            'mse': [],
            'mae': [],
            'pearson_mean': [],
            'pearson_high': [],
        }

        self.val_history = {
            'loss': [],
            'mse': [],
            'mae': [],
            'pearson_mean': [],
            'pearson_high': [],
        }


    def _preprocess_inputs(self, inputs):
        """
        æ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œè¾“å…¥é¢„å¤„ç†
        
        æ”¯æŒçš„æ¨¡å‹ç±»å‹:
        - MFBP: ä½¿ç”¨é»˜è®¤é¢„å¤„ç†
        - VAR_ST: ä½¿ç”¨VAR-STé¢„å¤„ç†
        """
        
        # æ ¹æ®æ¨¡å‹åç§°é€‰æ‹©é¢„å¤„ç†æ–¹æ³•
        if hasattr(self, 'model_name'):
            if self.model_name == 'VAR_ST':
                return self._preprocess_inputs_var_st(inputs)
        
        # é»˜è®¤é¢„å¤„ç†ï¼ˆMFBPåŠå…¶ä»–æ¨¡å‹ï¼‰
        return inputs



    def _preprocess_inputs_var_st(self, inputs):
        """
        VAR_STæ¨¡å‹çš„è¾“å…¥é¢„å¤„ç†
        
        VAR-STæ¨¡å‹æœŸæœ›çš„å‚æ•°ï¼š
        - histology_features: ç»„ç»‡å­¦ç‰¹å¾ 
        - spatial_coords: ç©ºé—´åæ ‡
        - target_genes: ç›®æ ‡åŸºå› è¡¨è¾¾ï¼ˆè®­ç»ƒæ—¶ï¼‰
        """
        processed = {}
        
        # ç»„ç»‡å­¦ç‰¹å¾å¤„ç†
        if 'img' in inputs:
            img_features = inputs['img']
            processed['histology_features'] = img_features
            
            # éªŒè¯ç»´åº¦
            if img_features.dim() not in [2, 3]:
                raise ValueError(f"ä¸æ”¯æŒçš„img_featuresç»´åº¦: {img_features.shape}")
        
        # ç©ºé—´åæ ‡å¤„ç†
        if 'positions' in inputs:
            spatial_coords = inputs['positions']
            processed['spatial_coords'] = spatial_coords
            
            # éªŒè¯ç»´åº¦
            if spatial_coords.dim() not in [2, 3]:
                raise ValueError(f"ä¸æ”¯æŒçš„spatial_coordsç»´åº¦: {spatial_coords.shape}")
        
        # åŸºå› è¡¨è¾¾æ•°æ®å¤„ç†ï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
        if 'target_genes' in inputs:
            target_genes = inputs['target_genes']
            processed['target_genes'] = target_genes
            
            # éªŒè¯ç»´åº¦
            if target_genes.dim() not in [2, 3]:
                raise ValueError(f"ä¸æ”¯æŒçš„target_genesç»´åº¦: {target_genes.shape}")
        
        return processed


    def _log_tensor_shapes(self, tensors_dict, prefix=""):
        """è®°å½•å¼ é‡å½¢çŠ¶ä¿¡æ¯åˆ°æ—¥å¿—"""
        if logger.isEnabledFor(logging.DEBUG):
            for name, tensor in tensors_dict.items():
                if isinstance(tensor, torch.Tensor):
                    logger.debug(f"{prefix}{name}: {tensor.shape}")

    

    def _update_metrics(self, stage, predictions, targets):
        try:
                    # VAR-STæ¨¡å‹ç›´æ¥è®¡ç®—åŸºå› è¡¨è¾¾æŒ‡æ ‡
            
            # è·å–å¯¹åº”é˜¶æ®µçš„æŒ‡æ ‡é›†åˆ
            metrics = getattr(self, f'{stage}_metrics')
            
            # ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®
            if predictions.dim() == 3:
                B, N, G = predictions.shape
                predictions = predictions.reshape(-1, G)  # [B*N, num_genes]
            if targets.dim() == 3:
                B, N, G = targets.shape
                targets = targets.reshape(-1, G)  # [B*N, num_genes]
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šè®­ç»ƒé˜¶æ®µæ¯æ¬¡éƒ½é‡ç½®æŒ‡æ ‡ï¼Œé¿å…ç´¯ç§¯
            if stage == 'train':
                metrics.reset()
            
            # æ›´æ–°æŒ‡æ ‡
            metrics.update(predictions, targets)

            metric_dict = metrics.compute()
            batch_size = predictions.size(0)
            for name, value in metric_dict.items():
                if isinstance(value, torch.Tensor):
                    values = torch.nan_to_num(value, nan=0.0, posinf=1e6, neginf=-1e6)
                    mean_value = values.mean()
                    
                    # ğŸ”§ ä¿®å¤ï¼šå®‰å…¨è®¡ç®—æ ‡å‡†å·®ï¼Œé¿å…degrees of freedomè­¦å‘Š
                    if values.numel() > 1:
                        std_value = values.std()
                    else:
                        std_value = torch.tensor(0.0)
                
                    # ğŸ”§ ä¼˜åŒ–è¿›åº¦æ¡æ˜¾ç¤ºï¼šåªæ˜¾ç¤ºæœ€é‡è¦çš„æŒ‡æ ‡
                    show_in_prog_bar = name in ['mse', 'mae']  # åªåœ¨è¿›åº¦æ¡æ˜¾ç¤ºMSEå’ŒMAE
                    
                    self.log(f'{stage}_{name}', mean_value, prog_bar=show_in_prog_bar, batch_size=batch_size)
                    self.log(f'{stage}_{name}_std', std_value, prog_bar=False, batch_size=batch_size)  # æ ‡å‡†å·®ä¸æ˜¾ç¤ºåœ¨è¿›åº¦æ¡

                    if name == 'pearson':
                        top_k = max(1,int(len(values)*0.3))
                        high_values = torch.topk(values, top_k)[0]
                        high_mean = high_values.mean()
                        
                        # ğŸ”§ ä¿®å¤ï¼šå®‰å…¨è®¡ç®—é«˜ç›¸å…³æ€§æ ‡å‡†å·®
                        if high_values.numel() > 1:
                            high_std = high_values.std()
                        else:
                            high_std = torch.tensor(0.0)
                        
                        # Pearsonç›¸å…³æ€§æŒ‡æ ‡ä¸æ˜¾ç¤ºåœ¨è¿›åº¦æ¡ï¼Œé¿å…è¿‡äºæ‹¥æŒ¤
                        self.log(f'{stage}_pearson_high_mean', high_mean, prog_bar=False, batch_size=batch_size)
                        self.log(f'{stage}_pearson_high_std', high_std, prog_bar=False, batch_size=batch_size)

        except Exception as e:
            logger.error(f"æ›´æ–°æŒ‡æ ‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise e
    
    
    def _save_step_outputs(self, phase, loss, preds, targets, batch_idx=None):
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šè®­ç»ƒé˜¶æ®µä¸ç´¯ç§¯æ•°æ®ï¼Œé¿å…å†…å­˜æ³„æ¼
        if phase == 'train':
            # è®­ç»ƒé˜¶æ®µä¸ä¿å­˜æ•°æ®ï¼Œé¿å…å†…å­˜æ— é™ç´¯ç§¯
            # è®­ç»ƒæŒ‡æ ‡é€šè¿‡Lightningçš„å†…ç½®æœºåˆ¶è®°å½•å³å¯
            return
        
        # åªå¯¹éªŒè¯å’Œæµ‹è¯•é˜¶æ®µç´¯ç§¯æ•°æ®ç”¨äºæœ€ç»ˆè¯„ä¼°
        output_dict = {
            'loss': loss.detach(),
            'preds': preds.detach().cpu(),
            'targets': targets.detach().cpu(),
        }
        if batch_idx is not None:
            output_dict['batch_idx'] = batch_idx

        getattr(self, f'{phase}_outputs').append(output_dict)

    def _process_epoch_end(self, phase):
        outputs = getattr(self, f'{phase}_outputs')
        if len(outputs) == 0:
            return
        
        # æ¸…ç©ºè¾“å‡ºåˆ—è¡¨
        outputs.clear()

    def _compute_and_log_evaluation_metrics(self, phase):
        """
        è®¡ç®—å¹¶è®°å½•è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬åŸå§‹è®¡æ•°å€¼å’Œæ ‡å‡†åŒ–å€¼çš„å¯¹æ¯”
        
        Args:
            phase: è¯„ä¼°é˜¶æ®µ ('val', 'test')
        """
        outputs = getattr(self, f'{phase}_outputs', [])
        
        if not outputs:
            logger.warning(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°{phase}é˜¶æ®µçš„è¾“å‡ºæ•°æ®")
            return
        
        logger.info(f"ğŸ“Š å¼€å§‹è®¡ç®—{phase}é˜¶æ®µçš„è¯¦ç»†è¯„ä¼°æŒ‡æ ‡...")
        
        # æ”¶é›†æ‰€æœ‰é¢„æµ‹å’Œç›®æ ‡ (è¿™äº›å·²ç»æ˜¯log2æ ‡å‡†åŒ–çš„å€¼)
        all_predictions = []
        all_targets = []
        
        for output in outputs:
            if 'preds' in output and 'targets' in output:
                all_predictions.append(output['preds'].cpu().numpy())
                all_targets.append(output['targets'].cpu().numpy())
        
        if not all_predictions:
            logger.warning(f"âš ï¸ {phase}é˜¶æ®µæ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹æ•°æ®")
            return
        
        # æ•´åˆæ•°æ®
        predictions_log2 = np.vstack(all_predictions)  # Log2æ ‡å‡†åŒ–çš„é¢„æµ‹å€¼
        targets_log2 = np.vstack(all_targets)  # Log2æ ‡å‡†åŒ–çš„ç›®æ ‡å€¼
        
        # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
        if predictions_log2.size == 0 or targets_log2.size == 0:
            logger.warning(f"âš ï¸ {phase}é˜¶æ®µæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡è¯„ä¼°æŒ‡æ ‡è®¡ç®—")
            return
        
        logger.info(f"æ•°æ®å½¢çŠ¶æ£€æŸ¥ - predictions: {predictions_log2.shape}, targets: {targets_log2.shape}")
        
        # è®¡ç®—log2æ ‡å‡†åŒ–ç©ºé—´çš„æŒ‡æ ‡
        metrics_log2 = self.calculate_evaluation_metrics(targets_log2, predictions_log2)
        
        # ğŸ†• è®¡ç®—åŸå§‹è®¡æ•°ç©ºé—´çš„æŒ‡æ ‡ç”¨äºå¯¹æ¯”
        predictions_raw = np.power(2, predictions_log2) - 1  # åå‘è½¬æ¢
        targets_raw = np.power(2, targets_log2) - 1
        predictions_raw = np.clip(predictions_raw, 0, None)  # ç¡®ä¿éè´Ÿ
        targets_raw = np.clip(targets_raw, 0, None)
        
        metrics_raw = self.calculate_evaluation_metrics(targets_raw, predictions_raw)
        
        # æ‰“å°å¯¹æ¯”æŠ¥å‘Š
        self.print_dual_evaluation_results(metrics_log2, metrics_raw, phase)
        
        # è®°å½•åˆ°wandbå’Œæ—¥å¿— - ğŸ”§ ä¿®å¤ï¼šç§»é™¤numpyæ•°ç»„ï¼Œä¿®æ­£æŒ‡æ ‡åç§°ï¼Œæ·»åŠ batch_size
        batch_size = predictions_log2.shape[0]  # è·å–batchå¤§å°
        for key, value in metrics_log2.items():
            if key != 'correlations':  # è·³è¿‡numpyæ•°ç»„
                # ç¡®ä¿å€¼æ˜¯æ ‡é‡
                if isinstance(value, (np.ndarray, list)):
                    if np.isscalar(value) or (hasattr(value, 'size') and value.size == 1):
                        value = float(value)
                    else:
                        continue  # è·³è¿‡éæ ‡é‡å€¼
                self.log(f'{phase}_{key}', float(value), on_epoch=True, logger=True, sync_dist=True, batch_size=batch_size)
        
        # è®°å½•åŸå§‹è®¡æ•°å€¼æŒ‡æ ‡ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        for key, value in metrics_raw.items():
            if key != 'correlations':  # è·³è¿‡numpyæ•°ç»„
                # ç¡®ä¿å€¼æ˜¯æ ‡é‡
                if isinstance(value, (np.ndarray, list)):
                    if np.isscalar(value) or (hasattr(value, 'size') and value.size == 1):
                        value = float(value)
                    else:
                        continue  # è·³è¿‡éæ ‡é‡å€¼
                self.log(f'{phase}_raw_{key}', float(value), on_epoch=True, logger=True, sync_dist=True, batch_size=batch_size)
        
        logger.info(f"âœ… {phase}é˜¶æ®µè¯„ä¼°æŒ‡æ ‡è®¡ç®—å®Œæˆ")

    def print_dual_evaluation_results(self, metrics_log2: dict, metrics_raw: dict, phase: str = ""):
        """
        æ‰“å°å¯¹æ¯”è¯„ä¼°ç»“æœï¼šlog2æ ‡å‡†åŒ– vs åŸå§‹è®¡æ•°å€¼
        
        Args:
            metrics_log2: Log2æ ‡å‡†åŒ–ç©ºé—´çš„æŒ‡æ ‡
            metrics_raw: åŸå§‹è®¡æ•°ç©ºé—´çš„æŒ‡æ ‡
            phase: è¯„ä¼°é˜¶æ®µåç§°
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š {phase.upper()} åŒé‡è¯„ä¼°ç»“æœå¯¹æ¯”")
        print(f"{'='*60}")
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æŒ‡æ ‡åç§°
        print(f"ğŸ”¹ Log2æ ‡å‡†åŒ–ç©ºé—´ (æ¨èæŒ‡æ ‡):")
        print(f"   PCC-10:  {metrics_log2.get('PCC-10', 0):.4f}")
        print(f"   PCC-50:  {metrics_log2.get('PCC-50', 0):.4f}")
        print(f"   PCC-200: {metrics_log2.get('PCC-200', 0):.4f}")
        print(f"   MSE:     {metrics_log2.get('MSE', 0):.4f}")
        print(f"   MAE:     {metrics_log2.get('MAE', 0):.4f}")
        print(f"   RVD:     {metrics_log2.get('RVD', 0):.4f}")
        
        print(f"\nğŸ”¸ åŸå§‹è®¡æ•°ç©ºé—´ (å‚è€ƒå¯¹æ¯”):")
        print(f"   PCC-10:  {metrics_raw.get('PCC-10', 0):.4f}")
        print(f"   PCC-50:  {metrics_raw.get('PCC-50', 0):.4f}")
        print(f"   PCC-200: {metrics_raw.get('PCC-200', 0):.4f}")
        print(f"   MSE:     {metrics_raw.get('MSE', 0):.1f}")
        print(f"   MAE:     {metrics_raw.get('MAE', 0):.1f}")
        print(f"   RVD:     {metrics_raw.get('RVD', 0):.4f}")
        
        print(f"\nğŸ’¡ è§£è¯»:")
        pcc_improvement = metrics_log2.get('PCC-200', 0) - metrics_raw.get('PCC-200', 0)
        if pcc_improvement > 0:
            print(f"   âœ… Log2æ ‡å‡†åŒ–æå‡äº†PCC-200: +{pcc_improvement:.4f}")
        else:
            print(f"   âš ï¸ Log2æ ‡å‡†åŒ–é™ä½äº†PCC-200: {pcc_improvement:.4f}")
        
        print(f"   ğŸ“ æ¨èä½¿ç”¨Log2æ ‡å‡†åŒ–æŒ‡æ ‡ä½œä¸ºæ¨¡å‹æ€§èƒ½è¯„ä¼°æ ‡å‡†")
        print(f"{'='*60}")
        print()  # æ·»åŠ ç©ºè¡Œ

    def on_train_epoch_end(self):
        # ğŸ”§ è®­ç»ƒæ•°æ®ä¸å†ç´¯ç§¯ï¼Œæ— éœ€æ¸…ç†
        # self._process_epoch_end('train')  # å·²ç§»é™¤ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®ä¸å†ç´¯ç§¯
        pass
    
    def on_validation_epoch_end(self):
        self._compute_and_log_evaluation_metrics('val')
        # åªæœ‰åœ¨éæœ€åä¸€ä¸ªepochæ—¶æ‰æ¸…ç©ºæ•°æ®ï¼Œä¿ç•™æœ€åä¸€ä¸ªepochçš„æ•°æ®ç”¨äºå¯è§†åŒ–
        if self.current_epoch < self.trainer.max_epochs - 1:
            self._process_epoch_end('val')
        
    def on_test_epoch_end(self):
        self._compute_and_log_evaluation_metrics('test')
        # åªæœ‰åœ¨éæœ€åä¸€ä¸ªepochæ—¶æ‰æ¸…ç©ºæ•°æ®ï¼Œä¿ç•™æœ€åä¸€ä¸ªepochçš„æ•°æ®ç”¨äºå¯è§†åŒ–
        if self.current_epoch < self.trainer.max_epochs - 1:
            self._process_epoch_end('test')
    
    def on_fit_end(self):
        """è®­ç»ƒå®Œæˆæ—¶çš„å›è°ƒ - ç”Ÿæˆæœ€ç»ˆå¯è§†åŒ–"""
        # å¤šGPUç¯å¢ƒä¸‹åªåœ¨ä¸»è¿›ç¨‹ï¼ˆrank 0ï¼‰æ‰§è¡Œå¯è§†åŒ–
        if self.trainer.is_global_zero:
            print("=" * 60)
            print("ğŸ‰ è®­ç»ƒå®Œæˆï¼å¼€å§‹ç”Ÿæˆæœ€ç»ˆå¯è§†åŒ–...")
            print("=" * 60)
            logger.info("è®­ç»ƒå®Œæˆï¼Œå¼€å§‹ç”Ÿæˆæœ€ç»ˆå¯è§†åŒ–...")
            logger.info(f"éªŒè¯æ•°æ®è¾“å‡ºæ•°é‡: {len(self.val_outputs)}")
            logger.info(f"æµ‹è¯•æ•°æ®è¾“å‡ºæ•°é‡: {len(self.test_outputs)}")
            print(f"ğŸ“Š éªŒè¯æ•°æ®è¾“å‡ºæ•°é‡: {len(self.val_outputs)}")
            print(f"ğŸ“Š æµ‹è¯•æ•°æ®è¾“å‡ºæ•°é‡: {len(self.test_outputs)}")
            
            # æ™ºèƒ½è·å–å¯è§†åŒ–è®¾ç½®
            enable_vis = self._get_visualization_setting()
            print(f"ğŸ” enable_visualization: {enable_vis}")
            print(f"ğŸ” VISUALIZATION_AVAILABLE: {VISUALIZATION_AVAILABLE}")
            
            if enable_vis:
                try:
                    # å¦‚æœæœ‰éªŒè¯æ•°æ®ï¼Œä½¿ç”¨éªŒè¯æ•°æ®ç”Ÿæˆå¯è§†åŒ–
                    if len(self.val_outputs) > 0:
                        print("ğŸ¨ å¼€å§‹ä½¿ç”¨éªŒè¯æ•°æ®ç”Ÿæˆæœ€ç»ˆå¯è§†åŒ–...")
                        logger.info("ä½¿ç”¨éªŒè¯æ•°æ®ç”Ÿæˆæœ€ç»ˆå¯è§†åŒ–...")
                        self._generate_final_visualization('val')
                        print("ğŸ¨ éªŒè¯æ•°æ®å¯è§†åŒ–å®Œæˆ")
                    elif len(self.test_outputs) > 0:
                        print("ğŸ¨ å¼€å§‹ä½¿ç”¨æµ‹è¯•æ•°æ®ç”Ÿæˆæœ€ç»ˆå¯è§†åŒ–...")
                        logger.info("ä½¿ç”¨æµ‹è¯•æ•°æ®ç”Ÿæˆæœ€ç»ˆå¯è§†åŒ–...")
                        self._generate_final_visualization('test')
                        print("ğŸ¨ æµ‹è¯•æ•°æ®å¯è§†åŒ–å®Œæˆ")
                    else:
                        print("âŒ æ²¡æœ‰å¯ç”¨çš„éªŒè¯æˆ–æµ‹è¯•æ•°æ®ç”¨äºç”Ÿæˆå¯è§†åŒ–")
                        logger.warning("æ²¡æœ‰å¯ç”¨çš„éªŒè¯æˆ–æµ‹è¯•æ•°æ®ç”¨äºç”Ÿæˆå¯è§†åŒ–")
                        
                except Exception as e:
                    print(f"âŒ å¯è§†åŒ–ç”Ÿæˆå¼‚å¸¸: {e}")
                    logger.error(f"æœ€ç»ˆå¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    logger.warning("è®­ç»ƒå·²å®Œæˆï¼Œä½†è·³è¿‡å¯è§†åŒ–ç”Ÿæˆ")
            else:
                print("âŒ å¯è§†åŒ–å·²ç¦ç”¨")
                logger.info("å¯è§†åŒ–å·²ç¦ç”¨ï¼Œè·³è¿‡å¯è§†åŒ–ç”Ÿæˆ")
            
            logger.info("è®­ç»ƒå’Œå¯è§†åŒ–ç”Ÿæˆå®Œæˆ")
        else:
            # éä¸»è¿›ç¨‹åªè®°å½•ä¿¡æ¯
            logger.info(f"GPUè¿›ç¨‹ {self.trainer.global_rank}: è®­ç»ƒå®Œæˆï¼Œè·³è¿‡å¯è§†åŒ–ç”Ÿæˆï¼ˆåªåœ¨ä¸»è¿›ç¨‹ç”Ÿæˆï¼‰")

    def _get_visualization_setting(self):
        """æ™ºèƒ½è·å–å¯è§†åŒ–è®¾ç½®"""
        # å°è¯•å¤šä¸ªå¯èƒ½çš„é…ç½®ä½ç½®
        possible_paths = [
            'enable_visualization',
            'GENERAL.enable_visualization', 
            'TRAINING.enable_visualization',
            'visualization.enable',
            'vis_enable'
        ]
        
        for attr_path in possible_paths:
            try:
                value = self.config
                for part in attr_path.split('.'):
                    value = getattr(value, part)
                # å¦‚æœæ‰¾åˆ°äº†å¸ƒå°”å€¼ï¼Œç›´æ¥è¿”å›
                if isinstance(value, bool):
                    logger.info(f"Found visualization setting at {attr_path}: {value}")
                    return value
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è½¬æ¢
                elif isinstance(value, str):
                    if value.lower() in ['true', '1', 'yes', 'on']:
                        logger.info(f"Found visualization setting at {attr_path}: {value} -> True")
                        return True
                    elif value.lower() in ['false', '0', 'no', 'off']:
                        logger.info(f"Found visualization setting at {attr_path}: {value} -> False")
                        return False
            except AttributeError:
                continue
        
        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°æˆ–ç¯å¢ƒå˜é‡
        if hasattr(self.config, '__dict__'):
            config_dict = vars(self.config)
            logger.debug(f"Config attributes: {list(config_dict.keys())}")
            
            # æŸ¥æ‰¾ä»»ä½•åŒ…å« 'visual' çš„å±æ€§
            for key, value in config_dict.items():
                if 'visual' in key.lower():
                    logger.info(f"Found visualization-related config: {key} = {value}")
                    if isinstance(value, bool):
                        return value
        
        # é»˜è®¤å¯ç”¨å¯è§†åŒ–
        logger.info("No explicit visualization setting found, defaulting to True")
        return True

    def _load_gene_names(self):
        """åŠ è½½åŸºå› åç§°åˆ—è¡¨"""
        try:
            # å°è¯•ä»é…ç½®çš„æ•°æ®è·¯å¾„åŠ è½½åŸºå› åˆ—è¡¨
            if hasattr(self.config, 'data_path'):
                gene_file = f"{self.config.data_path}processed_data/selected_gene_list.txt"
                if os.path.exists(gene_file):
                    with open(gene_file, 'r') as f:
                        gene_names = [line.strip() for line in f.readlines() if line.strip()]
                    logger.info(f"Loaded {len(gene_names)} gene names from {gene_file}")
                    return gene_names
            
            # å¦‚æœåŸºå› åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»è®­ç»ƒå™¨çš„æ•°æ®æ¨¡å—è·å–
            if hasattr(self.trainer, 'datamodule') and hasattr(self.trainer.datamodule, 'gene_names'):
                gene_names = self.trainer.datamodule.gene_names
                logger.info(f"Loaded {len(gene_names)} gene names from datamodule")
                return gene_names
                
            logger.warning("Could not load gene names, spatial visualization may be limited")
            return None
            
        except Exception as e:
            logger.error(f"Error loading gene names: {e}")
            return None

    def _load_adata_for_visualization(self, phase):
        """åŠ è½½ç”¨äºå¯è§†åŒ–çš„AnnDataå¯¹è±¡ï¼ŒåŒæ—¶è¿”å›å¯¹åº”çš„slide_id"""
        try:
            # å°è¯•ä»trainerçš„æ•°æ®æ¨¡å—è·å–ç›¸åº”é˜¶æ®µçš„æ•°æ®é›†
            if hasattr(self.trainer, 'datamodule'):
                datamodule = self.trainer.datamodule
                
                # æ ¹æ®é˜¶æ®µé€‰æ‹©ç›¸åº”çš„æ•°æ®é›†
                if phase == 'val' and hasattr(datamodule, 'val_dataloader'):
                    dataset = datamodule.val_dataloader().dataset
                elif phase == 'test' and hasattr(datamodule, 'test_dataloader'):
                    dataset = datamodule.test_dataloader().dataset
                else:
                    logger.warning(f"No {phase} dataloader found")
                    return None, None
                
                # æ–¹æ³•1ï¼šå°è¯•è·å–é¢„å­˜å‚¨çš„AnnDataå¯¹è±¡
                if hasattr(dataset, 'adata'):
                    adata = dataset.adata
                    # å°è¯•è·å–å¯¹åº”çš„slide_id
                    slide_id = dataset.ids[0] if hasattr(dataset, 'ids') and len(dataset.ids) > 0 else 'unknown_slide'
                    logger.info(f"Loaded AnnData for {phase} phase with {adata.n_obs} spots from slide: {slide_id}")
                    return adata, slide_id
                elif hasattr(dataset, 'dataset') and hasattr(dataset.dataset, 'adata'):
                    adata = dataset.dataset.dataset.adata
                    # å°è¯•è·å–å¯¹åº”çš„slide_id
                    slide_id = dataset.dataset.ids[0] if hasattr(dataset.dataset, 'ids') and len(dataset.dataset.ids) > 0 else 'unknown_slide'
                    logger.info(f"Loaded AnnData for {phase} phase with {adata.n_obs} spots from slide: {slide_id}")
                    return adata, slide_id
                
                # æ–¹æ³•2ï¼šå¦‚æœæ²¡æœ‰é¢„å­˜å‚¨çš„ï¼Œå°è¯•åŠ¨æ€åŠ è½½ï¼ˆåƒevalæ¨¡å¼é‚£æ ·ï¼‰
                elif hasattr(dataset, 'load_st') and hasattr(dataset, 'ids'):
                    # è·å–ç¬¬ä¸€ä¸ªslideçš„IDï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
                    if len(dataset.ids) > 0:
                        slide_id = dataset.ids[0]  # å–ç¬¬ä¸€ä¸ªslideç”¨äºå¯è§†åŒ–
                        logger.info(f"Dynamically loading AnnData for slide: {slide_id}")
                        
                        # ä½¿ç”¨æ•°æ®é›†çš„load_stæ–¹æ³•åŠ¨æ€åŠ è½½
                        adata = dataset.load_st(slide_id, dataset.genes if hasattr(dataset, 'genes') else None)
                        logger.info(f"Dynamically loaded AnnData for {phase} phase with {adata.n_obs} spots from slide: {slide_id}")
                        return adata, slide_id
                    else:
                        logger.warning(f"No slides found in {phase} dataset")
                        return None, None
                
                # æ–¹æ³•3ï¼šå¦‚æœæ˜¯åŒ…è£…ç±»ï¼Œå°è¯•æ·±åº¦æŸ¥æ‰¾
                else:
                    logger.warning(f"Trying to find AnnData in nested dataset structure...")
                    current_dataset = dataset
                    for i in range(3):  # æœ€å¤šæŸ¥æ‰¾3å±‚
                        if hasattr(current_dataset, 'dataset'):
                            current_dataset = current_dataset.dataset
                            if hasattr(current_dataset, 'adata'):
                                adata = current_dataset.adata
                                slide_id = current_dataset.ids[0] if hasattr(current_dataset, 'ids') and len(current_dataset.ids) > 0 else 'unknown_slide'
                                logger.info(f"Found AnnData at depth {i+1} for {phase} phase with {adata.n_obs} spots from slide: {slide_id}")
                                return adata, slide_id
                            elif hasattr(current_dataset, 'load_st') and hasattr(current_dataset, 'ids'):
                                if len(current_dataset.ids) > 0:
                                    slide_id = current_dataset.ids[0]
                                    logger.info(f"Dynamically loading AnnData for slide: {slide_id} at depth {i+1}")
                                    adata = current_dataset.load_st(slide_id, getattr(current_dataset, 'genes', None))
                                    logger.info(f"Dynamically loaded AnnData for {phase} phase with {adata.n_obs} spots from slide: {slide_id}")
                                    return adata, slide_id
                        else:
                            break
                    
                    logger.warning(f"No AnnData object found in {phase} dataset after deep search")
                    return None, None
            else:
                logger.warning("No datamodule found in trainer")
                return None, None
                
        except Exception as e:
            logger.error(f"Error loading AnnData for visualization: {e}")
            import traceback
            traceback.print_exc()
            return None, None

    def _generate_final_visualization(self, phase):
        """ç”Ÿæˆæœ€ç»ˆçš„å¯è§†åŒ–æŠ¥å‘Š"""
        print(f"ğŸ“Š _generate_final_visualization called with phase: {phase}")
        outputs = getattr(self, f'{phase}_outputs')
        print(f"ğŸ“Š Found {len(outputs)} outputs for {phase}")
        if len(outputs) == 0:
            print(f"âŒ æ²¡æœ‰{phase}æ•°æ®ç”¨äºç”Ÿæˆå¯è§†åŒ–")
            logger.warning(f"æ²¡æœ‰{phase}æ•°æ®ç”¨äºç”Ÿæˆå¯è§†åŒ–")
            return
        
        print(f"ğŸ¨ å¼€å§‹å¤„ç†{phase}é˜¶æ®µçš„æœ€ç»ˆå¯è§†åŒ–...")
        logger.info(f"å¼€å§‹ç”Ÿæˆ{phase}é˜¶æ®µçš„æœ€ç»ˆå¯è§†åŒ–...")
        
        # è·å–AnnDataå¯¹è±¡å’Œå¯¹åº”çš„slide_idç”¨äºç©ºé—´å¯è§†åŒ–
        adata, slide_id = self._load_adata_for_visualization(phase)
        
        # æ”¶é›†æ‰€æœ‰é¢„æµ‹å’Œç›®æ ‡
        all_preds = []
        all_targets = []
        
        for output in outputs:
            preds = output['preds']
            targets = output['targets']
            
            # ç¡®ä¿ç»´åº¦æ­£ç¡®
            if preds.dim() == 3:
                preds = preds.reshape(-1, preds.size(-1))
            if targets.dim() == 3:
                targets = targets.reshape(-1, targets.size(-1))
                
            all_preds.append(preds)
            all_targets.append(targets)
        
        if len(all_preds) == 0:
            logger.warning(f"æ²¡æœ‰æœ‰æ•ˆçš„{phase}é¢„æµ‹æ•°æ®")
            return
            
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
        all_preds = torch.cat(all_preds, dim=0)
        all_targets = torch.cat(all_targets, dim=0)
        
        # å¦‚æœæœ‰AnnDataï¼Œç¡®ä¿é¢„æµ‹æ•°æ®ä¸ç©ºé—´åæ ‡ç»´åº¦åŒ¹é…
        if adata is not None:
            n_spots = adata.n_obs
            print(f"ğŸ” AnnData spots: {n_spots}, Prediction spots: {all_preds.shape[0]}")
            
            # å¦‚æœé¢„æµ‹æ•°æ®æ¯”ç©ºé—´ç‚¹å¤šï¼Œåªå–å‰n_spotsä¸ªï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªslideçš„æ•°æ®ï¼‰
            if all_preds.shape[0] > n_spots:
                print(f"ğŸ“ Truncating prediction data from {all_preds.shape[0]} to {n_spots} to match spatial coordinates")
                all_preds = all_preds[:n_spots]
                all_targets = all_targets[:n_spots]
            elif all_preds.shape[0] < n_spots:
                print(f"âš ï¸ Warning: Prediction data ({all_preds.shape[0]}) is less than spatial coordinates ({n_spots})")
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics = self.calculate_evaluation_metrics(all_targets.numpy(), all_preds.numpy())
        
        try:
            # è·å–æ•°æ®é›†åç§°å’Œæ ‡è®°åŸºå› 
            dataset_name = getattr(self.config, 'expr_name', 'default')
            marker_genes = self.get_marker_genes_for_dataset(dataset_name)
            
            # è·å–åŸºå› åç§°åˆ—è¡¨
            gene_names = self._load_gene_names()
            
            print(f"ğŸ§¬ Dataset: {dataset_name}")
            print(f"ğŸ¯ Marker genes: {marker_genes}")
            print(f"ğŸ“ Gene names loaded: {len(gene_names) if gene_names else 0}")
            print(f"ğŸ—ºï¸ AnnData available: {adata is not None}")
            
            # åˆ›å»ºæœ€ç»ˆå¯è§†åŒ–
            self.create_visualizations(
                phase=f"{phase}_final",  # æ·»åŠ "final"æ ‡è¯†
                y_true=all_targets.numpy(),
                y_pred=all_preds.numpy(),
                metrics=metrics,
                gene_names=gene_names,  # ä»é…ç½®ä¸­åŠ è½½çš„åŸºå› åç§°
                marker_genes=marker_genes,
                adata=adata,  # ä»æ•°æ®é›†åŠ è½½çš„AnnDataå¯¹è±¡
                slide_id=slide_id,  # ä»æ•°æ®é›†è·å–çš„å®é™…slide_id
                img_path=None  # å¦‚æœéœ€è¦å¯ä»¥é…ç½®
            )
            
            logger.info(f"{phase}é˜¶æ®µæœ€ç»ˆå¯è§†åŒ–ç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆ{phase}æœ€ç»ˆå¯è§†åŒ–æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

    def predict_step(self, batch, batch_idx):
        batch = self._preprocess_inputs(batch)
        
        results_dict = self.model(**batch)
        
        dataset = self._trainer.predict_dataloaders.dataset
        _id = dataset.int2id[batch_idx]
        
        preds = results_dict['logits']
        
        return preds, _id

    def on_before_optimizer_step(self, optimizer):
        """åœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¹‹å‰è¿›è¡Œæ¢¯åº¦è£å‰ª"""
        grad_norm = torch.nn.utils.clip_grad_norm_(
            self.parameters(), 
            self.trainer.gradient_clip_val
        )
        
        self.log('grad_norm', grad_norm)

    def load_model(self):
        """åŠ è½½æ¨¡å‹ç±»"""
        try:
            if '_' in self.model_name:
                camel_name = ''.join([i.capitalize() for i in self.model_name.split('_')])
            else:
                camel_name = self.model_name
                
            logger.debug(f"åŠ è½½æ¨¡å‹ç±»ï¼š{self.model_name}")
            logger.debug(f"è½¬æ¢åçš„åç§°ï¼š{camel_name}")
            
            # åŠ è½½VAR-STæ¨¡å‹
            logger.info("åŠ è½½VAR-STæ¨¡å‹...")
            Model = getattr(importlib.import_module(
                f'model.VAR.two_stage_var_st'), 'VARST')
                
            logger.debug("æ¨¡å‹ç±»åŠ è½½æˆåŠŸ")
                
            # å®ä¾‹åŒ–æ¨¡å‹
            model = self.instancialize(Model)
            
            logger.debug("æ¨¡å‹å®ä¾‹åŒ–æˆåŠŸ")
                
            return model
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}")
            # ğŸ”§ ä¿®å¤ï¼šä¿ç•™åŸå§‹é”™è¯¯ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯ä¸¤é˜¶æ®µVAR-STçš„é…ç½®é”™è¯¯
            if "stage1_ckpt_path is required" in str(e):
                raise ValueError(f"Two-stage VAR-STé…ç½®é”™è¯¯: {str(e)}")
            elif "training_stage" in str(e):
                raise ValueError(f"Two-stage VAR-STå‚æ•°é”™è¯¯: {str(e)}")
            else:
                raise ValueError(f'æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}')

    def instancialize(self, Model, **other_args):
        try:
            # è·å–æ¨¡å‹åˆå§‹åŒ–å‚æ•°
            class_args = inspect.getfullargspec(Model.__init__).args[1:]
            
            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†addict.Dictå¯¹è±¡
            if isinstance(self.model_config, AddictDict):
                # å¯¹äºaddict.Dictï¼Œç›´æ¥ä½¿ç”¨dict()è½¬æ¢
                model_config_dict = dict(self.model_config)
                inkeys = model_config_dict.keys()
            elif hasattr(self.model_config, '__dict__'):
                # Namespaceå¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—å…¸
                model_config_dict = vars(self.model_config)
                inkeys = model_config_dict.keys()
            else:
                # å­—å…¸å¯¹è±¡
                model_config_dict = self.model_config
                inkeys = model_config_dict.keys()
            
            args1 = {}
            
            # ä»é…ç½®ä¸­è·å–å‚æ•°
            for arg in class_args:
                if arg in inkeys:
                    args1[arg] = model_config_dict[arg]
                elif arg == 'config':  # å¦‚æœéœ€è¦configå‚æ•°ï¼Œä¼ å…¥å®Œæ•´é…ç½®
                    args1[arg] = self.config
                elif arg == 'histology_feature_dim' and 'feature_dim' in inkeys:
                    # ğŸ”§ ä¸ºVAR_STæ¨¡å‹æ˜ å°„feature_dimåˆ°histology_feature_dim
                    args1[arg] = model_config_dict['feature_dim']
                    logger.debug(f"æ˜ å°„å‚æ•°: feature_dim ({model_config_dict['feature_dim']}) -> histology_feature_dim")
                elif arg == 'current_stage' and 'training_stage' in inkeys:
                    # ğŸ”§ ä¿®å¤ï¼šä¸ºTWO_STAGE_VAR_STæ¨¡å‹æ˜ å°„training_stageåˆ°current_stage
                    args1[arg] = model_config_dict['training_stage']
                    logger.debug(f"æ˜ å°„å‚æ•°: training_stage ({model_config_dict['training_stage']}) -> current_stage")
                elif arg == 'current_stage' and self.model_name == 'TWO_STAGE_VAR_ST':
                    # ğŸš¨ å…³é”®ä¿®å¤ï¼šå¦‚æœæ˜¯ä¸¤é˜¶æ®µæ¨¡å‹ä½†æ²¡æœ‰training_stageé…ç½®ï¼Œå¿…é¡»æŠ¥é”™
                    raise ValueError(
                        f"TWO_STAGE_VAR_STæ¨¡å‹éœ€è¦training_stageå‚æ•°ï¼Œä½†é…ç½®ä¸­æœªæ‰¾åˆ°ã€‚"
                        f"è¯·ç¡®ä¿æ­£ç¡®æŒ‡å®š --training_stage 1 æˆ– --training_stage 2"
                    )
                    
            # æ·»åŠ å…¶ä»–å‚æ•°
            args1.update(other_args)
            
            
            logger.debug(f"æ¨¡å‹å‚æ•°ï¼š{args1}")
                
            # å®ä¾‹åŒ–æ¨¡å‹
            return Model(**args1)
            
        except Exception as e:
            logger.error(f"æ¨¡å‹å®ä¾‹åŒ–å¤±è´¥ï¼š{str(e)}")
            logger.error(f"æ¨¡å‹å‚æ•°ï¼š{args1 if 'args1' in locals() else 'Not available'}")
            raise
    


    def _compute_loss(self, outputs, batch):
        """
        è®¡ç®—æŸå¤± - VAR_STæ¨¡å‹ä¸“ç”¨
        
        Args:
            outputs: æ¨¡å‹è¾“å‡º
            batch: è¾“å…¥æ‰¹æ¬¡æ•°æ®
            
        Returns:
            loss: è®¡ç®—å¾—åˆ°çš„æŸå¤±å€¼
        """
        return self._compute_loss_var_st(outputs, batch)



    def _compute_loss_var_st(self, outputs, batch):
        """
        VAR_STæ¨¡å‹çš„æŸå¤±è®¡ç®—
        
        VAR_STè¿”å›çš„è¾“å‡ºåŒ…å«ï¼š
        - loss: æ€»æŸå¤± (å·²ç»åœ¨æ¨¡å‹å†…éƒ¨è®¡ç®—å¥½)
        - predictions: é¢„æµ‹çš„åŸºå› è¡¨è¾¾
        """
        if 'loss' in outputs:
            # å¦‚æœæ¨¡å‹å·²ç»è®¡ç®—å¥½æ€»æŸå¤±ï¼Œç›´æ¥ä½¿ç”¨
            total_loss = outputs['loss']
            logger.debug(f"VAR_STæ€»æŸå¤±: {total_loss.item():.4f}")
            return total_loss
        else:
            raise ValueError("VAR_STæ¨¡å‹è¾“å‡ºæ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘æŸå¤±ä¿¡æ¯")

    def _compute_loss_mfbp(self, outputs, batch):
        """åŸæœ‰çš„MFBPæŸå¤±è®¡ç®—"""
        logits = outputs['logits']
        target_genes = batch['target_genes']
        
        # ç¡®ä¿ç»´åº¦åŒ¹é…
        if logits.dim() != target_genes.dim():
            if logits.dim() == 3 and target_genes.dim() == 2:
                logits = logits.squeeze(1)
            elif logits.dim() == 2 and target_genes.dim() == 3:
                target_genes = target_genes.squeeze(1)
        
        # è®¡ç®—MSEæŸå¤±
        loss = self.criterion(logits, target_genes)
        
        logger.debug(f"MFBPåŸºå› è¡¨è¾¾é¢„æµ‹æŸå¤±: {loss.item():.4f}")
        
        return loss

    def calculate_gene_correlations(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        """
        Calculate gene-wise Pearson correlation coefficients.
        
        Args:
            y_true: Ground truth gene expression [num_spots, num_genes]
            y_pred: Predicted gene expression [num_spots, num_genes]
            
        Returns:
            Array of correlation coefficients for each gene [num_genes]
        """
        num_genes = y_true.shape[1]
        correlations = []
        
        for i in range(num_genes):
            # Extract gene expression for all spots
            true_gene = y_true[:, i]
            pred_gene = y_pred[:, i]
            
            # Calculate Pearson correlation
            if np.std(true_gene) == 0 or np.std(pred_gene) == 0:
                # Handle constant values (no variation)
                corr = 0.0
            else:
                corr = np.corrcoef(true_gene, pred_gene)[0, 1]
                # Handle NaN values
                if np.isnan(corr):
                    corr = 0.0
            
            correlations.append(corr)
        
        return np.array(correlations)

    def calculate_evaluation_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> dict:
        """
        Calculate comprehensive evaluation metrics for spatial transcriptomics.
        
        Args:
            y_true: Ground truth gene expression [num_spots, num_genes]
            y_pred: Predicted gene expression [num_spots, num_genes]
            
        Returns:
            Dictionary containing all evaluation metrics
        """
        # Ensure inputs are numpy arrays
        if torch.is_tensor(y_true):
            y_true = y_true.cpu().numpy()
        if torch.is_tensor(y_pred):
            y_pred = y_pred.cpu().numpy()
        
        # Calculate gene-wise correlations
        correlations = self.calculate_gene_correlations(y_true, y_pred)
        
        # Sort correlations in descending order for PCC metrics
        sorted_corr = np.sort(correlations)[::-1]
        
        # Calculate PCC metrics (top-k correlations)
        pcc_10 = np.mean(sorted_corr[:10]) if len(sorted_corr) >= 10 else np.mean(sorted_corr)
        pcc_50 = np.mean(sorted_corr[:50]) if len(sorted_corr) >= 50 else np.mean(sorted_corr)
        pcc_200 = np.mean(sorted_corr[:200]) if len(sorted_corr) >= 200 else np.mean(sorted_corr)
        
        # Calculate MSE and MAE
        mse = np.mean((y_true - y_pred) ** 2)
        mae = np.mean(np.abs(y_true - y_pred))
        
        # Calculate RVD (Relative Variance Difference)
        pred_var = np.var(y_pred, axis=0)  # Variance across spots for each gene
        true_var = np.var(y_true, axis=0)  # Variance across spots for each gene
        
        # Avoid division by zero
        valid_mask = true_var > 1e-8
        if np.sum(valid_mask) > 0:
            rvd = np.mean(((pred_var[valid_mask] - true_var[valid_mask]) ** 2) / (true_var[valid_mask] ** 2))
        else:
            rvd = 0.0
        
        return {
            'PCC-10': float(pcc_10),
            'PCC-50': float(pcc_50), 
            'PCC-200': float(pcc_200),
            'MSE': float(mse),
            'MAE': float(mae),
            'RVD': float(rvd),
            'correlations': correlations  # Keep for detailed analysis
        }

    def print_evaluation_results(self, metrics: dict, prefix: str = "") -> None:
        """
        Print evaluation metrics in a formatted way.
        
        Args:
            metrics: Dictionary containing evaluation metrics
            prefix: Optional prefix for the output (e.g., "Val", "Test")
        """
        # ğŸ”§ åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œåªåœ¨ä¸»è¿›ç¨‹è¾“å‡ºè¯„ä¼°ç»“æœ
        import os
        is_main_process = int(os.environ.get('LOCAL_RANK', 0)) == 0
        
        if not is_main_process:
            return  # éä¸»è¿›ç¨‹ç›´æ¥è¿”å›ï¼Œä¸è¾“å‡º
        
        if prefix:
            print(f"\n========== {prefix} è¯„ä¼°ç»“æœ ==========")
        else:
            print(f"\n========== è¯„ä¼°ç»“æœ ==========")
        
        print(f"PCC-10: {metrics['PCC-10']:.4f}")
        print(f"PCC-50: {metrics['PCC-50']:.4f}")
        print(f"PCC-200: {metrics['PCC-200']:.4f}")
        print(f"MSE: {metrics['MSE']:.4f}")
        print(f"MAE: {metrics['MAE']:.4f}")
        print(f"RVD: {metrics['RVD']:.4f}")

    def save_evaluation_results(self, metrics: dict, save_path: str, 
                               slide_id: str = "", model_name: str = "MFBP") -> None:
        """
        Save evaluation metrics to file.
        
        Args:
            metrics: Dictionary containing evaluation metrics
            save_path: Path to save the results
            slide_id: Optional slide identifier
            model_name: Optional model name
        """
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        with open(save_path, 'w') as f:
            f.write(f"Model: {model_name}\n")
            if slide_id:
                f.write(f"Slide: {slide_id}\n")
            f.write(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("\n")
            f.write(f"PCC-10: {metrics['PCC-10']:.4f}\n")
            f.write(f"PCC-50: {metrics['PCC-50']:.4f}\n")
            f.write(f"PCC-200: {metrics['PCC-200']:.4f}\n")
            f.write(f"MSE: {metrics['MSE']:.4f}\n")
            f.write(f"MAE: {metrics['MAE']:.4f}\n")
            f.write(f"RVD: {metrics['RVD']:.4f}\n")

    def create_visualizations(self, phase: str, y_true: np.ndarray, y_pred: np.ndarray, 
                            metrics: dict, gene_names: list = None, 
                            marker_genes: list = None, adata=None, slide_id: str = None, img_path: str = None) -> None:
        """
        Create comprehensive visualizations for model evaluation.
        
        Args:
            phase: Training phase ('val', 'test', etc.)
            y_true: Ground truth gene expression [num_spots, num_genes]
            y_pred: Predicted gene expression [num_spots, num_genes]
            metrics: Dictionary containing evaluation metrics
            gene_names: Optional list of gene names
            marker_genes: Optional list of marker genes for spatial visualization
            adata: Optional AnnData object for spatial coordinates
            img_path: Optional path to tissue image
        """
        if not VISUALIZATION_AVAILABLE:
            logger.warning("Visualization module not available. Skipping visualization creation.")
            return
        
        try:
            # Create visualization directory
            if hasattr(self.config, 'GENERAL') and hasattr(self.config.GENERAL, 'log_path'):
                log_dir = self.config.GENERAL.log_path
            else:
                log_dir = './logs'
            
            # å¤„ç†ä¸åŒçš„é˜¶æ®µæ ¼å¼
            if phase.endswith('_final'):
                vis_dir = os.path.join(log_dir, 'vis', phase)
            else:
                vis_dir = os.path.join(log_dir, 'vis', f'{phase}_epoch_{self.current_epoch}')
            
            # Initialize visualizer
            visualizer = GeneVisualizer(save_dir=vis_dir)
            
            # 1. Create gene variation curves
            logger.info(f"Creating gene variation curves for {phase}...")
            visualizer.plot_gene_variation_curves(
                y_true, y_pred, 
                save_name=f"{phase}_gene_variation_curves",
                show_plots=False
            )
            
            # 2. Create correlation analysis
            logger.info(f"Creating correlation analysis for {phase}...")
            correlations = visualizer.plot_correlation_analysis(
                y_true, y_pred,
                gene_names=gene_names,
                save_name=f"{phase}_correlation_analysis", 
                show_plots=False
            )
            
            # 3. Create spatial gene expression maps (if data available)
            if marker_genes and adata is not None:
                logger.info(f"Creating spatial gene expression maps for {phase}...")
                # Get dataset path from config
                data_path = getattr(self.config, 'data_path', '')
                
                # Use the passed slide_id if available, otherwise fall back to config
                if slide_id is None:
                    slide_id = getattr(self.config, 'slide_test', 'unknown_slide')
                    if phase == 'val':
                        slide_id = getattr(self.config, 'slide_val', slide_id)
                
                logger.info(f"Using slide_id for WSI visualization: {slide_id}")
                
                visualizer.plot_spatial_gene_expression(
                    adata, y_pred, gene_names or [], marker_genes,
                    data_path=data_path,
                    slide_id=slide_id,
                    save_name=f"{phase}_spatial_expression",
                    show_plots=False
                )
            
            # 4. Create summary report
            logger.info(f"Creating summary report for {phase}...")
            visualizer.create_summary_report(
                metrics, correlations,
                save_name=f"{phase}_summary_report"
            )
            
            logger.info(f"All visualizations for {phase} saved to: {vis_dir}")
            
        except Exception as e:
            logger.error(f"Error creating visualizations for {phase}: {e}")
            logger.error(f"Visualization will be skipped for this epoch.")

    def get_marker_genes_for_dataset(self, dataset_name: str) -> list:
        """
        Get default marker genes for different datasets.
        
        Args:
            dataset_name: Name of the dataset
            
        Returns:
            List of marker gene names
        """
        # Default marker genes for different datasets
        marker_genes_dict = {
            'PRAD': ['KLK3', 'AR', 'FOLH1', 'ACPP', 'KLK2', 'STEAP2', 'PSMA', 'NKX3-1'],
            'her2st': ['ERBB2', 'ESR1', 'PGR', 'MKI67', 'TP53', 'BRCA1', 'BRCA2'],
            'default': ['CD3E', 'CD4', 'CD8A', 'CD19', 'CD68', 'PTPRC', 'VIM', 'KRT19']
        }
        
        dataset_key = dataset_name.upper() if dataset_name else 'default'
        return marker_genes_dict.get(dataset_key, marker_genes_dict['default'])

    def _extract_predictions_and_targets(self, results_dict, batch):
        """
        ä»æ¨¡å‹è¾“å‡ºå’Œæ‰¹æ¬¡æ•°æ®ä¸­æå–é¢„æµ‹å’Œç›®æ ‡
        
        Args:
            results_dict: æ¨¡å‹è¾“å‡º
            batch: è¾“å…¥æ‰¹æ¬¡æ•°æ®
            
        Returns:
            tuple: (logits, target_genes)
        """
        # VAR_STæ¨¡å‹çš„ç‰¹æ®Šå¤„ç†
        if 'predictions' in results_dict:
            logits = results_dict['predictions']
        else:
            # å¦‚æœæ²¡æœ‰é¢„æµ‹ç»“æœï¼Œå¯èƒ½æ˜¯è®­ç»ƒæ—¶çš„è¾“å‡º
            logits = results_dict.get('generated_sequence', None)
            if logits is None:
                raise ValueError("VAR_STæ¨¡å‹åº”è¯¥æœ‰predictionsæˆ–generated_sequenceè¾“å‡º")
                
        # ç›®æ ‡æ•°æ®
        if 'target_genes' in batch:
            target_genes = batch['target_genes']
        else:
            raise ValueError("æ‰¹æ¬¡æ•°æ®ä¸­æ‰¾ä¸åˆ°target_genes")
        
        # ğŸ”§ å…³é”®æ”¹è¿›ï¼šå°†ç¦»æ•£è®¡æ•°å€¼è½¬æ¢ä¸ºlog2æ ‡å‡†åŒ–å€¼è¿›è¡Œè¯„ä¼°
        logits_normalized, target_genes_normalized = self._apply_log2_normalization(logits, target_genes)
        
        return logits_normalized, target_genes_normalized

    def _apply_log2_normalization(self, predictions, targets):
        """
        å¯¹ç¦»æ•£è®¡æ•°å€¼åº”ç”¨log2(x+1)æ ‡å‡†åŒ–
        
        Args:
            predictions: åŸå§‹é¢„æµ‹è®¡æ•°å€¼ [B, num_genes]
            targets: åŸå§‹ç›®æ ‡è®¡æ•°å€¼ [B, num_genes]
            
        Returns:
            tuple: (predictions_log2, targets_log2) - æ ‡å‡†åŒ–åçš„å€¼
        """
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat
        if predictions.dtype in [torch.long, torch.int]:
            predictions = predictions.float()
        if targets.dtype in [torch.long, torch.int]:
            targets = targets.float()
        
        # åº”ç”¨log2(x+1)æ ‡å‡†åŒ–
        predictions_log2 = torch.log2(predictions + 1.0)
        targets_log2 = torch.log2(targets + 1.0)
        
        # éªŒè¯æ ‡å‡†åŒ–ç»“æœ
        if torch.isnan(predictions_log2).any() or torch.isnan(targets_log2).any():
            logger.warning("âš ï¸ Log2æ ‡å‡†åŒ–åå‘ç°NaNå€¼ï¼Œå¯èƒ½å­˜åœ¨è´Ÿæ•°è¾“å…¥")
        
        logger.debug(f"ğŸ”¢ Log2æ ‡å‡†åŒ–: é¢„æµ‹å€¼èŒƒå›´ [{predictions_log2.min():.3f}, {predictions_log2.max():.3f}]")
        logger.debug(f"ğŸ”¢ Log2æ ‡å‡†åŒ–: ç›®æ ‡å€¼èŒƒå›´ [{targets_log2.min():.3f}, {targets_log2.max():.3f}]")
        
        return predictions_log2, targets_log2

    def test_full_slide(self, slide_data: Dict[str, torch.Tensor]) -> Dict[str, np.ndarray]:
        """
        å¯¹æ•´ä¸ªslideè¿›è¡Œæµ‹è¯•ï¼Œé€spoté¢„æµ‹åæ•´åˆç»“æœ
        
        Args:
            slide_data: å®Œæ•´slideæ•°æ®ï¼ŒåŒ…å«ï¼š
                - img: [num_spots, feature_dim]
                - target_genes: [num_spots, num_genes]
                - positions: [num_spots, 2]
                - slide_id: str
                - num_spots: int
                
        Returns:
            åŒ…å«é¢„æµ‹ç»“æœå’Œè¯„ä»·æŒ‡æ ‡çš„å­—å…¸
        """
        self.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        # è·å–slideä¿¡æ¯
        slide_id = slide_data['slide_id']
        num_spots = slide_data['num_spots']
        
        print(f"ğŸ”¬ å¼€å§‹æµ‹è¯•slide: {slide_id}ï¼Œå…±{num_spots}ä¸ªspots")
        logger.info(f"Testing full slide: {slide_id} with {num_spots} spots")
        
        # å‡†å¤‡ç»“æœå®¹å™¨
        all_predictions = []
        all_targets = []
        
        # é€spotè¿›è¡Œé¢„æµ‹
        with torch.no_grad():
            for spot_idx in range(num_spots):
                # æ„é€ å•ä¸ªspotçš„batchæ•°æ®
                single_spot_batch = {
                    'img': slide_data['img'][spot_idx:spot_idx+1],  # [1, feature_dim]
                    'target_genes': slide_data['target_genes'][spot_idx:spot_idx+1],  # [1, num_genes]
                    'positions': slide_data['positions'][spot_idx:spot_idx+1],  # [1, 2]
                    'slide_id': slide_id,
                    'spot_idx': spot_idx
                }
                
                # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
                for key, value in single_spot_batch.items():
                    if isinstance(value, torch.Tensor):
                        single_spot_batch[key] = value.to(self.device)
                
                # é¢„å¤„ç†è¾“å…¥
                processed_batch = self._preprocess_inputs(single_spot_batch)
                
                # æ¨¡å‹é¢„æµ‹
                results_dict = self.model(**processed_batch)
                

                
                # æå–é¢„æµ‹å’Œç›®æ ‡
                prediction, target = self._extract_predictions_and_targets(results_dict, single_spot_batch)
                
                # æ”¶é›†ç»“æœ
                all_predictions.append(prediction.cpu().numpy())
                all_targets.append(target.cpu().numpy())
                
                # æ¯100ä¸ªspotæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if (spot_idx + 1) % 100 == 0 or spot_idx == num_spots - 1:
                    print(f"  ğŸ“ˆ å·²å¤„ç† {spot_idx + 1}/{num_spots} spots")
        
        # æ•´åˆæ‰€æœ‰é¢„æµ‹ç»“æœ
        predictions_array = np.vstack(all_predictions)  # [num_spots, num_genes]
        targets_array = np.vstack(all_targets)  # [num_spots, num_genes]
        
        print(f"âœ… Slide {slide_id} æµ‹è¯•å®Œæˆ")
        print(f"   é¢„æµ‹ç»“æœå½¢çŠ¶: {predictions_array.shape}")
        print(f"   ç›®æ ‡æ•°æ®å½¢çŠ¶: {targets_array.shape}")
        
        # è®¡ç®—å®Œæ•´çš„è¯„ä»·æŒ‡æ ‡
        metrics = self.calculate_evaluation_metrics(targets_array, predictions_array)
        
        # æ‰“å°è¯„ä»·ç»“æœ
        self.print_evaluation_results(metrics, f"Slide {slide_id}")
        
        return {
            'predictions': predictions_array,
            'targets': targets_array,
            'metrics': metrics,
            'slide_id': slide_id,
            'num_spots': num_spots
        }

    def run_full_slide_testing(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„slideæµ‹è¯•æµç¨‹
        
        Returns:
            åŒ…å«æ‰€æœ‰slideæµ‹è¯•ç»“æœçš„å­—å…¸
        """
        print("ğŸ¯ å¼€å§‹æ•´slideæµ‹è¯•æ¨¡å¼...")
        logger.info("Starting full slide testing mode...")
        
        # è·å–æµ‹è¯•æ•°æ®é›†
        if not hasattr(self.trainer, 'datamodule'):
            raise ValueError("No datamodule found in trainer")
        
        datamodule = self.trainer.datamodule
        if not hasattr(datamodule, 'test_dataloader'):
            raise ValueError("No test dataloader found")
        
        test_dataset = datamodule.test_dataloader().dataset
        
        # è·å–åŸå§‹datasetï¼ˆå¯èƒ½è¢«åŒ…è£…äº†ï¼‰
        original_dataset = test_dataset
        while hasattr(original_dataset, 'dataset'):
            original_dataset = original_dataset.dataset
        
        # è·å–æµ‹è¯•slideåˆ—è¡¨
        test_slide_ids = original_dataset.get_test_slide_ids()
        
        if not test_slide_ids:
            raise ValueError("No test slides found")
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(test_slide_ids)} ä¸ªæµ‹è¯•slides: {test_slide_ids}")
        
        # å­˜å‚¨æ‰€æœ‰slideçš„ç»“æœ
        all_slide_results = {}
        aggregated_predictions = []
        aggregated_targets = []
        
        # é€ä¸ªæµ‹è¯•æ¯ä¸ªslide
        for slide_id in test_slide_ids:
            print(f"\n{'='*60}")
            print(f"ğŸ”¬ æµ‹è¯•Slide: {slide_id}")
            print(f"{'='*60}")
            
            # è·å–å®Œæ•´slideæ•°æ®
            slide_data = original_dataset.get_full_slide_for_testing(slide_id)
            
            # è¿›è¡Œæµ‹è¯•
            slide_results = self.test_full_slide(slide_data)
            
            # ä¿å­˜ç»“æœ
            all_slide_results[slide_id] = slide_results
            
            # ç´¯ç§¯æ‰€æœ‰æ•°æ®ç”¨äºæ€»ä½“è¯„ä¼°
            aggregated_predictions.append(slide_results['predictions'])
            aggregated_targets.append(slide_results['targets'])
            
            # ä¿å­˜å•ä¸ªslideçš„ç»“æœ
            if hasattr(self.config, 'GENERAL') and hasattr(self.config.GENERAL, 'log_path'):
                save_dir = os.path.join(self.config.GENERAL.log_path, 'test_results')
            else:
                save_dir = './logs/test_results'
            
            os.makedirs(save_dir, exist_ok=True)
            save_path = os.path.join(save_dir, f"{slide_id}_results.txt")
            self.save_evaluation_results(slide_results['metrics'], save_path, slide_id, "VAR_ST")
        
        # è®¡ç®—æ‰€æœ‰slideçš„èšåˆç»“æœ
        print(f"\n{'='*60}")
        print("ğŸ“Š è®¡ç®—èšåˆè¯„ä»·æŒ‡æ ‡...")
        print(f"{'='*60}")
        
        all_predictions = np.vstack(aggregated_predictions)
        all_targets = np.vstack(aggregated_targets)
        
        overall_metrics = self.calculate_evaluation_metrics(all_targets, all_predictions)
        self.print_evaluation_results(overall_metrics, "æ•´ä½“æµ‹è¯•ç»“æœ")
        
        # ä¿å­˜æ•´ä½“ç»“æœ
        if hasattr(self.config, 'GENERAL') and hasattr(self.config.GENERAL, 'log_path'):
            save_dir = os.path.join(self.config.GENERAL.log_path, 'test_results')
        else:
            save_dir = './logs/test_results'
        
        save_path = os.path.join(save_dir, "overall_results.txt")
        self.save_evaluation_results(overall_metrics, save_path, "ALL_SLIDES", "VAR_ST")
        
        # ç”Ÿæˆå¯è§†åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        enable_vis = self._get_visualization_setting()
        if enable_vis and VISUALIZATION_AVAILABLE:
            print("ğŸ¨ ç”Ÿæˆæµ‹è¯•ç»“æœå¯è§†åŒ–...")
            
            # è·å–åŸºå› åç§°å’ŒmarkeråŸºå› 
            gene_names = self._load_gene_names()
            marker_genes = self.get_marker_genes_for_dataset(getattr(self.config, 'expr_name', 'default'))
            
            # ä¸ºæ¯ä¸ªslideç”Ÿæˆå¯è§†åŒ–
            for slide_id, slide_results in all_slide_results.items():
                try:
                    # è·å–å¯¹åº”çš„adata
                    slide_data = original_dataset.get_full_slide_for_testing(slide_id)
                    adata = slide_data.get('adata', None)
                    
                    self.create_visualizations(
                        phase=f"test_{slide_id}",
                        y_true=slide_results['targets'],
                        y_pred=slide_results['predictions'],
                        metrics=slide_results['metrics'],
                        gene_names=gene_names,
                        marker_genes=marker_genes,
                        adata=adata,
                        slide_id=slide_id
                    )
                except Exception as e:
                    logger.warning(f"Failed to create visualization for slide {slide_id}: {e}")
            
            # ç”Ÿæˆæ•´ä½“å¯è§†åŒ–
            try:
                self.create_visualizations(
                    phase="test_overall",
                    y_true=all_targets,
                    y_pred=all_predictions,
                    metrics=overall_metrics,
                    gene_names=gene_names,
                    marker_genes=marker_genes
                )
            except Exception as e:
                logger.warning(f"Failed to create overall visualization: {e}")
        
        print(f"\nğŸ‰ æ•´slideæµ‹è¯•å®Œæˆ!")
        print(f"  æµ‹è¯•slidesæ•°é‡: {len(test_slide_ids)}")
        print(f"  æ€»spotsæ•°é‡: {all_predictions.shape[0]}")
        print(f"  åŸºå› æ•°é‡: {all_predictions.shape[1]}")
        print(f"  æ•´ä½“PCC-10: {overall_metrics['PCC-10']:.4f}")
        print(f"  æ•´ä½“MSE: {overall_metrics['MSE']:.4f}")
        
        return {
            'slide_results': all_slide_results,
            'overall_metrics': overall_metrics,
            'overall_predictions': all_predictions,
            'overall_targets': all_targets,
            'test_slide_ids': test_slide_ids
        }
