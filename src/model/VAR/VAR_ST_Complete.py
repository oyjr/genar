"""
VAR-ST Complete: åŸºå› è¡¨è¾¾å‘é‡çš„è§†è§‰è‡ªå›å½’æ¨¡å‹

åŸºäºVARåŸå§‹è®¾è®¡ç†å¿µçš„åŸºå› ç»´åº¦å¤šå°ºåº¦å®ç°ï¼š
- å°†åŸºå› è¡¨è¾¾å‘é‡ç»„ç»‡ä¸ºä¸åŒç²’åº¦çš„ç‰¹å¾è¡¨ç¤º
- ä½¿ç”¨VQVAEå¯¹æ¯ä¸ªå°ºåº¦çš„åŸºå› ç‰¹å¾è¿›è¡Œç¼–ç 
- VARè‡ªå›å½’ç”Ÿæˆï¼šä»ç²—ç²’åº¦å…¨å±€æ¨¡å¼åˆ°ç»†ç²’åº¦åŸºå› è¡¨è¾¾
- å®Œæ•´ä¿ç•™VARçš„æ‰€æœ‰ç»„ä»¶å’ŒåŠŸèƒ½
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Any
import numpy as np

from .vqvae_st import VQVAE
from .var_st import VAR
from .gene_multiscale_organizer import GeneMultiscaleOrganizer


class VAR_ST_Complete(nn.Module):
    """
    VAR-ST Complete: åŸºå› è¡¨è¾¾å‘é‡çš„å®Œæ•´VARå®ç°
    
    å®ç°çœŸæ­£çš„åŸºå› ç»´åº¦å¤šå°ºåº¦VARï¼š
    1. åŸºå› å¤šå°ºåº¦ç»„ç»‡ï¼šå°†åŸºå› å‘é‡ç»„ç»‡ä¸ºä¸åŒç²’åº¦çš„ç‰¹å¾è¡¨ç¤º
    2. å¤šå°ºåº¦VQVAEç¼–ç ï¼šæ¯ä¸ªå°ºåº¦ä½¿ç”¨ä¸“é—¨çš„VQVAEç¼–ç å™¨
    3. VARè‡ªå›å½’ç”Ÿæˆï¼šä»ç²—ç²’åº¦åˆ°ç»†ç²’åº¦æ¸è¿›å¼ç”Ÿæˆ
    4. åŸºå› é‡å»ºï¼šä»å¤šå°ºåº¦ç‰¹å¾é‡å»ºåˆ°å®Œæ•´åŸºå› è¡¨è¾¾
    """
    
    def __init__(
        self,
        num_genes: int = 200,
        histology_feature_dim: int = 512,
        gene_scales: List[int] = [1, 4, 16, 64, 200],
        vqvae_configs: Optional[List[Dict]] = None,
        var_config: Optional[Dict] = None,
        gene_config: Optional[Dict] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–VAR-ST Completeæ¨¡å‹
        
        Args:
            num_genes: åŸºå› æ•°é‡
            histology_feature_dim: ç»„ç»‡å­¦ç‰¹å¾ç»´åº¦ 
            gene_scales: åŸºå› å¤šå°ºåº¦ç‰¹å¾æ•°é‡åˆ—è¡¨ [1, 4, 16, 64, 200]
            vqvae_configs: æ¯ä¸ªå°ºåº¦çš„VQVAEé…ç½®åˆ—è¡¨
            var_config: VARæ¨¡å‹é…ç½®
            gene_config: åŸºå› ç»„ç»‡å™¨é…ç½®
        """
        super().__init__()
        
        self.num_genes = num_genes
        self.histology_feature_dim = histology_feature_dim
        self.gene_scales = gene_scales
        self.num_scales = len(gene_scales)
        
        print(f"ğŸ§¬ åˆå§‹åŒ–VAR_ST_Complete (åŸºå› ç»´åº¦å¤šå°ºåº¦æ¨¡å¼)")
        print(f"   - ç›®æ ‡åŸºå› æ•°: {num_genes}")
        print(f"   - ç»„ç»‡å­¦ç‰¹å¾ç»´åº¦: {histology_feature_dim}")
        print(f"   - åŸºå› å¤šå°ºåº¦: {gene_scales}")
        
        # åˆå§‹åŒ–åŸºå› å¤šå°ºåº¦ç»„ç»‡å™¨
        gene_config = gene_config or {}
        self.gene_organizer = GeneMultiscaleOrganizer(
            num_genes=num_genes,
            scales=gene_scales,
            projection_method=gene_config.get('projection_method', 'learned'),
            preserve_variance=gene_config.get('preserve_variance', True),
            normalize_features=gene_config.get('normalize_features', True)
        )
        
        # ä¸ºæ¯ä¸ªåŸºå› å°ºåº¦åˆ›å»ºä¸“é—¨çš„VQVAEç¼–ç å™¨
        self.vqvaes = nn.ModuleList()
        self.codebook_sizes = []
        
        if vqvae_configs is None:
            vqvae_configs = [self._get_default_vqvae_config(scale) for scale in gene_scales]
        
        for scale_idx, scale in enumerate(gene_scales):
            print(f"ğŸ§¬ åˆå§‹åŒ–å°ºåº¦ {scale} ç‰¹å¾çš„VQVAE:")
            
            vqvae_config = vqvae_configs[scale_idx] if scale_idx < len(vqvae_configs) else vqvae_configs[-1]
            
            # æ¯ä¸ªå°ºåº¦çš„VQVAEå¤„ç†å¯¹åº”ç»´åº¦çš„åŸºå› ç‰¹å¾å‘é‡ [scale]
            vqvae = VQVAE(
                input_dim=scale,
                hidden_dim=vqvae_config.get('hidden_dim', max(32, scale // 2)),
                latent_dim=vqvae_config.get('latent_dim', min(32, scale)),
                num_embeddings=vqvae_config.get('num_embeddings', min(8192, 512 * scale)),
                commitment_cost=vqvae_config.get('commitment_cost', 0.25)
            )
            
            self.vqvaes.append(vqvae)
            self.codebook_sizes.append(vqvae_config.get('num_embeddings', min(8192, 512 * scale)))
            
            print(f"  - è¾“å…¥ç»´åº¦: {scale}")
            print(f"  - éšè—ç»´åº¦: {vqvae_config.get('hidden_dim', max(32, scale // 2))}")
            print(f"  - æ½œåœ¨ç»´åº¦: {vqvae_config.get('latent_dim', min(32, scale))}")
            print(f"  - ç æœ¬å¤§å°: {vqvae_config.get('num_embeddings', min(8192, 512 * scale))}")
        
        # è®¡ç®—æ¯ä¸ªå°ºåº¦çš„tokenæ•°é‡ï¼ˆåŸºå› ç»´åº¦å¤šå°ºåº¦æ¯ä¸ªbatchæ ·æœ¬äº§ç”Ÿ1ä¸ªtokenï¼‰
        self.tokens_per_scale = [1 for _ in gene_scales]  # æ¯ä¸ªå°ºåº¦æ¯ä¸ªæ ·æœ¬äº§ç”Ÿ1ä¸ªtoken
        self.total_tokens = sum(self.tokens_per_scale)
        
        print(f"   - æ¯å°ºåº¦tokenæ•°: {self.tokens_per_scale}")
        print(f"   - æ€»tokenæ•°: {self.total_tokens}")
        
        # åˆå§‹åŒ–VARæ¨¡å‹
        if var_config is None:
            var_config = self._get_default_var_config()
        
        # VARéœ€è¦å¤„ç†æ‰€æœ‰å°ºåº¦çš„ç»„åˆtokenåºåˆ—
        self.var_model = VAR(
            vocab_size=max(self.codebook_sizes),  # ä½¿ç”¨æœ€å¤§çš„è¯æ±‡è¡¨
            embed_dim=var_config.get('embed_dim', 1024),
            depth=var_config.get('depth', 16),
            num_heads=var_config.get('num_heads', 16),
            sequence_length=self.total_tokens,
            class_dropout_prob=var_config.get('class_dropout_prob', 0.1)
        )
        
        print(f"ğŸš€ VARæ¨¡å‹åˆå§‹åŒ–:")
        print(f"  - è¯æ±‡è¡¨å¤§å°: {max(self.codebook_sizes)}")
        print(f"  - åµŒå…¥ç»´åº¦: {var_config.get('embed_dim', 1024)}")
        print(f"  - åºåˆ—é•¿åº¦: {self.total_tokens}")
        
        # ç»„ç»‡å­¦ç‰¹å¾å¤„ç†å™¨ï¼ˆæ”¯æŒåŠ¨æ€ç»´åº¦é€‚é…ï¼‰
        self.base_histology_dim = histology_feature_dim
        self.histology_processors = nn.ModuleDict()
        
        print(f"âœ… VAR_ST_Completeåˆå§‹åŒ–å®Œæˆ")
    
    def _get_default_vqvae_config(self, scale: int) -> Dict:
        """ä¸ºä¸åŒå°ºåº¦ç”Ÿæˆé»˜è®¤VQVAEé…ç½®"""
        # æ ¹æ®ç‰¹å¾ç»´åº¦è°ƒæ•´æ¨¡å‹å¤æ‚åº¦
        return {
            'hidden_dim': max(32, min(512, scale * 2)),  # éšè—å±‚å¤§å°ä¸ç‰¹å¾ç»´åº¦æˆæ­£æ¯”
            'latent_dim': min(32, max(8, scale // 2)),   # æ½œåœ¨ç»´åº¦é€‚é…ç‰¹å¾ç»´åº¦
            'num_embeddings': min(8192, max(256, 512 * scale)),  # ç æœ¬å¤§å°ä¸ç‰¹å¾ç»´åº¦æˆæ­£æ¯”
            'commitment_cost': 0.25
        }
    
    def _get_default_var_config(self) -> Dict:
        """ç”Ÿæˆé»˜è®¤VARé…ç½®"""
        return {
            'embed_dim': 1024,
            'depth': 16,
            'num_heads': 16,
            'class_dropout_prob': 0.1
        }
    
    def _get_histology_processor(self, input_dim: int) -> nn.Module:
        """è·å–æˆ–åˆ›å»ºå¯¹åº”ç»´åº¦çš„ç»„ç»‡å­¦ç‰¹å¾å¤„ç†å™¨"""
        key = str(input_dim)
        
        if key not in self.histology_processors:
            # åˆ›å»ºæ–°çš„å¤„ç†å™¨
            if input_dim == self.base_histology_dim:
                # ç»´åº¦åŒ¹é…ï¼Œä½¿ç”¨æ’ç­‰æ˜ å°„
                processor = nn.Identity()
            else:
                # ç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨çº¿æ€§å˜æ¢
                processor = nn.Sequential(
                    nn.Linear(input_dim, self.base_histology_dim),
                    nn.ReLU(),
                    nn.Dropout(0.1)
                )
            
            # ç¡®ä¿å¤„ç†å™¨ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
            device = next(self.parameters()).device
            processor = processor.to(device)
            
            self.histology_processors[key] = processor
            print(f"ğŸ”§ åˆ›å»ºç»„ç»‡å­¦ç‰¹å¾å¤„ç†å™¨: {input_dim} â†’ {self.base_histology_dim} (è®¾å¤‡: {device})")
        
        return self.histology_processors[key]
    
    def forward_training(
        self,
        gene_expression: torch.Tensor,
        histology_features: torch.Tensor,
        positions: Optional[torch.Tensor] = None,
        class_labels: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        è®­ç»ƒé˜¶æ®µå‰å‘ä¼ æ’­ - åŸºå› ç»´åº¦å¤šå°ºåº¦æ¨¡å¼
        
        Args:
            gene_expression: [B, num_genes] or [B, N, num_genes] - åŸºå› è¡¨è¾¾å‘é‡
            histology_features: [B, feature_dim] or [B, N, feature_dim] - ç»„ç»‡å­¦ç‰¹å¾
            positions: Optional[torch.Tensor] - ç©ºé—´åæ ‡(åŸºå› æ¨¡å¼ä¸‹ä¸ä½¿ç”¨)
            class_labels: [B] - æ¡ä»¶ç±»åˆ«æ ‡ç­¾(å¯é€‰)
        
        Returns:
            DictåŒ…å«æ‰€æœ‰æŸå¤±å’Œé¢„æµ‹ç»“æœ
        """
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šspotæ¨¡å¼ï¼ˆéªŒè¯/æµ‹è¯•æ—¶å¯èƒ½å‡ºç°ï¼‰
        if gene_expression.dim() == 3 and gene_expression.shape[1] > 1:
            # å¤šspotæ¨¡å¼ï¼šä½¿ç”¨ä¸“é—¨çš„å¤šspotå¤„ç†æ–¹æ³•
            print(f"ğŸ”„ æ£€æµ‹åˆ°å¤šspotè¾“å…¥ {gene_expression.shape}ï¼Œåˆ‡æ¢åˆ°å¤šspotæ¨¡å¼")
            return self.forward_multi_spot(gene_expression, histology_features, positions, class_labels)
        
        # å•spotæ¨¡å¼ï¼šåŸæœ‰çš„è®­ç»ƒé€»è¾‘
        if gene_expression.dim() == 3:
            # å¦‚æœè¾“å…¥æ˜¯[B, 1, num_genes]ï¼Œå‹ç¼©ç»´åº¦
            B, N, num_genes = gene_expression.shape
            if N == 1:
                gene_expression = gene_expression.squeeze(1)  # [B, num_genes]
                print(f"ğŸ”§ å‹ç¼©å•spotè¾“å…¥: [B, N=1, num_genes] -> [B, num_genes]")
            else:
                # è¿™ç§æƒ…å†µç°åœ¨ç”±forward_multi_spotå¤„ç†
                raise ValueError(f"æ„å¤–çš„å¤šspotè¾“å…¥åœ¨å•spotæ¨¡å¼ä¸­: {gene_expression.shape}")
        
        if histology_features.dim() == 3:
            # å¦‚æœè¾“å…¥æ˜¯[B, 1, feature_dim]ï¼Œå‹ç¼©ç»´åº¦
            B, N, feature_dim = histology_features.shape
            if N == 1:
                histology_features = histology_features.squeeze(1)  # [B, feature_dim]
                print(f"ğŸ”§ å‹ç¼©å•spotç‰¹å¾: [B, N=1, feature_dim] -> [B, feature_dim]")
            else:
                # å¦‚æœæ˜¯å¤šspotï¼Œå–å¹³å‡ï¼ˆå…¼å®¹æ€§å¤„ç†ï¼‰
                histology_features = histology_features.mean(dim=1)  # [B, feature_dim]
                print(f"ğŸ”§ å¹³å‡å¤šspotç‰¹å¾: [B, N={N}, feature_dim] -> [B, feature_dim]")
        
        B, num_genes = gene_expression.shape
        device = gene_expression.device
        
        print(f"ğŸ“Š VAR-STè®­ç»ƒå‰å‘ä¼ æ’­ (åŸºå› å¤šå°ºåº¦æ¨¡å¼):")
        print(f"   - åŸºå› è¡¨è¾¾: {gene_expression.shape}")
        print(f"   - ç»„ç»‡å­¦ç‰¹å¾: {histology_features.shape}")
        
        # ç¡®ä¿è¾“å…¥å¼ é‡è¿ç»­æ€§
        gene_expression = gene_expression.contiguous()
        histology_features = histology_features.contiguous()
        
        # åŠ¨æ€é€‚é…ç»„ç»‡å­¦ç‰¹å¾ç»´åº¦
        actual_hist_dim = histology_features.shape[-1]
        if actual_hist_dim != self.base_histology_dim:
            print(f"ğŸ”§ æ£€æµ‹åˆ°ç»„ç»‡å­¦ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.base_histology_dim}, å®é™…{actual_hist_dim}")
            print(f"   - è‡ªåŠ¨é€‚é…: {'UNIç¼–ç å™¨(1024ç»´)' if actual_hist_dim == 1024 else 'CONCHç¼–ç å™¨(512ç»´)'}")
        
        histology_processor = self._get_histology_processor(actual_hist_dim)
        processed_hist = histology_processor(histology_features).contiguous()  # [B, base_hist_dim]
        
        # ç”Ÿæˆç±»åˆ«æ ‡ç­¾ (å¦‚æœæ²¡æœ‰æä¾›)
        if class_labels is None:
            # ä½¿ç”¨ç»„ç»‡å­¦ç‰¹å¾çš„ç»Ÿè®¡é‡ä½œä¸ºç±»åˆ«æ ‡ç­¾
            hist_stats = torch.mean(processed_hist, dim=1) * 1000
            class_labels = hist_stats.long() % 1000  # [B]
            class_labels = class_labels.contiguous()
        
        print(f"   - ç±»åˆ«æ ‡ç­¾: {class_labels.shape}")
        
        # Stage 1: åŸºå› å¤šå°ºåº¦ç»„ç»‡
        print(f"ğŸ§¬ Stage 1: åŸºå› å¤šå°ºåº¦ç»„ç»‡")
        multiscale_expressions = self.gene_organizer.organize_multiscale(
            gene_expression  # [B, num_genes] -> List[[B, scale_i]]
        )
        
        # Stage 2: å¤šå°ºåº¦VQVAEç¼–ç 
        print(f"ğŸ”§ Stage 2: å¤šå°ºåº¦VQVAEç¼–ç ")
        all_tokens = []
        all_vqvae_losses = []
        
        for scale_idx, scale_expression in enumerate(multiscale_expressions):
            scale = self.gene_scales[scale_idx]
            scale_vqvae = self.vqvaes[scale_idx]
            
            print(f"   - ç¼–ç å°ºåº¦ {scale} ç‰¹å¾: {scale_expression.shape}")
            
            # ç¡®ä¿scale_expressionè¿ç»­æ€§
            scale_expression = scale_expression.contiguous()  # [B, scale]
            
            # VQVAEç¼–ç  - ç›´æ¥å¤„ç†[B, scale]æ ¼å¼
            vq_result = scale_vqvae.encode_to_tokens(scale_expression)
            tokens = vq_result['tokens']  # [B, 1] or [B]
            vq_loss = vq_result['loss']
            
            # ç¡®ä¿tokensè¿ç»­æ€§å¹¶ç»Ÿä¸€æ ¼å¼
            tokens = tokens.contiguous()
            if tokens.dim() == 2 and tokens.shape[1] == 1:
                tokens = tokens.squeeze(1).contiguous()  # [B]
            # ä¸ºVARåºåˆ—å‡†å¤‡ï¼šæ¯ä¸ªæ ·æœ¬æ¯ä¸ªå°ºåº¦è´¡çŒ®1ä¸ªtoken
            tokens = tokens.unsqueeze(1).contiguous()  # [B, 1] - æ¯ä¸ªå°ºåº¦1ä¸ªtoken
            
            all_tokens.append(tokens)
            all_vqvae_losses.append(vq_loss)
            
            print(f"     -> tokens: {tokens.shape}, loss: {vq_loss.item():.4f}")
        
        # Stage 3: ç»„åˆtokensåºåˆ—
        print(f"ğŸ”— Stage 3: ç»„åˆtokensåºåˆ—")
        combined_tokens = torch.cat(all_tokens, dim=1).contiguous()  # [B, total_tokens]
        print(f"   - ç»„åˆtokens: {combined_tokens.shape}")
        
        # Stage 4: VARè‡ªå›å½’è®­ç»ƒ
        print(f"ğŸš€ Stage 4: VARè‡ªå›å½’è®­ç»ƒ")
        var_result = self.var_model.forward_training(
            tokens=combined_tokens,
            class_labels=class_labels,
            cfg=1.0,  # è®­ç»ƒæ—¶ä¸ä½¿ç”¨CFG
            cond_drop_prob=0.1
        )
        
        # Stage 5: é‡å»ºéªŒè¯
        print(f"ğŸ”„ Stage 5: é‡å»ºéªŒè¯")
        with torch.no_grad():
            # ä»tokensé‡å»ºå¤šå°ºåº¦è¡¨è¾¾
            split_tokens = self._split_tokens_by_scale(combined_tokens)
            reconstructed_multiscale = self._decode_multiscale_from_tokens(split_tokens)
            
            # ä»å¤šå°ºåº¦é‡å»ºåŸå§‹åŸºå› è¡¨è¾¾
            reconstructed_expression = self.gene_organizer.reconstruct_from_multiscale(
                reconstructed_multiscale, reconstruction_method='finest_scale'
            )
            reconstructed_expression = reconstructed_expression.contiguous()
        
        # è®¡ç®—æ€»æŸå¤±
        total_vqvae_loss = sum(all_vqvae_losses) / len(all_vqvae_losses)
        var_loss = var_result['loss']
        
        # åŸºå› é‡å»ºæŸå¤±
        gene_recon_loss = F.mse_loss(reconstructed_expression, gene_expression)
        
        # ç»„åˆæŸå¤±
        total_loss = var_loss + 0.1 * total_vqvae_loss + 0.1 * gene_recon_loss
        
        print(f"ğŸ“Š æŸå¤±ç»Ÿè®¡:")
        print(f"   - VARæŸå¤±: {var_loss.item():.4f}")
        print(f"   - VQVAEæŸå¤±: {total_vqvae_loss.item():.4f}")
        print(f"   - åŸºå› é‡å»ºæŸå¤±: {gene_recon_loss.item():.4f}")
        print(f"   - æ€»æŸå¤±: {total_loss.item():.4f}")
        
        return {
            'loss': total_loss,
            'var_loss': var_loss,
            'vqvae_loss': total_vqvae_loss,
            'gene_recon_loss': gene_recon_loss,
            'predictions': reconstructed_expression,
            'targets': gene_expression,
            'tokens': combined_tokens,
            'multiscale_expressions': multiscale_expressions,
            'predicted_expression': reconstructed_expression,
            'logits': reconstructed_expression
        }

    def forward_multi_spot(
        self,
        gene_expression: torch.Tensor,  # [B, N, num_genes]
        histology_features: torch.Tensor,  # [B, N, feature_dim] 
        positions: Optional[torch.Tensor] = None,  # [B, N, 2]
        class_labels: Optional[torch.Tensor] = None  # [B]
    ) -> Dict[str, torch.Tensor]:
        """
        å¤šspotå‰å‘ä¼ æ’­ - ç‹¬ç«‹é¢„æµ‹æ¯ä¸ªspotçš„åŸºå› è¡¨è¾¾
        
        è¿™ä¸ªæ–¹æ³•ä¸“é—¨å¤„ç†éªŒè¯/æµ‹è¯•æ—¶çš„å¤šspotè¾“å…¥ï¼Œ
        ä¸ºæ¯ä¸ªspotç‹¬ç«‹è¿›è¡ŒåŸºå› å¤šå°ºåº¦é¢„æµ‹ã€‚
        
        Args:
            gene_expression: [B, N, num_genes] - å¤šä¸ªspotsçš„åŸºå› è¡¨è¾¾
            histology_features: [B, N, feature_dim] - å¤šä¸ªspotsçš„ç»„ç»‡å­¦ç‰¹å¾
            positions: Optional[B, N, 2] - ç©ºé—´ä½ç½®(åŸºå› æ¨¡å¼ä¸‹ä¸ä½¿ç”¨)
            class_labels: Optional[B] - æ¡ä»¶ç±»åˆ«(æ‰©å±•åˆ°æ‰€æœ‰spots)
        
        Returns:
            DictåŒ…å«å¤šspoté¢„æµ‹ç»“æœ
        """
        B, N, num_genes = gene_expression.shape
        device = gene_expression.device
        
        print(f"ğŸŒŸ VAR-STå¤šspotå‰å‘ä¼ æ’­:")
        print(f"   - è¾“å…¥shape: {gene_expression.shape}")
        print(f"   - Batch size: {B}, Spots per sample: {N}")
        print(f"   - ç»„ç»‡å­¦ç‰¹å¾: {histology_features.shape}")
        
        # é‡å¡‘è¾“å…¥ï¼š[B, N, *] -> [B*N, *] ä»¥ä¾¿ç‹¬ç«‹å¤„ç†æ¯ä¸ªspot
        gene_expr_flat = gene_expression.view(B * N, num_genes).contiguous()  # [B*N, num_genes]
        
        # å¤„ç†ç»„ç»‡å­¦ç‰¹å¾
        if histology_features.dim() == 3:
            hist_feat_flat = histology_features.view(B * N, -1).contiguous()  # [B*N, feature_dim]
        else:
            # å¦‚æœç»„ç»‡å­¦ç‰¹å¾æ˜¯[B, feature_dim]ï¼Œéœ€è¦æ‰©å±•åˆ°[B*N, feature_dim]
            hist_feat_flat = histology_features.unsqueeze(1).expand(-1, N, -1).view(B * N, -1).contiguous()
        
        # å¤„ç†ç±»åˆ«æ ‡ç­¾
        if class_labels is not None:
            if class_labels.dim() == 1 and class_labels.shape[0] == B:
                # [B] -> [B*N]
                class_labels_flat = class_labels.unsqueeze(1).expand(-1, N).view(B * N).contiguous()
            else:
                class_labels_flat = class_labels
        else:
            class_labels_flat = None
        
        print(f"   - é‡å¡‘ååŸºå› è¡¨è¾¾: {gene_expr_flat.shape}")
        print(f"   - é‡å¡‘åç»„ç»‡å­¦ç‰¹å¾: {hist_feat_flat.shape}")
        
        # è°ƒç”¨å•spotè®­ç»ƒæ–¹æ³•å¤„ç†æ¯ä¸ªspot
        spot_results = self.forward_training(
            gene_expression=gene_expr_flat,
            histology_features=hist_feat_flat,
            positions=None,  # åŸºå› æ¨¡å¼ä¸‹ä¸ä½¿ç”¨ç©ºé—´ä½ç½®
            class_labels=class_labels_flat
        )
        
        # é‡å¡‘è¾“å‡ºï¼š[B*N, *] -> [B, N, *]
        predictions = spot_results['predictions']  # [B*N, num_genes]
        predictions = predictions.view(B, N, num_genes).contiguous()  # [B, N, num_genes]
        
        targets = gene_expression  # ä¿æŒåŸå§‹ç›®æ ‡æ ¼å¼ [B, N, num_genes]
        
        print(f"   - è¾“å‡ºé¢„æµ‹shape: {predictions.shape}")
        print(f"   - è¾“å‡ºç›®æ ‡shape: {targets.shape}")
        
        # è¿”å›å¤šspotæ ¼å¼çš„ç»“æœ
        return {
            'loss': spot_results['loss'],  # æ ‡é‡æŸå¤±
            'var_loss': spot_results['var_loss'],
            'vqvae_loss': spot_results['vqvae_loss'], 
            'gene_recon_loss': spot_results['gene_recon_loss'],
            'predictions': predictions,  # [B, N, num_genes]
            'targets': targets,  # [B, N, num_genes]
            'tokens': spot_results['tokens'],  # [B*N, total_tokens]
            'multiscale_expressions': spot_results['multiscale_expressions'],
            'predicted_expression': predictions,  # [B, N, num_genes]
            'logits': predictions  # [B, N, num_genes]
        }
    
    def forward_inference(
        self,
        histology_features: torch.Tensor,
        positions: Optional[torch.Tensor] = None,
        class_labels: Optional[torch.Tensor] = None,
        cfg_scale: float = 1.5,
        top_k: int = 50,
        top_p: float = 0.9,
        temperature: float = 1.0,
        num_samples: int = 1
    ) -> Dict[str, torch.Tensor]:
        """
        æ¨ç†é˜¶æ®µï¼šä»ç»„ç»‡å­¦ç‰¹å¾ç”ŸæˆåŸºå› è¡¨è¾¾é¢„æµ‹
        
        Args:
            histology_features: [B, feature_dim] - ç»„ç»‡å­¦ç‰¹å¾
            positions: Optional[torch.Tensor] - ç©ºé—´åæ ‡(åŸºå› æ¨¡å¼ä¸‹ä¸ä½¿ç”¨)
            class_labels: [B] - æ¡ä»¶ç±»åˆ«(å¯é€‰)
            cfg_scale: Classifier-free guidanceç¼©æ”¾å› å­
            top_k, top_p, temperature: é‡‡æ ·å‚æ•°
            num_samples: ç”Ÿæˆæ ·æœ¬æ•°é‡
        
        Returns:
            DictåŒ…å«ç”Ÿæˆçš„åŸºå› è¡¨è¾¾é¢„æµ‹
        """
        # å¤„ç†è¾“å…¥ç»´åº¦
        if histology_features.dim() == 3:
            B, N, feature_dim = histology_features.shape
            if N == 1:
                histology_features = histology_features.squeeze(1)  # [B, feature_dim]
            else:
                histology_features = histology_features.mean(dim=1)  # [B, feature_dim]
            print(f"ğŸ”§ è½¬æ¢å¤šspotç‰¹å¾: [B, N={N}, feature_dim] -> [B, feature_dim]")
        
        B, feature_dim = histology_features.shape
        device = histology_features.device
        
        print(f"ğŸ”® VAR-STæ¨ç†ç”Ÿæˆ (åŸºå› å¤šå°ºåº¦æ¨¡å¼):")
        print(f"   - è¾“å…¥ç‰¹å¾: {histology_features.shape}")
        print(f"   - CFG scale: {cfg_scale}")
        
        # å¤„ç†ç»„ç»‡å­¦ç‰¹å¾
        actual_hist_dim = histology_features.shape[-1]
        histology_processor = self._get_histology_processor(actual_hist_dim)
        processed_hist = histology_processor(histology_features)
        
        # ç”Ÿæˆç±»åˆ«æ ‡ç­¾
        if class_labels is None:
            hist_stats = torch.mean(processed_hist, dim=1) * 1000
            class_labels = hist_stats.long() % 1000
        
        print(f"   - ç±»åˆ«æ ‡ç­¾: {class_labels.shape}")
        
        # VARç”Ÿæˆtokensåºåˆ—
        print(f"ğŸš€ VARè‡ªå›å½’ç”Ÿæˆtokens...")
        generated_tokens = self.var_model.autoregressive_infer_cfg(
            B=B * num_samples,
            class_labels=class_labels.repeat(num_samples) if class_labels is not None else None,
            cfg=cfg_scale,
            top_k=top_k,
            top_p=top_p,
            temperature=temperature
        )
        
        if isinstance(generated_tokens, list):
            # å¦‚æœVARè¿”å›listï¼Œè½¬æ¢ä¸ºtensor
            generated_tokens = torch.cat(generated_tokens, dim=1)  # [B*num_samples, total_tokens]
        
        print(f"   - ç”Ÿæˆtokens: {generated_tokens.shape}")
        
        # ä»tokensè§£ç å¤šå°ºåº¦åŸºå› è¡¨è¾¾
        print(f"ğŸ”„ ä»tokensè§£ç åŸºå› è¡¨è¾¾...")
        split_tokens = self._split_tokens_by_scale(generated_tokens)
        decoded_multiscale = self._decode_multiscale_from_tokens(split_tokens)
        
        # ä»å¤šå°ºåº¦é‡å»ºæœ€ç»ˆåŸºå› è¡¨è¾¾
        print(f"ğŸ”„ ä»å¤šå°ºåº¦é‡å»ºæœ€ç»ˆåŸºå› è¡¨è¾¾...")
        final_expression = self.gene_organizer.reconstruct_from_multiscale(
            decoded_multiscale, reconstruction_method='finest_scale'
        )
        
        # é‡å¡‘ä¸ºåŸå§‹æ‰¹æ¬¡å¤§å°
        if num_samples > 1:
            final_expression = final_expression.view(B, num_samples, -1)
        
        print(f"âœ… æ¨ç†å®Œæˆ: {final_expression.shape}")
        
        return {
            'predictions': final_expression,
            'generated_tokens': generated_tokens,
            'multiscale_expressions': decoded_multiscale,
            'predicted_expression': final_expression,
            'logits': final_expression
        }
    
    def _decode_multiscale_from_tokens(
        self, 
        split_tokens: List[torch.Tensor]
    ) -> List[torch.Tensor]:
        """ä»åˆ†å‰²çš„tokensè§£ç å¤šå°ºåº¦åŸºå› è¡¨è¾¾"""
        decoded_expressions = []
        
        for scale_idx, scale_tokens in enumerate(split_tokens):
            scale = self.gene_scales[scale_idx]
            scale_vqvae = self.vqvaes[scale_idx]
            
            # ç¡®ä¿scale_tokensè¿ç»­æ€§
            scale_tokens = scale_tokens.contiguous()
            
            # å¤„ç†tokensæ ¼å¼ï¼š[B, 1] -> [B]
            if scale_tokens.dim() == 2 and scale_tokens.shape[1] == 1:
                scale_tokens = scale_tokens.squeeze(1).contiguous()  # [B]
            
            # VQVAEè§£ç 
            decoded = scale_vqvae.decode_from_tokens(scale_tokens)  # [B, scale]
            decoded = decoded.contiguous()
            
            decoded_expressions.append(decoded)
            
            print(f"   - è§£ç å°ºåº¦{scale_idx+1}: tokens{scale_tokens.shape} -> è¡¨è¾¾{decoded.shape}")
        
        return decoded_expressions
    
    def _split_tokens_by_scale(self, combined_tokens: torch.Tensor) -> List[torch.Tensor]:
        """å°†ç»„åˆçš„tokensåºåˆ—åˆ†å‰²å›å„ä¸ªå°ºåº¦"""
        # ç¡®ä¿è¾“å…¥tokensè¿ç»­æ€§
        combined_tokens = combined_tokens.contiguous()
        
        B = combined_tokens.shape[0]
        split_tokens = []
        start_idx = 0
        
        for scale_idx, tokens_count in enumerate(self.tokens_per_scale):
            end_idx = start_idx + tokens_count
            scale_tokens = combined_tokens[:, start_idx:end_idx].contiguous()  # [B, tokens_count]
            split_tokens.append(scale_tokens)
            start_idx = end_idx
        
        return split_tokens
    
    def forward(self, **inputs) -> Dict[str, torch.Tensor]:
        """ç»Ÿä¸€å‰å‘ä¼ æ’­æ¥å£"""
        mode = inputs.get('mode', 'training')
        
        # æ£€æŸ¥è¾“å…¥æ•°æ®æ ¼å¼
        gene_expression = inputs.get('gene_expression')
        histology_features = inputs.get('histology_features')
        
        # æ™ºèƒ½æ¨¡å¼æ£€æµ‹
        if mode == 'training':
            # è®­ç»ƒæ¨¡å¼ï¼šä¼˜å…ˆä½¿ç”¨forward_training
            # ä½†å¦‚æœæ£€æµ‹åˆ°å¤šspotè¾“å…¥ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å¤šspotæ¨¡å¼
            if (gene_expression is not None and 
                gene_expression.dim() == 3 and 
                gene_expression.shape[1] > 1):
                print(f"ğŸ” è®­ç»ƒæ¨¡å¼æ£€æµ‹åˆ°å¤šspotè¾“å…¥ï¼Œè‡ªåŠ¨ä½¿ç”¨å¤šspotå¤„ç†")
                return self.forward_training(
                    gene_expression=gene_expression,
                    histology_features=histology_features,
                    positions=inputs.get('positions'),
                    class_labels=inputs.get('class_labels')
                )
            else:
                return self.forward_training(
                    gene_expression=gene_expression,
                    histology_features=histology_features,
                    positions=inputs.get('positions'),
                    class_labels=inputs.get('class_labels')
                )
        else:
            # æ¨ç†æ¨¡å¼ï¼šä½¿ç”¨forward_inference
            return self.forward_inference(
                histology_features=histology_features,
                positions=inputs.get('positions'),
                class_labels=inputs.get('class_labels'),
                cfg_scale=inputs.get('cfg_scale', 1.5),
                top_k=inputs.get('top_k', 50),
                top_p=inputs.get('top_p', 0.9),
                temperature=inputs.get('temperature', 1.0),
                num_samples=inputs.get('num_samples', 1)
            ) 