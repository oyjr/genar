"""
VARåŸºå› åŒ…è£…å™¨ - åŸºäºåŸºå› å¤šå°ºåº¦çš„ä»å¤´è®­ç»ƒç‰ˆæœ¬

è®¾è®¡ç†å¿µï¼š
- å®Œå…¨ä»å¤´è®­ç»ƒVARå’ŒVQVAE
- ä¸“é—¨ä¸º196åŸºå›  (14x14) è®¾è®¡
- åŸºå› ç»´åº¦å¤šå°ºåº¦ï¼š(1,2,3,4,5) å¯¹åº”ç”Ÿç‰©å­¦å±‚æ¬¡
- æ— è§†è§‰é¢„è®­ç»ƒä¾èµ–ï¼Œçº¯åŸºå› è¡¨è¾¾é€»è¾‘
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import sys
import os
from typing import Dict, List, Optional, Tuple, Any, Union
import numpy as np

# ğŸ”§ ä¿®å¤ï¼šå¯¼å…¥åŸå§‹VARé¡¹ç›®çš„æ¨¡å‹æ¶æ„ï¼ˆä¸åŠ è½½æƒé‡ï¼‰
# æ·»åŠ VARé¡¹ç›®è·¯å¾„åˆ°sys.path
VAR_PROJECT_PATH = "/home/ouyangjiarui/project/ST/VAR"
if VAR_PROJECT_PATH not in sys.path:
    sys.path.insert(0, VAR_PROJECT_PATH)

# ä»åŸå§‹VARé¡¹ç›®å¯¼å…¥æ¨¡å‹æ¶æ„
try:
    from models.var import VAR
    from models.vqvae import VQVAE
    print("âœ… æˆåŠŸå¯¼å…¥VARé¡¹ç›®æ¨¡å‹æ¶æ„")
    VAR_AVAILABLE = True
except ImportError as e:
    print(f"âŒ å¯¼å…¥VARæ¨¡å‹å¤±è´¥: {e}")
    print("ğŸ”„ å°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ç»§ç»­")
    VAR_AVAILABLE = False

# å¯¼å…¥æœ¬åœ°åŸºå› é€‚é…å™¨
from .gene_pseudo_image_adapter import GenePseudoImageAdapter

class VARGeneWrapper(nn.Module):
    """
    VARåŸºå› åŒ…è£…å™¨ - åŸºå› å¤šå°ºåº¦ä»å¤´è®­ç»ƒç‰ˆæœ¬
    
    ä¸“é—¨ä¸ºåŸºå› è¡¨è¾¾é¢„æµ‹è®¾è®¡çš„VARæ¶æ„ï¼š
    - 196ä¸ªåŸºå›  â†’ 14Ã—14Ã—1 ä¼ªå›¾åƒ
    - åŸºå› å¤šå°ºåº¦ï¼š(1,2,3,4,5) = 55 tokens
    - å®Œå…¨ä»å¤´è®­ç»ƒï¼Œæ— é¢„è®­ç»ƒä¾èµ–
    - ç”Ÿç‰©å­¦å¤šå°ºåº¦è¯­ä¹‰
    """
    
    def __init__(
        self,
        num_genes: int = 196,  # ğŸ”§ å›ºå®š196åŸºå› 
        image_size: int = 14,  # ğŸ”§ 14Ã—14å®Œç¾å¹³æ–¹
        histology_feature_dim: int = 512,
        patch_nums: Tuple[int, ...] = (1, 2, 3, 4, 5),  # ğŸ”§ åŸºå› å¤šå°ºåº¦
        var_config: Optional[Dict] = None,
        vqvae_config: Optional[Dict] = None,
        adapter_config: Optional[Dict] = None
    ):
        """
        åˆå§‹åŒ–åŸºå› å¤šå°ºåº¦VARæ¨¡å‹
        
        Args:
            num_genes: åŸºå› æ•°é‡ï¼Œå›ºå®š196 (14Ã—14)
            image_size: ä¼ªå›¾åƒå°ºå¯¸ï¼Œå›ºå®š14Ã—14
            histology_feature_dim: ç»„ç»‡å­¦ç‰¹å¾ç»´åº¦
            patch_nums: åŸºå› å¤šå°ºåº¦è®¾ç½® (1,2,3,4,5)
            var_config: VARé…ç½®
            vqvae_config: VQVAEé…ç½®
            adapter_config: é€‚é…å™¨é…ç½®
        """
        super().__init__()
        
        # ğŸ”§ ä¸¥æ ¼éªŒè¯åŸºå› æ•°é‡å’Œå›¾åƒå°ºå¯¸çš„åŒ¹é…ï¼ˆæ”¯æŒpaddingç­–ç•¥ï¼‰
        total_image_positions = image_size * image_size
        if num_genes == 196:
            # ğŸ”§ ä¿®æ”¹ï¼šå…è®¸196åŸºå› ä½¿ç”¨16Ã—16 paddingç­–ç•¥
            if image_size < 14:
                raise ValueError(f"196åŸºå› è‡³å°‘éœ€è¦14Ã—14å›¾åƒï¼Œä½†æŒ‡å®šäº†{image_size}Ã—{image_size}")
            if image_size == 14 and total_image_positions != 196:
                raise ValueError(f"14Ã—14å›¾åƒå¿…é¡»å®Œç¾åŒ¹é…196åŸºå› ï¼Œä½†ä½ç½®æ•°ä¸º{total_image_positions}")
            
            # å…è®¸ä½¿ç”¨16Ã—16 paddingç­–ç•¥
            if image_size >= 16:
                print(f"ğŸ”§ 196åŸºå› ä½¿ç”¨paddingç­–ç•¥: {image_size}Ã—{image_size} (padding {total_image_positions - 196}ä¸ªä½ç½®)")
                self.use_padding = True
                self.padding_size = total_image_positions - num_genes
            else:
                # 14Ã—14å®Œç¾åŒ¹é…æ¨¡å¼
                print(f"ğŸ”§ 196åŸºå› ä½¿ç”¨å®Œç¾åŒ¹é…: {image_size}Ã—{image_size}")
                self.use_padding = False
                self.padding_size = 0
        else:
            # å¯¹äºå…¶ä»–åŸºå› æ•°é‡ï¼Œå…è®¸çµæ´»åŒ¹é…
            if num_genes > total_image_positions:
                raise ValueError(f"åŸºå› æ•°é‡{num_genes}ä¸èƒ½å¤§äºå›¾åƒä½ç½®æ•°{image_size}Ã—{image_size}={total_image_positions}")
            self.use_padding = num_genes < total_image_positions
            self.padding_size = total_image_positions - num_genes
        
        self.num_genes = num_genes
        self.image_size = image_size
        self.histology_feature_dim = histology_feature_dim
        self.patch_nums = patch_nums
        
        # è®¡ç®—æ€»tokenæ•°
        self.total_tokens = sum(p*p for p in patch_nums)
        
        # ğŸ”‡ ç®€åŒ–åˆå§‹åŒ–è¾“å‡ºï¼Œå‡å°‘å†—ä½™ä¿¡æ¯
        print(f"ğŸ§¬ VARåŸºå› åŒ…è£…å™¨: {num_genes}åŸºå›  â†’ {image_size}Ã—{image_size}, {self.total_tokens}tokens")
        if num_genes == 196:
            print(f"   âœ… 196åŸºå› æ¨¡å¼ï¼šå®Œç¾åŒ¹é…14Ã—14")
        
        # ğŸ”‡ ç”Ÿç‰©å­¦è¯­ä¹‰æ˜¾ç¤ºå°†åœ¨åç»­è®¾ç½®
        self._biological_semantics = None  # å¾…çˆ¶ç±»ä¼ é€’
        
        # ğŸ”§ æ­¥éª¤1ï¼šåˆå§‹åŒ–åŸºå› é€‚é…å™¨ï¼ˆ196åŸºå›  â†’ 16Ã—16ä¼ªå›¾åƒï¼Œpaddingç­–ç•¥ï¼‰
        self.gene_adapter = GenePseudoImageAdapter(
            num_genes=num_genes,
            target_image_size=image_size,  # ğŸ”§ ä½¿ç”¨16Ã—16ï¼Œpaddingç­–ç•¥
            normalize_method='layer_norm',
            eps=1e-6
        )

        print(f"ğŸ§¬ åˆå§‹åŒ–åŸºå› é€‚é…å™¨:")
        print(f"   - åŸºå› æ•°é‡: {num_genes}")
        print(f"   - å›¾åƒå°ºå¯¸: {image_size}Ã—{image_size} (paddingç­–ç•¥)")
        print(f"   - Paddingå¤§å°: {image_size*image_size - num_genes}")
        print(f"   - ç©ºé—´åˆ©ç”¨ç‡: {num_genes/(image_size*image_size):.1%}")

        # éªŒè¯é€‚é…å™¨è½¬æ¢æ­£ç¡®æ€§
        print(f"ğŸ“Š éªŒè¯åŸºå› é€‚é…å™¨è½¬æ¢...")
        validation_result = self.gene_adapter.validate_conversion()
        if validation_result['conversion_successful']:
            print(f"   âœ… è½¬æ¢éªŒè¯æˆåŠŸ")
            print(f"   - æœ€å¤§é‡å»ºè¯¯å·®: {validation_result['max_reconstruction_error']:.2e}")
            print(f"   - å¹³å‡é‡å»ºè¯¯å·®: {validation_result['mean_reconstruction_error']:.2e}")
            print(f"   - PaddingåŒºåŸŸä¿æŒé›¶å€¼: {validation_result['padding_preserved']}")
        else:
            print(f"   âŒ è½¬æ¢éªŒè¯å¤±è´¥")
            print(f"   - è¯¯å·®: {validation_result['max_reconstruction_error']:.2e}")
        
        # ğŸ”§ æ­¥éª¤2ï¼šåˆå§‹åŒ–VQVAEé…ç½®ï¼ˆé€‚é…16Ã—16è¾“å…¥ï¼‰
        if VAR_AVAILABLE:
            print(f"ğŸ¨ ä½¿ç”¨å®Œæ•´VAR VQVAE (å•é€šé“è¾“å…¥ï¼Œ16Ã—16)")
            
            # ğŸ”§ ç¡®ä¿vqvae_configä¸ä¸ºNoneï¼Œæä¾›é»˜è®¤é…ç½®
            if vqvae_config is None:
                vqvae_config = {
                    'vocab_size': 4096,
                    'z_channels': 32,
                    'ch': 160,
                    'dropout': 0.0
                }
                print(f"   ğŸ”§ ä½¿ç”¨é»˜è®¤VQVAEé…ç½®: {vqvae_config}")
            
            # ğŸ”§ ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨VQVAEè€Œä¸æ˜¯å¤æ‚çš„é…ç½®ç±»
            # å› ä¸ºåŸå§‹VARé¡¹ç›®çš„VQVAEå¯èƒ½æœ‰ä¸åŒçš„åˆå§‹åŒ–æ–¹å¼
            try:
                # å°è¯•ç›´æ¥åˆå§‹åŒ–VQVAEï¼ˆå¦‚æœæ”¯æŒç®€å•å‚æ•°ï¼‰
                self.vqvae = VQVAE(
                    embed_dim=vqvae_config.get('embed_dim', 256),
                    n_embed=vqvae_config.get('n_embed', 8192),
                    double_z=vqvae_config.get('double_z', False),
                    z_channels=vqvae_config.get('z_channels', 256),
                    resolution=image_size,  # 16Ã—16åˆ†è¾¨ç‡
                    in_channels=1,  # ğŸ”§ å•é€šé“åŸºå› ä¼ªå›¾åƒ
                    out_ch=1,  # ğŸ”§ å•é€šé“è¾“å‡º
                    ch=vqvae_config.get('ch', 128),
                    ch_mult=vqvae_config.get('ch_mult', [1, 1, 2, 2, 4]),
                    num_res_blocks=vqvae_config.get('num_res_blocks', 2),
                    attn_resolutions=vqvae_config.get('attn_resolutions', [16]),  # é€‚é…16Ã—16
                    dropout=vqvae_config.get('dropout', 0.0)
                )
                print(f"   âœ… å®Œæ•´VAR VQVAEåˆå§‹åŒ–æˆåŠŸ (1â†’{vqvae_config.get('z_channels', 256)}â†’1é€šé“ï¼Œ16Ã—16)")
                
            except Exception as e:
                print(f"   âš ï¸ ç›´æ¥åˆå§‹åŒ–VQVAEå¤±è´¥: {e}")
                print(f"   ğŸ”„ å°è¯•ä»åŸå§‹VARé¡¹ç›®æŸ¥æ‰¾æ­£ç¡®çš„åˆå§‹åŒ–æ–¹å¼...")
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨VARé¡¹ç›®çš„æ ‡å‡†å‚æ•°ï¼Œç‰¹åˆ«æ˜¯v_patch_nums
                vqvae_kwargs = {
                    'vocab_size': vqvae_config.get('vocab_size', 4096),
                    'z_channels': vqvae_config.get('z_channels', 32),
                    'ch': vqvae_config.get('ch', 160),
                    'dropout': vqvae_config.get('dropout', 0.0),
                    'beta': 0.25,
                    'using_znorm': False,
                    'quant_conv_ks': 3,
                    'quant_resi': 0.5,
                    'share_quant_resi': 4,
                    'default_qresi_counts': 0,
                    'v_patch_nums': self.patch_nums,  # ğŸ”§ å…³é”®ï¼šä¼ é€’æ­£ç¡®çš„patch_numsåºåˆ—
                    'test_mode': False
                }
                
                self.vqvae = VQVAE(**vqvae_kwargs)
                print(f"   âœ… è‡ªé€‚åº”VQVAEåˆå§‹åŒ–æˆåŠŸï¼Œv_patch_nums={self.patch_nums}")
                
            except Exception as e2:
                print(f"   âŒ è‡ªé€‚åº”åˆå§‹åŒ–ä¹Ÿå¤±è´¥: {e2}")
                print(f"   ğŸ”„ ä½¿ç”¨æœ€å°åŒ–å‚æ•°åˆå§‹åŒ–...")
                
                # ğŸ”§ æœ€å°åŒ–å‚æ•°åˆå§‹åŒ–
                try:
                    # å°è¯•åªä¼ é€’æœ€å¿…è¦çš„å‚æ•°
                    self.vqvae = VQVAE(
                        vocab_size=4096,
                        z_channels=32,
                        v_patch_nums=self.patch_nums  # ğŸ”§ ç¡®ä¿ä¼ é€’patch_nums
                    )
                    print(f"   âš ï¸ ä½¿ç”¨é»˜è®¤å‚æ•°åˆå§‹åŒ–VQVAEï¼ˆå¯èƒ½éœ€è¦æ‰‹åŠ¨è°ƒæ•´ï¼‰")
                except Exception as e3:
                    print(f"   âŒ æ‰€æœ‰VQVAEåˆå§‹åŒ–å°è¯•éƒ½å¤±è´¥: {e3}")
                    raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–å®Œæ•´VAR VQVAE: {e3}")

        else:
            print(f"ğŸ¨ ä½¿ç”¨ç®€åŒ–VQVAE (å•é€šé“ï¼Œ16Ã—16)")
            # ç®€åŒ–VQVAEï¼Œé€‚é…16Ã—16è¾“å…¥
            self.vqvae = nn.Sequential(
                # ç¼–ç å™¨ï¼š1Ã—16Ã—16 â†’ 256Ã—4Ã—4  
                nn.Conv2d(1, 32, 3, padding=1),  # [B,1,16,16] â†’ [B,32,16,16]
                nn.ReLU(),
                nn.Conv2d(32, 64, 3, stride=2, padding=1),  # [B,32,16,16] â†’ [B,64,8,8]
                nn.ReLU(),
                nn.Conv2d(64, 128, 3, stride=2, padding=1),  # [B,64,8,8] â†’ [B,128,4,4]
                nn.ReLU(),
                nn.Conv2d(128, 256, 3, padding=1),  # [B,128,4,4] â†’ [B,256,4,4]
                
                # è§£ç å™¨ï¼š256Ã—4Ã—4 â†’ 1Ã—16Ã—16
                nn.ConvTranspose2d(256, 128, 3, padding=1),  # [B,256,4,4] â†’ [B,128,4,4]
                nn.ReLU(), 
                nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),  # [B,128,4,4] â†’ [B,64,8,8]
                nn.ReLU(),
                nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),  # [B,64,8,8] â†’ [B,32,16,16]
                nn.ReLU(),
                nn.ConvTranspose2d(32, 1, 3, padding=1),  # [B,32,16,16] â†’ [B,1,16,16]
                nn.Tanh()  # è¾“å‡ºèŒƒå›´[-1,1]
            )
            print(f"   âœ… ç®€åŒ–VQVAEåˆå§‹åŒ–å®Œæˆ (1â†’256â†’1é€šé“ï¼Œ16Ã—16)")

        print(f"   - è¾“å…¥æ ¼å¼: [B, 1, {image_size}, {image_size}] (å•é€šé“åŸºå› ä¼ªå›¾åƒ)")
        print(f"   - è¾“å‡ºæ ¼å¼: [B, 1, {image_size}, {image_size}] (é‡å»ºåŸºå› ä¼ªå›¾åƒ)")
        print(f"   - æ”¯æŒåˆ†è¾¨ç‡: 16Ã—16 (paddingç­–ç•¥è§£å†³å°ºå¯¸é™åˆ¶)")
        
        # ğŸ”§ Step 3: åˆå§‹åŒ–åŸºå› ä¸“ç”¨VARï¼ˆä»å¤´è®­ç»ƒï¼‰
        # print(f"ğŸ—ï¸ åˆå§‹åŒ–åŸºå› ä¸“ç”¨VARï¼ˆä»å¤´è®­ç»ƒï¼‰...")
        self._init_gene_var(var_config)
        
        # ğŸ”§ Step 4: æ¡ä»¶ç‰¹å¾å¤„ç†å™¨
        # æ˜ å°„ç»„ç»‡å­¦ç‰¹å¾åˆ°åŸºå› è¯­ä¹‰ç©ºé—´
        self.condition_processor = nn.Sequential(
            nn.Linear(histology_feature_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, self.var_embed_dim)  # æ˜ å°„åˆ°VARåµŒå…¥ç»´åº¦
        )
        
        print(f"ğŸ”§ æ¡ä»¶å¤„ç†å™¨é…ç½®:")
        print(f"   - è¾“å…¥ç»´åº¦: {histology_feature_dim}")
        print(f"   - VARåµŒå…¥ç»´åº¦: {self.var_embed_dim}")
        print(f"   - å¤„ç†é“¾: {histology_feature_dim} â†’ 512 â†’ 256 â†’ {self.var_embed_dim}")
        
        # è¾“å‡ºå‚æ•°ç»Ÿè®¡
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.parameters())
        print(f"ğŸ“Š å‚æ•°ç»Ÿè®¡ï¼ˆå…¨éƒ¨å¯è®­ç»ƒï¼‰:")
        print(f"   - æ€»å‚æ•°: {total_params:,}")
        print(f"   - å¯è®­ç»ƒå‚æ•°: {trainable_params:,} (100%)")
        print(f"âœ… VARåŸºå› åŒ…è£…å™¨åˆå§‹åŒ–å®Œæˆï¼ˆéœ€æ±‚å…¼å®¹ç‰ˆï¼‰")
        
        # ğŸ”§ åˆå§‹åŒ–æ—¥å¿—æ§åˆ¶å±æ€§
        self._verbose_logging = True  # é»˜è®¤å¯ç”¨è¯¦ç»†æ—¥å¿—
        self._step_count = 0  # æ­¥æ•°è®¡æ•°å™¨
    
    def _init_gene_var(self, var_config: Optional[Dict] = None):
        """åˆå§‹åŒ–åŸºå› ä¸“ç”¨VAR"""
        config = var_config or {}
        
        # åŸºå› VARé…ç½®
        self.var_embed_dim = config.get('embed_dim', 512)      # æ›´å°çš„åµŒå…¥ç»´åº¦
        self.var_depth = config.get('depth', 12)               # æ›´å°‘çš„å±‚æ•°
        self.var_num_heads = config.get('num_heads', 8)        # æ›´å°‘çš„æ³¨æ„åŠ›å¤´
        self.var_num_classes = config.get('num_classes', 10)   # åŸºå› è¡¨è¾¾ç±»å‹æ•°
        
        if VAR_AVAILABLE:
            self.var_model = VAR(
                vae_local=self.vqvae,
                num_classes=self.var_num_classes,
                depth=self.var_depth,
                embed_dim=self.var_embed_dim,
                num_heads=self.var_num_heads,
                mlp_ratio=config.get('mlp_ratio', 4.0),
                drop_rate=config.get('drop_rate', 0.1),
                attn_drop_rate=config.get('attn_drop_rate', 0.1),
                drop_path_rate=config.get('drop_path_rate', 0.1),
                norm_eps=config.get('norm_eps', 1e-6),
                shared_aln=config.get('shared_aln', False),
                cond_drop_rate=config.get('cond_drop_rate', 0.1),
                attn_l2_norm=config.get('attn_l2_norm', False),
                patch_nums=self.patch_nums,  # ğŸ”§ ä½¿ç”¨åŸºå› å¤šå°ºåº¦
                flash_if_available=config.get('flash_if_available', False),
                fused_if_available=config.get('fused_if_available', False)
            )
            print(f"   âœ… VAR: embed={self.var_embed_dim}, depth={self.var_depth}, heads={self.var_num_heads}")
        else:
            # ç®€åŒ–ç‰ˆVAR
            self.var_model = self._create_simple_var()
            print(f"   âš ï¸ ä½¿ç”¨ç®€åŒ–ç‰ˆVAR")
    
    def _create_simple_vqvae(self):
        """åˆ›å»ºç®€åŒ–ç‰ˆVQVAEï¼ˆå½“VARåº“ä¸å¯ç”¨æ—¶ï¼‰"""
        class SimpleVQVAE(nn.Module):
            def __init__(self, vocab_size=1024, z_channels=16):
                super().__init__()
                self.vocab_size = vocab_size
                self.z_channels = z_channels
                
                # ç®€å•çš„ç¼–ç å™¨
                self.encoder = nn.Sequential(
                    nn.Conv2d(1, 32, 3, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(32, 64, 3, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(64, z_channels, 3, padding=1)
                )
                
                # é‡åŒ–å±‚
                self.quantize = nn.Embedding(vocab_size, z_channels)
                
                # ç®€å•çš„è§£ç å™¨
                self.decoder = nn.Sequential(
                    nn.Conv2d(z_channels, 64, 3, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(64, 32, 3, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(32, 1, 3, padding=1)
                )
            
            def img_to_idxBl(self, x):
                # ç¼–ç å¹¶é‡åŒ–
                z = self.encoder(x)
                B, C, H, W = z.shape
                z_flat = z.view(B, C, -1).permute(0, 2, 1)
                
                # ç®€å•é‡åŒ–ï¼šæ‰¾æœ€è¿‘çš„codebookå‘é‡
                distances = torch.cdist(z_flat, self.quantize.weight)
                indices = torch.argmin(distances, dim=-1)
                
                # ä¸ºæ¯ä¸ªpatch_numåˆ›å»ºtokens
                tokens = []
                for p in self.training_patch_nums if hasattr(self, 'training_patch_nums') else [1,2,3,4,5]:
                    patch_size = H // p
                    if patch_size > 0:
                        # ç®€å•åœ°é‡å¤ç´¢å¼•
                        patch_tokens = indices[:, :p*p] if indices.shape[1] >= p*p else indices[:, :1].repeat(1, p*p)
                        tokens.append(patch_tokens)
                    else:
                        tokens.append(indices[:, :1])
                
                return tokens
            
            def idxBl_to_img(self, tokens, same_shape=True, last_one=True):
                # ç®€å•é‡å»ºï¼šä½¿ç”¨æœ€åä¸€ä¸ªtoken
                if last_one and len(tokens) > 0:
                    indices = tokens[-1]  # ä½¿ç”¨æœ€ç»†å°ºåº¦çš„tokens
                else:
                    indices = tokens[0] if tokens else torch.zeros(1, 1, dtype=torch.long)
                
                # é‡å»º
                B = indices.shape[0]
                quantized = self.quantize(indices)
                
                # ç®€å•åœ°reshapeå¹¶è§£ç 
                if quantized.dim() == 3:
                    H = W = int(np.sqrt(quantized.shape[1]))
                    if H * W != quantized.shape[1]:
                        H = W = 14  # é»˜è®¤å°ºå¯¸
                        quantized = quantized[:, :H*W]
                    z = quantized.permute(0, 2, 1).view(B, -1, H, W)
                else:
                    z = quantized.unsqueeze(-1).unsqueeze(-1)
                
                return self.decoder(z)
            
            def forward(self, x):
                tokens = self.img_to_idxBl(x)
                recon = self.idxBl_to_img(tokens)
                return {'tokens': tokens, 'recon': recon, 'vq_loss': torch.tensor(0.0)}
        
        vqvae = SimpleVQVAE(self.vqvae_vocab_size, self.vqvae_z_channels)
        vqvae.training_patch_nums = self.patch_nums
        return vqvae
    
    def _create_simple_var(self):
        """åˆ›å»ºç®€åŒ–ç‰ˆVARï¼ˆå½“VARåº“ä¸å¯ç”¨æ—¶ï¼‰"""
        class SimpleVAR(nn.Module):
            def __init__(self, embed_dim=512, depth=12, num_heads=8, vocab_size=1024):
                super().__init__()
                self.embed_dim = embed_dim
                self.token_embedding = nn.Embedding(vocab_size, embed_dim)
                self.pos_embedding = nn.Parameter(torch.randn(1, 100, embed_dim))  # è¶³å¤Ÿå¤§çš„ä½ç½®ç¼–ç 
                
                # Transformer layers
                encoder_layer = nn.TransformerEncoderLayer(
                    d_model=embed_dim,
                    nhead=num_heads,
                    dim_feedforward=embed_dim * 4,
                    dropout=0.1,
                    batch_first=True
                )
                self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)
                
                # è¾“å‡ºå±‚
                self.output_layer = nn.Linear(embed_dim, vocab_size)
                
            def forward(self, label_B=None, x_BLCv_wo_first_l=None, is_train=True, **kwargs):
                if x_BLCv_wo_first_l is None:
                    return torch.tensor(0.0, requires_grad=True)
                
                # å¤„ç†å¤šå°ºåº¦tokens
                if isinstance(x_BLCv_wo_first_l, list):
                    # è¿æ¥æ‰€æœ‰å°ºåº¦çš„tokens
                    all_tokens = []
                    for tokens in x_BLCv_wo_first_l:
                        if tokens.dim() == 2:
                            all_tokens.append(tokens)
                        elif tokens.dim() == 3:
                            all_tokens.append(tokens.flatten(1))
                    
                    if all_tokens:
                        tokens = torch.cat(all_tokens, dim=1)
                    else:
                        B = x_BLCv_wo_first_l[0].shape[0] if x_BLCv_wo_first_l else 1
                        tokens = torch.zeros(B, 10, dtype=torch.long, device=next(self.parameters()).device)
                else:
                    tokens = x_BLCv_wo_first_l
                
                # åµŒå…¥
                B, L = tokens.shape[:2]
                x = self.token_embedding(tokens)
                
                # ä½ç½®ç¼–ç 
                if L <= self.pos_embedding.shape[1]:
                    x = x + self.pos_embedding[:, :L]
                
                # Transformer
                x = self.transformer(x)
                
                # è¾“å‡º
                logits = self.output_layer(x)
                
                # è®¡ç®—æŸå¤±
                if is_train:
                    # ç®€å•çš„è‡ªå›å½’æŸå¤±
                    targets = tokens[:, 1:] if L > 1 else tokens
                    preds = logits[:, :-1] if L > 1 else logits
                    
                    if targets.numel() > 0 and preds.numel() > 0:
                        loss = F.cross_entropy(preds.reshape(-1, preds.shape[-1]), targets.reshape(-1))
                    else:
                        loss = torch.tensor(0.0, requires_grad=True)
                    
                    return loss
                else:
                    return logits
            
            def autoregressive_infer_cfg(self, B, label_B=None, cfg=1.5, top_k=50, top_p=0.9, more_smooth=False):
                # ç®€å•ç”Ÿæˆ
                device = next(self.parameters()).device
                generated = torch.zeros(B, 3, 14, 14, device=device)  # ç›´æ¥ç”Ÿæˆç›®æ ‡å°ºå¯¸
                return generated
        
        return SimpleVAR(self.var_embed_dim, self.var_depth, self.var_num_heads, self.vqvae_vocab_size)
    
    def forward_training(
        self,
        gene_expression: torch.Tensor,
        histology_features: torch.Tensor,
        class_labels: Optional[torch.Tensor] = None,
        show_details: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        è®­ç»ƒé˜¶æ®µå‰å‘ä¼ æ’­ - åŸºå› å¤šå°ºåº¦ä»å¤´è®­ç»ƒç‰ˆæœ¬
        
        æµç¨‹ï¼š
        1. åŸºå› è¡¨è¾¾ [B, 196] â†’ ä¼ªå›¾åƒ [B, 1, 14, 14]
        2. VQVAEç¼–ç  â†’ åŸºå› å¤šå°ºåº¦tokens (1,2,3,4,5)
        3. VARè‡ªå›å½’è®­ç»ƒ
        4. é‡å»ºéªŒè¯
        
        Args:
            gene_expression: [B, 196] - åŸºå› è¡¨è¾¾å‘é‡
            histology_features: [B, feature_dim] - ç»„ç»‡å­¦ç‰¹å¾
            class_labels: [B] - ç±»åˆ«æ ‡ç­¾(å¯é€‰)
            show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
        Returns:
            åŒ…å«æŸå¤±å’Œé¢„æµ‹ç»“æœçš„å­—å…¸
        """
        B, num_genes = gene_expression.shape
        device = gene_expression.device
        
        # ğŸ”§ å¢åŠ æ­¥æ•°è®¡æ•°
        self._step_count += 1
        
        # éªŒè¯è¾“å…¥ç»´åº¦
        if num_genes != self.num_genes:
            raise ValueError(f"è¾“å…¥åŸºå› æ•°é‡{num_genes}ä¸æ¨¡å‹æœŸæœ›{self.num_genes}ä¸åŒ¹é…")
        
        # ğŸ”‡ å¤§å¹…å‡å°‘è¯¦ç»†è¾“å‡ºï¼šåªåœ¨å‰3æ­¥å’Œæ¯1000æ­¥æ˜¾ç¤ºè¯¦æƒ…
        # ğŸ”§ åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œåªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        import os
        is_main_process = int(os.environ.get('LOCAL_RANK', 0)) == 0
        show_details = (self._verbose_logging and is_main_process and 
                       (self._step_count <= 3 or self._step_count % 1000 == 0))
        
        if show_details:
            print(f"ğŸ§¬ åŸºå› å¤šå°ºåº¦VARè®­ç»ƒ:")
            print(f"   è¾“å…¥åŸºå› è¡¨è¾¾: {gene_expression.shape}")
        
        try:
            # Step 1: åŸºå› è¡¨è¾¾ â†’ ä¼ªå›¾åƒ [B, 1, 14, 14]
            pseudo_images = self.gene_adapter.genes_to_pseudo_image(gene_expression)
            if show_details:
                print(f"   ä¼ªå›¾åƒè½¬æ¢: {gene_expression.shape} â†’ {pseudo_images.shape}")
                if self.use_padding:
                    print(f"   ä½¿ç”¨padding: {self.num_genes}åŸºå›  + {self.padding_size}padding = {self.image_size}Ã—{self.image_size}")
            
            # ç¡®ä¿ä¼ªå›¾åƒé€šé“æ•°ä¸º1ï¼ˆåŸºå› æ•°æ®æ˜¯å•é€šé“çš„ï¼‰
            if pseudo_images.shape[1] != 1:
                if pseudo_images.shape[1] == 3:
                    # å¦‚æœé€‚é…å™¨è¿”å›3é€šé“ï¼Œè½¬ä¸ºå•é€šé“
                    pseudo_images = pseudo_images.mean(dim=1, keepdim=True)
                else:
                    # å–ç¬¬ä¸€ä¸ªé€šé“
                    pseudo_images = pseudo_images[:, :1]
            
            # Step 2: å¤„ç†æ¡ä»¶ç‰¹å¾
            if class_labels is None:
                # é€šè¿‡æ¡ä»¶å¤„ç†å™¨ç”ŸæˆåŸºå› è¯­ä¹‰æ ‡ç­¾
                condition_embeddings = self.condition_processor(histology_features)  # [B, embed_dim]
                # ç®€å•æ˜ å°„åˆ°ç±»åˆ«æ ‡ç­¾
                class_labels = torch.argmax(condition_embeddings[:, :self.var_num_classes], dim=-1)
            
            # ğŸ”‡ å‡å°‘æ¡ä»¶æ ‡ç­¾è¾“å‡ºï¼Œåªåœ¨è¯¦ç»†æ¨¡å¼æ˜¾ç¤º
            # if show_details:
            #     print(f"   æ¡ä»¶æ ‡ç­¾: {class_labels.shape}")
            
            # ğŸ”§ æ–¹æ¡ˆ1ï¼šå°†å•é€šé“åŸºå› ä¼ªå›¾åƒè½¬æ¢ä¸º3é€šé“ï¼Œé€‚é…åŸå§‹VAR VQVAE
            # åŸå§‹VARçš„VQVAEç¼–ç å™¨æœŸæœ›3é€šé“RGBè¾“å…¥ï¼Œæˆ‘ä»¬éœ€è¦é€‚é…
            if pseudo_images.shape[1] == 1:
                # å°†å•é€šé“å¤åˆ¶ä¸º3é€šé“: [B, 1, H, W] â†’ [B, 3, H, W]
                pseudo_images_3ch = pseudo_images.repeat(1, 3, 1, 1)
                if show_details:
                    print(f"   ğŸ”§ é€šé“é€‚é…: {pseudo_images.shape} â†’ {pseudo_images_3ch.shape} (é€‚é…VAR VQVAE)")
            else:
                pseudo_images_3ch = pseudo_images
            
            # ç¡®ä¿ä¼ªå›¾åƒæ ¼å¼æ­£ç¡®ä¸º3é€šé“
            if pseudo_images_3ch.shape[1] != 3:
                if pseudo_images_3ch.shape[1] > 3:
                    # å–å‰3ä¸ªé€šé“
                    pseudo_images_3ch = pseudo_images_3ch[:, :3]
                    if show_details:
                        print(f"   ğŸ”§ æˆªå–å‰3é€šé“: â†’ {pseudo_images_3ch.shape}")
                else:
                    # ä¸è¶³3é€šé“ï¼Œè¡¥é½
                    channels_needed = 3 - pseudo_images_3ch.shape[1]
                    padding_channels = pseudo_images_3ch[:, :1].repeat(1, channels_needed, 1, 1)
                    pseudo_images_3ch = torch.cat([pseudo_images_3ch, padding_channels], dim=1)
                    if show_details:
                        print(f"   ğŸ”§ è¡¥é½åˆ°3é€šé“: â†’ {pseudo_images_3ch.shape}")
            
            # Step 3: VQVAEç¼–ç  â†’ åŸºå› å¤šå°ºåº¦tokens (ä½¿ç”¨3é€šé“å›¾åƒ)
            ms_tokens = self.vqvae.img_to_idxBl(pseudo_images_3ch)
            # ğŸ”‡ å‡å°‘tokensè¾“å‡º
            # if show_details:
            #     print(f"   å¤šå°ºåº¦tokens: {[t.shape if isinstance(t, torch.Tensor) else 'None' for t in ms_tokens]}")
            
            # ç¡®ä¿tokensæ ¼å¼æ­£ç¡®
            if not isinstance(ms_tokens, list):
                ms_tokens = [ms_tokens]
            
            # è¿‡æ»¤æ— æ•ˆtokens
            valid_tokens = []
            for i, tokens in enumerate(ms_tokens):
                if isinstance(tokens, torch.Tensor) and tokens.numel() > 0:
                    valid_tokens.append(tokens)
                else:
                    # åˆ›å»ºé»˜è®¤tokens
                    patch_size = self.patch_nums[i] if i < len(self.patch_nums) else 1
                    default_tokens = torch.zeros(B, patch_size*patch_size, dtype=torch.long, device=device)
                    valid_tokens.append(default_tokens)
            
            ms_tokens = valid_tokens
            
            # Step 4: VARè‡ªå›å½’è®­ç»ƒ
            # ğŸ”§ å¤„ç†multi-scale tokensï¼Œè½¬æ¢ä¸ºVARæœŸæœ›çš„æ ¼å¼
            if len(ms_tokens) == 0:
                # åˆ›å»ºé»˜è®¤tokens
                total_tokens = sum(p*p for p in self.patch_nums)
                default_tokens = torch.zeros(B, total_tokens, dtype=torch.long, device=device)
                ms_tokens = [default_tokens]

            # VARæœŸæœ›çš„æ ¼å¼ï¼šteacher forcing input [B, L-first_l, Cvae]
            # å…¶ä¸­ L = sum(patch_nums^2)ï¼Œfirst_l = patch_nums[0]^2
            total_tokens = sum(p*p for p in self.patch_nums)
            first_l = self.patch_nums[0] ** 2

            if isinstance(ms_tokens, list) and len(ms_tokens) > 1:
                # æ‹¼æ¥æ‰€æœ‰å°ºåº¦çš„tokens
                all_tokens = torch.cat([tokens.flatten(1) for tokens in ms_tokens], dim=1)  # [B, total_tokens]
            else:
                # å•ä¸ªtoken tensor
                tokens = ms_tokens[0] if isinstance(ms_tokens, list) else ms_tokens
                if tokens.numel() == B * total_tokens:
                    all_tokens = tokens.view(B, total_tokens)
                else:
                    # å¦‚æœtokenæ•°é‡ä¸åŒ¹é…ï¼Œåˆ›å»ºé»˜è®¤tokens
                    all_tokens = torch.zeros(B, total_tokens, dtype=torch.long, device=device)

            # ğŸ”§ å…³é”®ä¿®å¤ï¼šVARéœ€è¦é™¤äº†ç¬¬ä¸€å±‚å¤–çš„æ‰€æœ‰tokensçš„embedding
            # x_BLCv_wo_first_l: [B, L-first_l, Cvae]
            tokens_wo_first_l = all_tokens[:, first_l:]  # ç§»é™¤ç¬¬ä¸€å±‚tokens [B, L-first_l]

            # å°†token indicesè½¬æ¢ä¸ºVQVAE embeddings
            if hasattr(self.vqvae, 'quantize') and hasattr(self.vqvae.quantize, 'embedding'):
                # ä½¿ç”¨VQVAEçš„é‡åŒ–å™¨è·å–embeddings
                x_BLCv_wo_first_l = self.vqvae.quantize.embedding(tokens_wo_first_l)  # [B, L-first_l, Cvae]
            elif hasattr(self.vqvae, 'vae_quant_proxy'):
                # ä½¿ç”¨VARçš„ä»£ç†é‡åŒ–å™¨
                x_BLCv_wo_first_l = self.vqvae.vae_quant_proxy[0].embedding(tokens_wo_first_l)
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šéšæœºåˆå§‹åŒ–embeddings
                Cvae = getattr(self.vqvae, 'Cvae', 256)  # é»˜è®¤256ç»´
                x_BLCv_wo_first_l = torch.randn(B, tokens_wo_first_l.shape[1], Cvae, device=device)
                print(f"   âš ï¸ ä½¿ç”¨éšæœºembeddingsï¼Œå½¢çŠ¶: {x_BLCv_wo_first_l.shape}")

            if show_details:
                print(f"   ğŸ”§ VARè¾“å…¥è½¬æ¢: tokens{all_tokens.shape} â†’ embeddings{x_BLCv_wo_first_l.shape}")

            var_output = self.var_model(
                label_B=class_labels,
                x_BLCv_wo_first_l=x_BLCv_wo_first_l
            )
            
            # æå–VARæŸå¤±
            if isinstance(var_output, dict):
                var_loss = var_output.get('loss', var_output.get('ce_loss', 0.0))
            elif isinstance(var_output, torch.Tensor):
                var_loss = var_output
            else:
                var_loss = torch.tensor(0.0, device=device, requires_grad=True)
            
            # ğŸ”§ ç¡®ä¿var_lossæ˜¯æ ‡é‡
            if isinstance(var_loss, torch.Tensor):
                if var_loss.numel() > 1:
                    # å¦‚æœvar_lossæ˜¯å¤šç»´tensorï¼Œå–å¹³å‡å€¼
                    var_loss = var_loss.mean()
                elif var_loss.numel() == 0:
                    # å¦‚æœæ˜¯ç©ºtensorï¼Œè®¾ä¸º0
                    var_loss = torch.tensor(0.0, device=device, requires_grad=True)
            else:
                # å¦‚æœä¸æ˜¯tensorï¼Œè½¬æ¢ä¸ºtensor
                var_loss = torch.tensor(float(var_loss), device=device, requires_grad=True)
            
            # ğŸ”‡ åªåœ¨è¯¦ç»†æ¨¡å¼æ˜¾ç¤ºVARæŸå¤±
            # if show_details:
            #     print(f"   VARæŸå¤±: {var_loss.item():.4f}")
            
            # Step 5: é‡å»ºéªŒè¯
            with torch.no_grad():
                # è§£ç tokenså›ä¼ªå›¾åƒ
                reconstructed_images = self.vqvae.idxBl_to_img(ms_tokens, same_shape=True, last_one=True)
                
                # ç¡®ä¿é‡å»ºå›¾åƒå°ºå¯¸æ­£ç¡®
                if reconstructed_images.shape[-2:] != (self.image_size, self.image_size):
                    # ğŸ”‡ å‡å°‘å°ºå¯¸è°ƒæ•´è¾“å‡º
                    # if show_details:
                    #     print(f"   è°ƒæ•´é‡å»ºå›¾åƒå°ºå¯¸: {reconstructed_images.shape} â†’ [B, 1, {self.image_size}, {self.image_size}]")
                    reconstructed_images = F.interpolate(
                        reconstructed_images,
                        size=(self.image_size, self.image_size),
                        mode='bilinear',
                        align_corners=False
                    )
                
                # ç¡®ä¿é€šé“æ•°æ­£ç¡®
                if reconstructed_images.shape[1] != 1:
                    if reconstructed_images.shape[1] == 3:
                        # RGB â†’ ç°åº¦
                        reconstructed_images = reconstructed_images.mean(dim=1, keepdim=True)
                    else:
                        # å–ç¬¬ä¸€ä¸ªé€šé“
                        reconstructed_images = reconstructed_images[:, :1]
                
                # ä¼ªå›¾åƒ â†’ åŸºå› è¡¨è¾¾
                predicted_genes = self.gene_adapter.pseudo_image_to_genes(reconstructed_images)
                
                # é‡å»ºæŸå¤±
                recon_loss = F.mse_loss(predicted_genes, gene_expression)
                
                # ğŸ”‡ åªåœ¨è¯¦ç»†æ¨¡å¼æ˜¾ç¤ºé‡å»ºæŸå¤±
                # if show_details:
                #     print(f"   é‡å»ºæŸå¤±: {recon_loss.item():.4f}")
            
            # Step 6: VQVAEé‡åŒ–æŸå¤±ï¼ˆå¦‚æœæœ‰ï¼‰
            vq_loss = torch.tensor(0.0, device=device, requires_grad=True)
            if hasattr(self.vqvae, 'vq_loss') and self.vqvae.vq_loss is not None:
                vq_loss = self.vqvae.vq_loss
                # ğŸ”§ ç¡®ä¿vq_lossæ˜¯æ ‡é‡
                if isinstance(vq_loss, torch.Tensor) and vq_loss.numel() > 1:
                    vq_loss = vq_loss.mean()

            # ğŸ”§ ç¡®ä¿recon_lossæ˜¯æ ‡é‡
            if isinstance(recon_loss, torch.Tensor) and recon_loss.numel() > 1:
                recon_loss = recon_loss.mean()

            # æ€»æŸå¤±ç»„åˆ
            total_loss = var_loss + 0.1 * recon_loss + 0.01 * vq_loss

            if show_details:
                # ğŸ”§ å®‰å…¨çš„æŸå¤±æ˜¾ç¤ºï¼šç¡®ä¿æ‰€æœ‰æŸå¤±éƒ½æ˜¯æ ‡é‡
                try:
                    var_loss_val = var_loss.item() if isinstance(var_loss, torch.Tensor) else float(var_loss)
                    recon_loss_val = recon_loss.item() if isinstance(recon_loss, torch.Tensor) else float(recon_loss)
                    vq_loss_val = vq_loss.item() if isinstance(vq_loss, torch.Tensor) else float(vq_loss)
                    total_loss_val = total_loss.item() if isinstance(total_loss, torch.Tensor) else float(total_loss)
                    print(f"   æ€»æŸå¤±: {total_loss_val:.4f} (VAR: {var_loss_val:.4f}, é‡å»º: {recon_loss_val:.4f}, VQ: {vq_loss_val:.4f})")
                except Exception as loss_display_error:
                    print(f"   ğŸ’¡ æŸå¤±è®¡ç®—æˆåŠŸï¼Œä½†æ˜¾ç¤ºå‡ºé”™: {loss_display_error}")
                    print(f"   - VARæŸå¤±å½¢çŠ¶: {var_loss.shape if hasattr(var_loss, 'shape') else type(var_loss)}")
                    print(f"   - é‡å»ºæŸå¤±å½¢çŠ¶: {recon_loss.shape if hasattr(recon_loss, 'shape') else type(recon_loss)}")
                    print(f"   - æ€»æŸå¤±å½¢çŠ¶: {total_loss.shape if hasattr(total_loss, 'shape') else type(total_loss)}")
        
        except Exception as e:
            print(f"   âŒ VARè®­ç»ƒè¿‡ç¨‹å¤±è´¥: {e}")
            print(f"   ğŸ’¡ è¿™é€šå¸¸è¡¨ç¤º:")
            print(f"      1. VARæ¨¡å‹å¯¼å…¥å¤±è´¥")
            print(f"      2. VQVAEç¼–ç å‡ºé”™")
            print(f"      3. æ•°æ®ç»´åº¦ä¸åŒ¹é…")
            print(f"      4. GPUå†…å­˜ä¸è¶³")
            import traceback
            traceback.print_exc()
            
            # ğŸ”§ ä¸å†ä½¿ç”¨å›é€€æ–¹æ¡ˆï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼
            raise RuntimeError(f"VAR-STæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}") from e
        
        return {
            'loss': total_loss,
            'var_loss': var_loss,
            'recon_loss': recon_loss,
            'vq_loss': vq_loss,
            'predictions': predicted_genes,
            'targets': gene_expression,
            'class_labels': class_labels,
            'pseudo_images': pseudo_images,
            'tokens': ms_tokens
        }
    
    def forward_inference(
        self,
        histology_features: torch.Tensor,
        class_labels: Optional[torch.Tensor] = None,
        cfg_scale: float = 1.5,
        top_k: int = 50,
        top_p: float = 0.9,
        temperature: float = 1.0,
        num_samples: int = 1
    ) -> Dict[str, torch.Tensor]:
        """
        æ¨ç†å‰å‘ä¼ æ’­ï¼šä»ç»„ç»‡å­¦ç‰¹å¾ç”ŸæˆåŸºå› è¡¨è¾¾
        
        Args:
            histology_features: [B, feature_dim] ç»„ç»‡å­¦ç‰¹å¾
            class_labels: [B] å¯é€‰çš„ç±»åˆ«æ ‡ç­¾
            cfg_scale: Classifier-free guidanceç¼©æ”¾
            å…¶ä»–å‚æ•°ï¼šVARç”Ÿæˆå‚æ•°
        
        Returns:
            ç”Ÿæˆçš„åŸºå› è¡¨è¾¾é¢„æµ‹
        """
        B = histology_features.shape[0]
        device = histology_features.device
        
        # 1. å¤„ç†æ¡ä»¶
        if class_labels is None:
            condition_embeddings = self.condition_processor(histology_features)
            class_labels = torch.argmax(condition_embeddings[:, :self.var_num_classes], dim=-1)
        
        # 2. VARè‡ªå›å½’ç”Ÿæˆ æˆ– ç®€å•æ¨ç†
        try:
            if hasattr(self.var_model, 'autoregressive_infer_cfg'):
                # ä½¿ç”¨VARçš„è‡ªå›å½’ç”Ÿæˆ
                generated_images = self.var_model.autoregressive_infer_cfg(
                    B=B * num_samples,
                    label_B=class_labels.repeat(num_samples) if num_samples > 1 else class_labels,
                    cfg=cfg_scale,
                    top_k=top_k,
                    top_p=top_p,
                    more_smooth=False
                )
            else:
                # ç®€åŒ–æ¨ç†ï¼šç›´æ¥ç”Ÿæˆéšæœºä¼ªå›¾åƒ
                generated_images = torch.randn(B * num_samples, 1, self.image_size, self.image_size, device=device)
                
        except Exception as e:
            print(f"âš ï¸ VARæ¨ç†å¼‚å¸¸: {e}ï¼Œä½¿ç”¨éšæœºç”Ÿæˆ")
            # å›é€€ï¼šç”Ÿæˆéšæœºä¼ªå›¾åƒ
            generated_images = torch.randn(B * num_samples, 1, self.image_size, self.image_size, device=device)
        
        # 3. è°ƒæ•´ç”Ÿæˆå›¾åƒæ ¼å¼
        if generated_images.shape[-2:] != (self.image_size, self.image_size):
            generated_images = F.interpolate(
                generated_images,
                size=(self.image_size, self.image_size),
                mode='bilinear',
                align_corners=False
            )
        
        # ç¡®ä¿é€šé“æ•°ä¸º1ï¼ˆç°åº¦ï¼‰
        if generated_images.shape[1] != 1:
            if generated_images.shape[1] == 3:
                generated_images = generated_images.mean(dim=1, keepdim=True)
            else:
                generated_images = generated_images[:, :1]
        
        # 4. ç”Ÿæˆçš„ä¼ªå›¾åƒ â†’ åŸºå› è¡¨è¾¾
        predicted_genes = self.gene_adapter.pseudo_image_to_genes(generated_images)
        
        # 5. é‡å¡‘è¾“å‡º
        if num_samples > 1:
            predicted_genes = predicted_genes.view(B, num_samples, self.num_genes)
            generated_images = generated_images.view(B, num_samples, 1, self.image_size, self.image_size)
        
        return {
            'predictions': predicted_genes,
            'generated_images': generated_images,
            'class_labels': class_labels,
            'predicted_expression': predicted_genes
        }
    
    def forward(self, **inputs) -> Dict[str, torch.Tensor]:
        """ç»Ÿä¸€å‰å‘ä¼ æ’­æ¥å£"""
        mode = inputs.get('mode', 'training')
        
        if mode == 'training':
            return self.forward_training(
                gene_expression=inputs['gene_expression'],
                histology_features=inputs['histology_features'],
                class_labels=inputs.get('class_labels'),
                show_details=inputs.get('show_details', False)
            )
        else:
            return self.forward_inference(
                histology_features=inputs['histology_features'],
                class_labels=inputs.get('class_labels'),
                cfg_scale=inputs.get('cfg_scale', 1.5),
                top_k=inputs.get('top_k', 50),
                top_p=inputs.get('top_p', 0.9),
                temperature=inputs.get('temperature', 1.0),
                num_samples=inputs.get('num_samples', 1)
            ) 