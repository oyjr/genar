"""
VARåŸºå› åŒ…è£…å™¨ - å®Œæ•´VARå®ç°ç‰ˆæœ¬

åŸºäºçœŸæ­£çš„VARæ¶æ„é‡å†™ï¼ŒåŒ…å«ï¼š
- AdaLNSelfAttn: æ¡ä»¶è‡ªé€‚åº”LayerNorm
- å¤šå°ºåº¦è‡ªå›å½’ç”Ÿæˆ
- æ­£ç¡®çš„ä½ç½®ç¼–ç å’Œå±‚çº§ç¼–ç 
- Causal attention mask
- å®Œæ•´çš„autoregressiveæ¨ç†

æ— å¤–éƒ¨ä¾èµ–ï¼Œå®Œå…¨å†…ç½®å®ç°ã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import sys
import os
from typing import Dict, List, Optional, Tuple, Any, Union
import numpy as np
import math

# å¯¼å…¥æœ¬åœ°åŸºå› é€‚é…å™¨å’ŒVARåŸºç¡€ç»„ä»¶
from .gene_pseudo_image_adapter import GenePseudoImageAdapter
from .var_basic_components import *

class VARGeneWrapper(nn.Module):
    """
    VARåŸºå› åŒ…è£…å™¨ - å®Œæ•´VARå®ç°ç‰ˆæœ¬
    
    åŸºäºçœŸæ­£çš„VARæ¶æ„è®¾è®¡ï¼š
    - 196ä¸ªåŸºå›  â†’ 14Ã—14Ã—1 ä¼ªå›¾åƒ
    - å¤šå°ºåº¦patch_nums: (1,2,4) = 21 tokens
    - AdaLNæ¡ä»¶æ³¨å…¥æœºåˆ¶
    - å®Œæ•´çš„è‡ªå›å½’ç”Ÿæˆ
    """
    
    def __init__(
        self,
        histology_feature_dim: int,  # ğŸ”§ å¿…éœ€å‚æ•°æ”¾åœ¨å‰é¢
        num_genes: int = 196,
        image_size: int = 64,
        patch_nums: Tuple[int, ...] = (1, 2, 4),
        var_config: Optional[Dict] = None,
        vqvae_config: Optional[Dict] = None,
        adapter_config: Optional[Dict] = None,
        progressive_training: bool = True,
        warmup_steps: int = 1000,
        min_recon_weight: float = 0.5,
        max_recon_weight: float = 5.0
    ):
        super().__init__()
        
        # ğŸ”§ ä¸¥æ ¼å‚æ•°éªŒè¯ï¼Œä¸å…è®¸Noneæˆ–æ— æ•ˆå€¼
        if histology_feature_dim is None or histology_feature_dim <= 0:
            raise ValueError(f"histology_feature_dimå¿…é¡»æ˜¯æ­£æ•´æ•°ï¼Œå¾—åˆ°: {histology_feature_dim}")
        
        if num_genes <= 0:
            raise ValueError(f"num_geneså¿…é¡»æ˜¯æ­£æ•´æ•°ï¼Œå¾—åˆ°: {num_genes}")
            
        if image_size <= 0:
            raise ValueError(f"image_sizeå¿…é¡»æ˜¯æ­£æ•´æ•°ï¼Œå¾—åˆ°: {image_size}")
            
        if not patch_nums or any(p <= 0 for p in patch_nums):
            raise ValueError(f"patch_numså¿…é¡»æ˜¯æ­£æ•´æ•°åºåˆ—ï¼Œå¾—åˆ°: {patch_nums}")
        
        # ä¿å­˜é…ç½®
        self.progressive_training = progressive_training
        self.warmup_steps = warmup_steps
        self.min_recon_weight = min_recon_weight
        self.max_recon_weight = max_recon_weight
        self.current_step = 0
        
        # åŸºç¡€é…ç½®
        self.num_genes = num_genes
        self.image_size = image_size
        self.histology_feature_dim = histology_feature_dim
        self.patch_nums = patch_nums
        
        # éªŒè¯åŸºå› æ•°é‡
        if num_genes == 196:
            self.use_upsampling = True
            self.intermediate_size = 14
        else:
            self.use_upsampling = False
            self.intermediate_size = int(math.sqrt(num_genes))
            if self.intermediate_size ** 2 != num_genes:
                raise ValueError(f"åŸºå› æ•°é‡{num_genes}ä¸æ˜¯å®Œå…¨å¹³æ–¹æ•°")
        
        print(f"ğŸ§¬ VARåŸºå› åŒ…è£…å™¨ - å®Œæ•´å®ç°ç‰ˆæœ¬")
        print(f"   åŸºå› æ•°é‡: {num_genes} â†’ {image_size}Ã—{image_size}")
        print(f"   å¤šå°ºåº¦patch_nums: {patch_nums}")
        
        # è®¡ç®—å¤šå°ºåº¦å‚æ•°
        self.L = sum(pn ** 2 for pn in self.patch_nums)  # æ€»tokenæ•°
        self.first_l = self.patch_nums[0] ** 2  # ç¬¬ä¸€å±‚tokenæ•°
        self.begin_ends = []
        cur = 0
        for i, pn in enumerate(self.patch_nums):
            self.begin_ends.append((cur, cur + pn ** 2))
            cur += pn ** 2
        
        print(f"   æ€»tokenæ•°: {self.L}, ç¬¬ä¸€å±‚: {self.first_l}")
        print(f"   å„å±‚èŒƒå›´: {self.begin_ends}")
        
        # åˆå§‹åŒ–åŸºå› é€‚é…å™¨
        self.gene_adapter = GenePseudoImageAdapter(
            num_genes=num_genes,
            intermediate_size=self.intermediate_size,
            target_image_size=image_size,
            normalize_method='none',
            eps=1e-6
        )

        # åˆå§‹åŒ–VQVAE
        self._init_vqvae(vqvae_config)
        
        # åˆå§‹åŒ–VAR
        self._init_full_var(var_config)
        
        # åˆå§‹åŒ–æ¡ä»¶å¤„ç†å™¨
        self._init_condition_processor()
        
        # è¾“å‡ºç»Ÿè®¡
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"ğŸ“Š æ€»å‚æ•°: {trainable_params:,}")
        print(f"âœ… VARåŸºå› åŒ…è£…å™¨åˆå§‹åŒ–å®Œæˆ")
        
        self._verbose_logging = True
        self._step_count = 0
    
    def _init_vqvae(self, vqvae_config: Optional[Dict] = None):
        """åˆå§‹åŒ–å†…ç½®VQVAE"""
        config = vqvae_config or {}
        
        self.vocab_size = config.get('vocab_size', 8192)
        self.z_channels = config.get('z_channels', 64)
        
        print(f"ğŸ¨ åˆå§‹åŒ–å†…ç½®VQVAE:")
        print(f"   è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        print(f"   æ½œåœ¨ç»´åº¦: {self.z_channels}")
        
        # ç¼–ç å™¨: 1Ã—64Ã—64 â†’ z_channelsÃ—4Ã—4 (16å€ä¸‹é‡‡æ ·)
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 32, 4, stride=2, padding=1),     # 64â†’32
            nn.ReLU(),
            nn.Conv2d(32, 64, 4, stride=2, padding=1),    # 32â†’16
            nn.ReLU(),
            nn.Conv2d(64, 128, 4, stride=2, padding=1),   # 16â†’8
            nn.ReLU(),
            nn.Conv2d(128, self.z_channels, 4, stride=2, padding=1),  # 8â†’4
        )
        
        # é‡åŒ–å±‚
        self.quantize = nn.Embedding(self.vocab_size, self.z_channels)
        
        # è§£ç å™¨: z_channelsÃ—4Ã—4 â†’ 1Ã—64Ã—64
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(self.z_channels, 128, 4, stride=2, padding=1),  # 4â†’8
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),   # 8â†’16
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),    # 16â†’32
            nn.ReLU(),
            nn.ConvTranspose2d(32, 1, 4, stride=2, padding=1),     # 32â†’64
            nn.Tanh()
        )
        
        print(f"   æ¶æ„: 1Ã—64Ã—64 â†’ {self.z_channels}Ã—4Ã—4 â†’ 1Ã—64Ã—64")
    
    def _init_full_var(self, var_config: Optional[Dict] = None):
        """åˆå§‹åŒ–å®Œæ•´VARæ¨¡å‹"""
        config = var_config or {}
        
        # VARæ ¸å¿ƒå‚æ•°
        self.embed_dim = config.get('embed_dim', 512)
        self.depth = config.get('depth', 12)
        self.num_heads = config.get('num_heads', 8)
        self.num_classes = config.get('num_classes', 10)
        
        print(f"ğŸ—ï¸ åˆå§‹åŒ–å®Œæ•´VARæ¨¡å‹:")
        print(f"   åµŒå…¥ç»´åº¦: {self.embed_dim}")
        print(f"   æ·±åº¦: {self.depth}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {self.num_heads}")
        print(f"   ç±»åˆ«æ•°: {self.num_classes}")
        
        # 1. Word embedding (å°†VQVAEçš„æ½œåœ¨è¡¨ç¤ºæ˜ å°„åˆ°VARç©ºé—´)
        self.word_embed = nn.Linear(self.z_channels, self.embed_dim)
        
        # 2. Class embedding
        init_std = math.sqrt(1 / self.embed_dim / 3)
        self.class_emb = nn.Embedding(self.num_classes + 1, self.embed_dim)
        nn.init.trunc_normal_(self.class_emb.weight.data, mean=0, std=init_std)
        
        # èµ·å§‹ä½ç½®ç¼–ç 
        self.pos_start = nn.Parameter(torch.empty(1, self.first_l, self.embed_dim))
        nn.init.trunc_normal_(self.pos_start.data, mean=0, std=init_std)
        
        # 3. ç»å¯¹ä½ç½®ç¼–ç  (æ¯ä¸ªå°ºåº¦ç‹¬ç«‹)
        pos_1LC = []
        for i, pn in enumerate(self.patch_nums):
            pe = torch.empty(1, pn * pn, self.embed_dim)
            nn.init.trunc_normal_(pe, mean=0, std=init_std)
            pos_1LC.append(pe)
        self.pos_1LC = nn.Parameter(torch.cat(pos_1LC, dim=1))
        
        # 4. å±‚çº§ç¼–ç  (æ¯ä¸ªtokençš„å±‚çº§ä¿¡æ¯)
        lvl_1L = []
        for i, pn in enumerate(self.patch_nums):
            lvl_1L.extend([i] * (pn ** 2))
        self.register_buffer('lvl_1L', torch.tensor(lvl_1L, dtype=torch.long))
        
        # å±‚çº§embedding
        self.lvl_embed = nn.Embedding(len(self.patch_nums), self.embed_dim)
        nn.init.trunc_normal_(self.lvl_embed.weight.data, mean=0, std=init_std)
        
        # 5. AdaLN-Zero transformer blocks
        self.blocks = nn.ModuleList([
            AdaLNSelfAttn(
                block_idx=i,
                last_drop_p=0.0 if i == 0 else 0.1,
                embed_dim=self.embed_dim,
                cond_dim=self.embed_dim,
                shared_aln=False,
                norm_layer=nn.LayerNorm,
                num_heads=self.num_heads,
                mlp_ratio=4.0,
                drop=0.1,
                attn_drop=0.1,
                drop_path=0.1,
                attn_l2_norm=False,
                flash_if_available=False,
                fused_if_available=True,
                enable_histology_injection=True,
                histology_dim=self.histology_feature_dim,
            ) for i in range(self.depth)
        ])
        
        # 6. Final normalization and head
        self.head_nm = AdaLNBeforeHead(self.embed_dim, self.embed_dim, norm_layer=nn.LayerNorm)
        self.head = nn.Linear(self.embed_dim, self.vocab_size)
        nn.init.trunc_normal_(self.head.weight.data, mean=0, std=init_std)
        nn.init.constant_(self.head.bias.data, 0)
        
        print(f"   transformerå—æ•°: {len(self.blocks)}")
        print(f"   è¾“å‡ºè¯æ±‡è¡¨: {self.vocab_size}")
    
    def _init_condition_processor(self):
        """åˆå§‹åŒ–æ¡ä»¶å¤„ç†å™¨"""
        print(f"ğŸ›ï¸ åˆå§‹åŒ–æ¡ä»¶å¤„ç†å™¨ (è¾“å…¥: {self.histology_feature_dim})")
        
        # ç®€å•çš„çº¿æ€§æ˜ å°„
        self.condition_processor = nn.Sequential(
            nn.Linear(self.histology_feature_dim, 512),
                    nn.ReLU(),
            nn.Linear(512, 512),
        )
        
        print(f"   æ¶æ„: {self.histology_feature_dim} â†’ 512 â†’ 512")

    def img_to_idxBl(self, x: torch.Tensor) -> List[torch.Tensor]:
        """å›¾åƒç¼–ç ä¸ºå¤šå°ºåº¦token indices"""
        B = x.shape[0]
        
        # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        z = self.encoder(x)  # [B, z_channels, 4, 4]
        B, C, H, W = z.shape
        
        # å±•å¹³å¹¶é‡åŒ–
        z_flat = z.view(B, C, H*W).permute(0, 2, 1)  # [B, H*W, C]
        
        # æ‰¾æœ€è¿‘çš„codebookå‘é‡
        distances = torch.cdist(z_flat, self.quantize.weight)  # [B, H*W, vocab_size]
        indices = torch.argmin(distances, dim=-1)  # [B, H*W]
        
        # æŒ‰å¤šå°ºåº¦ç»„ç»‡tokens
        tokens = []
        for pn in self.patch_nums:
            if pn <= H:  # ç¡®ä¿patch sizeä¸è¶…è¿‡ç‰¹å¾å›¾å°ºå¯¸
                # å¯¹äºæ¯ä¸ªå°ºåº¦ï¼Œä»4Ã—4ç‰¹å¾å›¾ä¸­é‡‡æ ·
                patch_tokens = []
                for i in range(pn):
                    for j in range(pn):
                        # å°†pnÃ—pnç½‘æ ¼æ˜ å°„åˆ°4Ã—4ç‰¹å¾å›¾
                        hi = min(int(i * H / pn), H-1)
                        wi = min(int(j * W / pn), W-1)
                        idx = hi * W + wi
                        patch_tokens.append(indices[:, idx])
                
                patch_tensor = torch.stack(patch_tokens, dim=1)  # [B, pn*pn]
                tokens.append(patch_tensor)
            else:
                # å¦‚æœpatch sizeå¤ªå¤§ï¼Œé‡å¤ä½¿ç”¨ç°æœ‰tokens
                repeat_times = (pn * pn + H*W - 1) // (H*W)  # å‘ä¸Šå–æ•´
                repeated = indices.repeat(1, repeat_times)[:, :pn*pn]
                tokens.append(repeated)
                
        return tokens
            
    def idxBl_to_img(self, tokens: List[torch.Tensor], same_shape: bool = True, last_one: bool = True) -> torch.Tensor:
        """å¤šå°ºåº¦token indicesè§£ç ä¸ºå›¾åƒ"""
        if last_one and len(tokens) > 0:
            # ä½¿ç”¨æœ€é«˜åˆ†è¾¨ç‡çš„tokens
            indices = tokens[-1]
        else:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæˆ–å¹³å‡
            indices = tokens[0] if tokens else torch.zeros(1, 1, dtype=torch.long)
                
        B = indices.shape[0]
                
        # é‡åŒ–åµŒå…¥
        quantized = self.quantize(indices)  # [B, L, z_channels]
        
        # é‡å¡‘ä¸º4Ã—4ç‰¹å¾å›¾
        if quantized.dim() == 3:
            L = quantized.shape[1]
            side = int(math.sqrt(L))
            if side * side != L:
                side = 4  # é»˜è®¤4Ã—4
                quantized = quantized[:, :side*side]
            
            z = quantized.permute(0, 2, 1).view(B, self.z_channels, side, side)
        else:
            z = quantized.view(B, self.z_channels, 1, 1)
        
        # å¦‚æœä¸æ˜¯4Ã—4ï¼Œæ’å€¼åˆ°4Ã—4
        if z.shape[-1] != 4:
            z = F.interpolate(z, size=(4, 4), mode='bilinear', align_corners=False)
        
        # è§£ç 
        return self.decoder(z)
    
    def get_logits(self, h: torch.Tensor, cond_BD: torch.Tensor) -> torch.Tensor:
        """è·å–è¾“å‡ºlogits"""
        return self.head(self.head_nm(h.float(), cond_BD).float()).float()
    
    def forward_training(
        self,
        gene_expression: torch.Tensor,
        histology_features: torch.Tensor,
        class_labels: Optional[torch.Tensor] = None,
        show_details: bool = False
    ) -> Dict[str, torch.Tensor]:
        """è®­ç»ƒå‰å‘ä¼ æ’­"""
        B, num_genes = gene_expression.shape
        device = gene_expression.device
        
        self._step_count += 1
        
        # æ˜¾ç¤ºè¯¦æƒ…æ§åˆ¶
        import os
        is_main_process = int(os.environ.get('LOCAL_RANK', 0)) == 0
        show_details = (self._verbose_logging and is_main_process and 
                       (self._step_count <= 3 or self._step_count % 1000 == 0))
        
        if show_details:
            print(f"ğŸ§¬ å®Œæ•´VARè®­ç»ƒ (æ­¥éª¤ {self._step_count}):")
        
        # 1. åŸºå›  â†’ ä¼ªå›¾åƒ
        pseudo_images = self.gene_adapter.genes_to_pseudo_image(gene_expression)
        if pseudo_images.shape[1] != 1:
            pseudo_images = pseudo_images.mean(dim=1, keepdim=True)
            
        # 2. å¤„ç†æ¡ä»¶
        condition_embeddings = self.condition_processor(histology_features)
        
        if class_labels is None:
            # åŠ¨æ€ç”Ÿæˆç±»åˆ«æ ‡ç­¾
            with torch.no_grad():
                n_classes = min(self.num_classes, condition_embeddings.shape[1])
                class_features = condition_embeddings[:, :n_classes]
                class_probs = torch.softmax(class_features, dim=-1)
                class_labels = torch.multinomial(class_probs, 1).squeeze(-1)
        
        # 3. VQVAEç¼–ç 
        ms_tokens = self.img_to_idxBl(pseudo_images)
        
        if show_details:
            print(f"   å¤šå°ºåº¦tokens: {[t.shape for t in ms_tokens]}")
        
        # 4. VARè®­ç»ƒ
        # å‡†å¤‡è¾“å…¥ï¼šé™¤äº†ç¬¬ä¸€å±‚çš„æ‰€æœ‰tokens
        all_tokens = torch.cat([tokens.flatten(1) for tokens in ms_tokens], dim=1)  # [B, L]
        x_BLCv_wo_first_l = all_tokens[:, self.first_l:]  # [B, L-first_l]
        
        # è·å–åµŒå…¥
        if x_BLCv_wo_first_l.numel() == 0:
            raise RuntimeError(
                f"Tokenåºåˆ—ä¸ºç©ºï¼Œè¿™ä¸åº”è¯¥å‘ç”Ÿã€‚"
                f"first_l: {self.first_l}, all_tokenså½¢çŠ¶: {all_tokens.shape if 'all_tokens' in locals() else 'N/A'}, "
                f"patch_nums: {self.patch_nums}"
            )
        
        # é‡åŒ–åµŒå…¥
        token_embeddings = self.quantize(x_BLCv_wo_first_l)  # [B, L-first_l, z_channels]
        # æ˜ å°„åˆ°VARç©ºé—´
        x = self.word_embed(token_embeddings)  # [B, L-first_l, embed_dim]
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        if x.shape[1] <= self.pos_1LC.shape[1] - self.first_l:
            x = x + self.pos_1LC[:, self.first_l:self.first_l + x.shape[1]]
        
        # ğŸ”§ ä¿®å¤å±‚çº§ç¼–ç ï¼šæ­£ç¡®è®¡ç®—ç´¢å¼•èŒƒå›´
        # lvl_1Lçš„å½¢çŠ¶æ˜¯[L]ï¼ŒåŒ…å«æ¯ä¸ªä½ç½®å¯¹åº”çš„å±‚çº§ID (0, 1, 2, ...)
        # æˆ‘ä»¬éœ€è¦å–å‡ºå¯¹åº”çš„å±‚çº§IDï¼Œç„¶åè·å–å¯¹åº”çš„ç¼–ç 
        if x.shape[1] > len(self.lvl_1L) - self.first_l:
            raise RuntimeError(
                f"Tokenåºåˆ—é•¿åº¦{x.shape[1]}è¶…å‡ºå±‚çº§ç¼–ç èŒƒå›´{len(self.lvl_1L) - self.first_l}ã€‚"
                f"æ€»é•¿åº¦: {len(self.lvl_1L)}, first_l: {self.first_l}, å¯ç”¨é•¿åº¦: {len(self.lvl_1L) - self.first_l}"
            )
        
        lvl_indices = self.lvl_1L[self.first_l:self.first_l + x.shape[1]]  # [L-first_l]
        lvl_pos = self.lvl_embed(lvl_indices)  # [L-first_l, embed_dim]
        x = x + lvl_pos.unsqueeze(0)  # [B, L-first_l, embed_dim]
        
        # Classæ¡ä»¶
        cond_BD = self.class_emb(class_labels)  # [B, embed_dim]
        
        # é€šè¿‡transformer blocks
        for block in self.blocks:
            x = block(x, cond_BD, attn_bias=None, histology_condition=histology_features)
        
        # è·å–logits
        logits = self.get_logits(x, cond_BD)  # [B, L-first_l, vocab_size]
        
        # è®¡ç®—VARæŸå¤±
        if x_BLCv_wo_first_l.numel() > 0:
            target_tokens = x_BLCv_wo_first_l
            var_loss = F.cross_entropy(
                logits.reshape(-1, logits.size(-1)), 
                target_tokens.reshape(-1),
                ignore_index=-1
            )
        else:
            var_loss = torch.tensor(0.0, device=device, requires_grad=True)
                
        # 5. ç”ŸæˆéªŒè¯
        with torch.no_grad():
            # ä½¿ç”¨å½“å‰æ¨¡å‹ç”Ÿæˆ
            generated_tokens = self.autoregressive_infer_cfg(
                B=B,
                label_B=class_labels,
                histology_condition=histology_features
            )
            generated_images = generated_tokens  # autoregressive_infer_cfgç›´æ¥è¿”å›å›¾åƒ
            
            # ç¡®ä¿æ ¼å¼æ­£ç¡®
            if generated_images.shape[-2:] != (self.image_size, self.image_size):
                generated_images = F.interpolate(
                    generated_images,
                    size=(self.image_size, self.image_size),
                    mode='bilinear',
                    align_corners=False
                )
                
            if generated_images.shape[1] != 1:
                generated_images = generated_images.mean(dim=1, keepdim=True)
                
            # ä¼ªå›¾åƒ â†’ åŸºå› è¡¨è¾¾
            predicted_genes = self.gene_adapter.pseudo_image_to_genes(generated_images)
                
            # é‡å»ºæŸå¤±
            recon_loss = F.mse_loss(predicted_genes, gene_expression)
                
        # 6. æ€»æŸå¤±
        if self.progressive_training:
            # æ¸è¿›å¼è®­ç»ƒæƒé‡
            progress = min(1.0, self.current_step / self.warmup_steps)
            recon_weight = self.min_recon_weight + progress * (self.max_recon_weight - self.min_recon_weight)
        else:
            recon_weight = self.max_recon_weight
        
        total_loss = var_loss + recon_weight * recon_loss
        
        if show_details:
            print(f"   VARæŸå¤±: {var_loss:.4f}")
            print(f"   é‡å»ºæŸå¤±: {recon_loss:.4f} (æƒé‡: {recon_weight:.2f})")
            print(f"   æ€»æŸå¤±: {total_loss:.4f}")
        
        return {
            'loss': total_loss,
            'var_loss': var_loss,
            'recon_loss': recon_loss,
            'predicted_expression': predicted_genes,
            'predictions': predicted_genes
        }

    @torch.no_grad()
    def autoregressive_infer_cfg(
        self, 
        B: int, 
        label_B: Optional[torch.Tensor] = None,
        g_seed: Optional[int] = None,
        cfg: float = 1.5,
        top_k: int = 50,
        top_p: float = 0.9,
        histology_condition: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """è‡ªå›å½’æ¨ç†ç”Ÿæˆ"""
        device = next(self.parameters()).device
        
        if label_B is None:
            label_B = torch.zeros(B, dtype=torch.long, device=device)
        
        if g_seed is not None:
            torch.manual_seed(g_seed)
        
        # åˆå§‹åŒ–åºåˆ—
        seq_len = self.L  # æ€»tokené•¿åº¦
        input_ids = torch.zeros(B, seq_len, dtype=torch.long, device=device)
        
        # ç”Ÿæˆç¬¬ä¸€å±‚tokens (éšæœºåˆå§‹åŒ–)
        for i in range(self.first_l):
            input_ids[:, i] = torch.randint(0, self.vocab_size, (B,), device=device)
        
        # è‡ªå›å½’ç”Ÿæˆå‰©ä½™tokens
        for cur_L in range(self.first_l, seq_len):
            # è·å–å½“å‰åºåˆ—
            x_BLCv_wo_first_l = input_ids[:, self.first_l:cur_L]
            
            if x_BLCv_wo_first_l.shape[1] == 0:
                # å¦‚æœåºåˆ—ä¸ºç©ºï¼Œè·³è¿‡
                continue
            
            # åµŒå…¥
            token_embeddings = self.quantize(x_BLCv_wo_first_l)
            x = self.word_embed(token_embeddings)
            
            # ä½ç½®ç¼–ç 
            if x.shape[1] <= self.pos_1LC.shape[1] - self.first_l:
                x = x + self.pos_1LC[:, self.first_l:self.first_l + x.shape[1]]
            
            # å±‚çº§ç¼–ç 
            if x.shape[1] <= len(self.lvl_1L) - self.first_l:
                lvl_indices = self.lvl_1L[self.first_l:self.first_l + x.shape[1]]
                lvl_pos = self.lvl_embed(lvl_indices)
                x = x + lvl_pos.unsqueeze(0)
            
            # Classæ¡ä»¶
            cond_BD = self.class_emb(label_B)
            
            # é€šè¿‡transformer
            for block in self.blocks:
                x = block(x, cond_BD, attn_bias=None, histology_condition=histology_condition)
            
            # é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
            logits = self.get_logits(x, cond_BD)  # [B, cur_L-first_l, vocab_size]
            
            if logits.shape[1] > 0:
                next_token_logits = logits[:, -1, :]  # [B, vocab_size]
                
                # Top-kå’Œtop-pé‡‡æ ·
                if top_k > 0:
                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
                    next_token_logits[indices_to_remove] = float('-inf')
                
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0
                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                    next_token_logits[indices_to_remove] = float('-inf')
                
                # é‡‡æ ·
                probs = F.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                input_ids[:, cur_L] = next_token.squeeze(-1)
        
        # è§£ç ä¸ºå›¾åƒ
        # å°†æ‰€æœ‰tokensè½¬æ¢ä¸ºå¤šå°ºåº¦æ ¼å¼
        ms_tokens = []
        start_idx = 0
        for pn in self.patch_nums:
            length = pn ** 2
            tokens = input_ids[:, start_idx:start_idx + length]
            ms_tokens.append(tokens)
            start_idx += length
        
        # è§£ç 
        generated_images = self.idxBl_to_img(ms_tokens, last_one=True)
        
        return generated_images
    
    def forward_inference(
        self,
        histology_features: torch.Tensor,
        class_labels: Optional[torch.Tensor] = None,
        cfg_scale: float = 1.5,
        top_k: int = 50,
        top_p: float = 0.9,
        temperature: float = 1.0,
        num_samples: int = 1
    ) -> Dict[str, torch.Tensor]:
        """æ¨ç†æ¨¡å¼å‰å‘ä¼ æ’­"""
        B = histology_features.shape[0]
        device = histology_features.device
        
        # ç”Ÿæˆå›¾åƒ
        generated_images = self.autoregressive_infer_cfg(
            B=B,
            label_B=class_labels,
                cfg=cfg_scale,
                top_k=top_k,
                top_p=top_p,
            histology_condition=histology_features
            )
        
        # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡®
        if generated_images.shape[-2:] != (self.image_size, self.image_size):
            generated_images = F.interpolate(
                generated_images,
                size=(self.image_size, self.image_size),
                mode='bilinear',
                align_corners=False
            )
        
        if generated_images.shape[1] != 1:
                generated_images = generated_images.mean(dim=1, keepdim=True)
        
        # è½¬æ¢ä¸ºåŸºå› è¡¨è¾¾
        predicted_genes = self.gene_adapter.pseudo_image_to_genes(generated_images)
        
        return {
            'predicted_expression': predicted_genes,
            'predictions': predicted_genes,
            'generated_images': generated_images
        }
    
    def forward(self, **inputs) -> Dict[str, torch.Tensor]:
        """ä¸»å‰å‘ä¼ æ’­å…¥å£"""
        mode = inputs.get('mode', 'training')
        
        if mode == 'training' or 'gene_expression' in inputs:
            # è®­ç»ƒæ¨¡å¼
            return self.forward_training(
                gene_expression=inputs['gene_expression'],
                histology_features=inputs['histology_features'],
                class_labels=inputs.get('class_labels'),
                show_details=inputs.get('show_details', False)
            )
        else:
            # æ¨ç†æ¨¡å¼
            return self.forward_inference(
                histology_features=inputs['histology_features'],
                class_labels=inputs.get('class_labels'),
                cfg_scale=inputs.get('cfg_scale', 1.5),
                top_k=inputs.get('top_k', 50),
                top_p=inputs.get('top_p', 0.9),
                temperature=inputs.get('temperature', 1.0),
                num_samples=inputs.get('num_samples', 1)
            ) 