"""
VARåŸºå› åŒ…è£…å™¨ - å®Œæ•´VARå®ç°ç‰ˆæœ¬

åŸºäºçœŸæ­£çš„VARæ¶æ„é‡å†™ï¼ŒåŒ…å«ï¼š
- AdaLNSelfAttn: æ¡ä»¶è‡ªé€‚åº”LayerNorm
- å¤šå°ºåº¦è‡ªå›å½’ç”Ÿæˆ
- æ­£ç¡®çš„ä½ç½®ç¼–ç å’Œå±‚çº§ç¼–ç 
- Causal attention mask
- å®Œæ•´çš„autoregressiveæ¨ç†

æ— å¤–éƒ¨ä¾èµ–ï¼Œå®Œå…¨å†…ç½®å®ç°ã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import sys
import os
from typing import Dict, List, Optional, Tuple, Any, Union
import numpy as np
import math
from functools import partial

# å¯¼å…¥æœ¬åœ°åŸºå› é€‚é…å™¨å’ŒVARåŸºç¡€ç»„ä»¶
from .gene_pseudo_image_adapter import GenePseudoImageAdapter
from .var_basic_components import *

class VARGeneWrapper(nn.Module):
    """
    VARåŸºå› åŒ…è£…å™¨ - å®Œæ•´VARå®ç°ç‰ˆæœ¬
    
    åŸºäºçœŸæ­£çš„VARæ¶æ„è®¾è®¡ï¼š
    - 196ä¸ªåŸºå›  â†’ 14Ã—14Ã—1 ä¼ªå›¾åƒ
    - å¤šå°ºåº¦patch_nums: (1,2,4) = 21 tokens
    - AdaLNæ¡ä»¶æ³¨å…¥æœºåˆ¶
    - å®Œæ•´çš„è‡ªå›å½’ç”Ÿæˆ
    """
    
    def __init__(
        self,
        histology_feature_dim: int,  # ğŸ”§ å¿…éœ€å‚æ•°æ”¾åœ¨å‰é¢
        num_genes: int = 196,
        image_size: int = 64,
        patch_nums: Tuple[int, ...] = (1, 14),  # ğŸ”§ æ–¹æ¡ˆCï¼š1ä¸ªå…¨å±€token + 196ä¸ªåŸºå› tokens
        var_config: Optional[Dict] = None,
        vqvae_config: Optional[Dict] = None,
        adapter_config: Optional[Dict] = None,
        progressive_training: bool = True,
        warmup_steps: int = 1000,
        min_recon_weight: float = 0.5,
        max_recon_weight: float = 5.0
    ):
        super().__init__()
        
        # ğŸ”§ ä¸¥æ ¼å‚æ•°éªŒè¯ï¼Œä¸å…è®¸Noneæˆ–æ— æ•ˆå€¼
        if histology_feature_dim is None or histology_feature_dim <= 0:
            raise ValueError(f"histology_feature_dimå¿…é¡»æ˜¯æ­£æ•´æ•°ï¼Œå¾—åˆ°: {histology_feature_dim}")
        
        if num_genes <= 0:
            raise ValueError(f"num_geneså¿…é¡»æ˜¯æ­£æ•´æ•°ï¼Œå¾—åˆ°: {num_genes}")
            
        if image_size <= 0:
            raise ValueError(f"image_sizeå¿…é¡»æ˜¯æ­£æ•´æ•°ï¼Œå¾—åˆ°: {image_size}")
            
        if not patch_nums or any(p <= 0 for p in patch_nums):
            raise ValueError(f"patch_numså¿…é¡»æ˜¯æ­£æ•´æ•°åºåˆ—ï¼Œå¾—åˆ°: {patch_nums}")
        
        # ğŸ”§ éªŒè¯patch_numsä¸åŸºå› æ•°é‡çš„åŒ¹é…
        expected_total = sum(pn ** 2 for pn in patch_nums)
        if expected_total != num_genes + 1:  # +1 for global token
            print(f"âš ï¸ è­¦å‘Š: patch_numsæ€»tokenæ•°({expected_total}) != åŸºå› æ•°+1({num_genes + 1})")
            print(f"   å½“å‰patch_nums: {patch_nums}")
            print(f"   å„å±‚tokenæ•°: {[pn**2 for pn in patch_nums]}")
            print(f"   å»ºè®®çš„patch_numsè®¾è®¡ï¼šç¬¬1å±‚=1(å…¨å±€), ç¬¬2å±‚=196(åŸºå› )")
        
        # ä¿å­˜é…ç½®
        self.progressive_training = progressive_training
        self.warmup_steps = warmup_steps
        self.min_recon_weight = min_recon_weight
        self.max_recon_weight = max_recon_weight
        self.current_step = 0
        
        # åŸºç¡€é…ç½®
        self.num_genes = num_genes
        self.image_size = image_size
        self.histology_feature_dim = histology_feature_dim
        self.patch_nums = patch_nums
        
        # éªŒè¯åŸºå› æ•°é‡
        if num_genes == 196:
            self.use_upsampling = True
            self.intermediate_size = 14
        else:
            self.use_upsampling = False
            self.intermediate_size = int(math.sqrt(num_genes))
            if self.intermediate_size ** 2 != num_genes:
                raise ValueError(f"åŸºå› æ•°é‡{num_genes}ä¸æ˜¯å®Œå…¨å¹³æ–¹æ•°")
        
        print(f"ğŸ§¬ VARåŸºå› åŒ…è£…å™¨ - æ–¹æ¡ˆCé‡æ„ç‰ˆæœ¬")
        print(f"   åŸºå› æ•°é‡: {num_genes} â†’ {image_size}Ã—{image_size}")
        print(f"   å¤šå°ºåº¦patch_nums: {patch_nums}")
        print(f"   è®¾è®¡ç†å¿µ: ç¬¬1å±‚({patch_nums[0]}Â²)=å…¨å±€ç‰¹å¾, ç¬¬2å±‚({patch_nums[1]}Â²)=åŸºå› çº§ç‰¹å¾")
        
        # è®¡ç®—å¤šå°ºåº¦å‚æ•°
        self.L = sum(pn ** 2 for pn in self.patch_nums)  # æ€»tokenæ•° = 1 + 196 = 197
        self.first_l = self.patch_nums[0] ** 2  # ç¬¬ä¸€å±‚tokenæ•° = 1
        self.begin_ends = []
        cur = 0
        for i, pn in enumerate(self.patch_nums):
            self.begin_ends.append((cur, cur + pn ** 2))
            cur += pn ** 2
        
        print(f"   æ€»tokenæ•°: {self.L}")
        print(f"   ç¬¬ä¸€å±‚(å…¨å±€): {self.first_l} tokens")
        print(f"   å„å±‚èŒƒå›´: {self.begin_ends}")
        
        # ğŸ”§ æ–°å¢ï¼šéªŒè¯å¤šå°ºåº¦è®¾è®¡çš„åˆç†æ€§
        if len(self.patch_nums) == 2 and self.patch_nums[0] == 1 and self.patch_nums[1] == 14:
            print(f"âœ… æ–¹æ¡ˆCé…ç½®éªŒè¯é€šè¿‡ï¼š")
            print(f"   - å…¨å±€å±‚: 1ä¸ªtoken (æ•´ä½“åŸºå› è¡¨è¾¾æ¨¡å¼)")
            print(f"   - åŸºå› å±‚: 196ä¸ªtokens (æ¯ä¸ªåŸºå› çš„è¯¦ç»†è¡¨è¾¾)")
            print(f"   - æ€»è®¡: {self.L} tokens")
        else:
            print(f"âš ï¸ éæ ‡å‡†æ–¹æ¡ˆCé…ç½®ï¼Œè¯·ç¡®è®¤è®¾è®¡æ„å›¾")
        
        # åˆå§‹åŒ–åŸºå› é€‚é…å™¨
        self.gene_adapter = GenePseudoImageAdapter(
            num_genes=num_genes,
            intermediate_size=self.intermediate_size,
            target_image_size=image_size,
            normalize_method='none',
            eps=1e-6
        )

        # åˆå§‹åŒ–VQVAE
        self._init_vqvae(vqvae_config)
        
        # åˆå§‹åŒ–VAR
        self._init_full_var(var_config)
        
        # åˆå§‹åŒ–æ¡ä»¶å¤„ç†å™¨
        self._init_condition_processor()
        
        # è¾“å‡ºç»Ÿè®¡
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"ğŸ“Š æ€»å‚æ•°: {trainable_params:,}")
        print(f"âœ… VARåŸºå› åŒ…è£…å™¨åˆå§‹åŒ–å®Œæˆ")
        
        self._verbose_logging = True
        self._step_count = 0
    
    def _init_vqvae(self, vqvae_config: Optional[Dict] = None):
        """åˆå§‹åŒ–å†…ç½®VQVAE"""
        config = vqvae_config or {}
        
        self.vocab_size = config.get('vocab_size', 8192)
        self.z_channels = config.get('z_channels', 64)
        
        print(f"ğŸ¨ åˆå§‹åŒ–å†…ç½®VQVAE:")
        print(f"   è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        print(f"   æ½œåœ¨ç»´åº¦: {self.z_channels}")
        
        # ç¼–ç å™¨: 1Ã—64Ã—64 â†’ z_channelsÃ—4Ã—4 (16å€ä¸‹é‡‡æ ·)
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 32, 4, stride=2, padding=1),     # 64â†’32
            nn.ReLU(),
            nn.Conv2d(32, 64, 4, stride=2, padding=1),    # 32â†’16
            nn.ReLU(),
            nn.Conv2d(64, 128, 4, stride=2, padding=1),   # 16â†’8
            nn.ReLU(),
            nn.Conv2d(128, self.z_channels, 4, stride=2, padding=1),  # 8â†’4
        )
        
        # é‡åŒ–å±‚
        self.quantize = nn.Embedding(self.vocab_size, self.z_channels)
        
        # è§£ç å™¨: z_channelsÃ—4Ã—4 â†’ 1Ã—64Ã—64
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(self.z_channels, 128, 4, stride=2, padding=1),  # 4â†’8
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),   # 8â†’16
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),    # 16â†’32
            nn.ReLU(),
            nn.ConvTranspose2d(32, 1, 4, stride=2, padding=1),     # 32â†’64
            nn.Tanh()
        )
        
        print(f"   æ¶æ„: 1Ã—64Ã—64 â†’ {self.z_channels}Ã—4Ã—4 â†’ 1Ã—64Ã—64")
    
    def _init_full_var(self, var_config: Optional[Dict] = None):
        """åˆå§‹åŒ–å®Œæ•´VARæ¨¡å‹"""
        config = var_config or {}
        
        # VARæ ¸å¿ƒå‚æ•°
        self.embed_dim = config.get('embed_dim', 512)
        self.depth = config.get('depth', 12)
        self.num_heads = config.get('num_heads', 8)
        self.num_classes = config.get('num_classes', 10)
        
        print(f"ğŸ—ï¸ åˆå§‹åŒ–å®Œæ•´VARæ¨¡å‹:")
        print(f"   åµŒå…¥ç»´åº¦: {self.embed_dim}")
        print(f"   æ·±åº¦: {self.depth}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {self.num_heads}")
        print(f"   ç±»åˆ«æ•°: {self.num_classes}")
        
        # Progressive trainingå‚æ•°
        self.prog_si = -1  # -1è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨å°ºåº¦ï¼Œ>=0è¡¨ç¤ºå½“å‰è®­ç»ƒçš„å°ºåº¦ç´¢å¼•
        self.num_stages_minus_1 = len(self.patch_nums) - 1
        
        # 1. Word embedding (å°†VQVAEçš„æ½œåœ¨è¡¨ç¤ºæ˜ å°„åˆ°VARç©ºé—´)
        self.word_embed = nn.Linear(self.z_channels, self.embed_dim)
        
        # 2. Class embedding
        init_std = math.sqrt(1 / self.embed_dim / 3)
        self.class_emb = nn.Embedding(self.num_classes + 1, self.embed_dim)
        nn.init.trunc_normal_(self.class_emb.weight.data, mean=0, std=init_std)
        
        # èµ·å§‹ä½ç½®ç¼–ç 
        self.pos_start = nn.Parameter(torch.empty(1, self.first_l, self.embed_dim))
        nn.init.trunc_normal_(self.pos_start.data, mean=0, std=init_std)
        
        # 3. ç»å¯¹ä½ç½®ç¼–ç  (æ¯ä¸ªå°ºåº¦ç‹¬ç«‹)
        pos_1LC = []
        for i, pn in enumerate(self.patch_nums):
            pe = torch.empty(1, pn*pn, self.embed_dim)
            nn.init.trunc_normal_(pe, mean=0, std=init_std)
            pos_1LC.append(pe)
        pos_1LC = torch.cat(pos_1LC, dim=1)  # 1, L, C
        assert tuple(pos_1LC.shape) == (1, self.L, self.embed_dim)
        self.pos_1LC = nn.Parameter(pos_1LC)
        
        # 4. Level embedding (å±‚çº§ç¼–ç ï¼Œç”¨äºåŒºåˆ†ä¸åŒå°ºåº¦çš„token)
        self.lvl_embed = nn.Embedding(len(self.patch_nums), self.embed_dim)
        nn.init.trunc_normal_(self.lvl_embed.weight.data, mean=0, std=init_std)
        
        # 5. æ„å»ºCausal Attention Mask (VARçš„æ ¸å¿ƒç»„ä»¶)
        print(f"ğŸ¯ æ„å»ºCausal Attention Mask:")
        
        # åˆ›å»ºlevelæ ‡è¯†ï¼šæ¯ä¸ªtokenå±äºå“ªä¸ªå°ºåº¦çº§åˆ«
        d = torch.cat([torch.full((pn*pn,), i) for i, pn in enumerate(self.patch_nums)]).view(1, self.L, 1)
        dT = d.transpose(1, 2)  # dT: 1,1,L
        lvl_1L = dT[:, 0].contiguous()  # 1,L - æ¯ä¸ªä½ç½®çš„level
        self.register_buffer('lvl_1L', lvl_1L)
        
        # æ„å»ºcausal mask: åªæœ‰level >= å½“å‰levelçš„tokenå¯ä»¥è¢«çœ‹åˆ°
        attn_bias_for_masking = torch.where(d >= dT, 0., -torch.inf).reshape(1, 1, self.L, self.L)
        self.register_buffer('attn_bias_for_masking', attn_bias_for_masking.contiguous())
        
        print(f"   Levelåˆ†å¸ƒ: {[torch.sum(lvl_1L == i).item() for i in range(len(self.patch_nums))]}")
        print(f"   Maskå½¢çŠ¶: {attn_bias_for_masking.shape}")
        print(f"   å¯è§tokenæ•°é‡: {(attn_bias_for_masking[0, 0] != -torch.inf).sum(dim=-1)}")
        
        # 6. Transformer Blocks
        self.blocks = nn.ModuleList()
        for block_idx in range(self.depth):
            block = AdaLNSelfAttn(
                block_idx=block_idx,
                last_drop_p=0 if block_idx == 0 else 0.1,
                embed_dim=self.embed_dim,
                cond_dim=self.embed_dim,
                shared_aln=False,
                norm_layer=partial(nn.LayerNorm, eps=1e-6),
                num_heads=self.num_heads,
                mlp_ratio=4.0,
                drop=0.0,
                attn_drop=0.0,
                drop_path=0.1 * block_idx / (self.depth - 1) if self.depth > 1 else 0.0,
                attn_l2_norm=False,
                flash_if_available=True,
                fused_if_available=True,
                enable_histology_injection=True,
                histology_dim=self.histology_feature_dim
            )
            self.blocks.append(block)
            print(f"   âœ… Block {block_idx}: å¯ç”¨ç»„ç»‡å­¦æ¡ä»¶æ³¨å…¥ (histology_dim={self.histology_feature_dim} â†’ embed_dim={self.embed_dim})")
        
        print(f"   transformerå—æ•°: {len(self.blocks)}")
        
        # 7. Output head
        self.head_nm = AdaLNBeforeHead(self.embed_dim, self.embed_dim, norm_layer=partial(nn.LayerNorm, eps=1e-6))
        self.head = nn.Linear(self.embed_dim, self.vocab_size)
        
        print(f"   è¾“å‡ºè¯æ±‡è¡¨: {self.vocab_size}")
        
        # 8. éšæœºæ•°ç”Ÿæˆå™¨ (ç”¨äºæ¨ç†)
        self.rng = torch.Generator()
        
        print(f"âœ… VARæ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ŒåŒ…å«å®Œæ•´çš„Causal Attention Mask")
    
    def _init_condition_processor(self):
        """åˆå§‹åŒ–æ¡ä»¶å¤„ç†å™¨"""
        print(f"ğŸ›ï¸ åˆå§‹åŒ–æ¡ä»¶å¤„ç†å™¨ (è¾“å…¥: {self.histology_feature_dim})")
        
        # ç®€å•çš„çº¿æ€§æ˜ å°„
        self.condition_processor = nn.Sequential(
            nn.Linear(self.histology_feature_dim, 512),
                    nn.ReLU(),
            nn.Linear(512, 512),
        )
        
        print(f"   æ¶æ„: {self.histology_feature_dim} â†’ 512 â†’ 512")

    def img_to_idxBl(self, x: torch.Tensor) -> List[torch.Tensor]:
        """
        å°†å›¾åƒç¼–ç ä¸ºå¤šå°ºåº¦tokenåˆ—è¡¨
        
        Args:
            x: è¾“å…¥å›¾åƒæˆ–ç‰¹å¾ [B, C, H, W] æˆ– [B, L, C]
            
        Returns:
            tokens: å¤šå°ºåº¦tokenåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º [B, pn*pn]
        """
        B = x.shape[0]
        
        if x.dim() == 3:
            # å¦‚æœæ˜¯[B, L, C]ï¼Œå…ˆé‡å¡‘ä¸ºå›¾åƒæ ¼å¼
            L, C = x.shape[1], x.shape[2]
            if L == 16 and C == self.z_channels:
                # å‡è®¾æ˜¯4x4çš„ç‰¹å¾å›¾
                x = x.transpose(1, 2).reshape(B, C, 4, 4)
            else:
                raise ValueError(f"æ— æ³•å¤„ç†ç»´åº¦ {x.shape}")
        
        if x.dim() != 4:
            raise ValueError(f"æœŸæœ›4Dè¾“å…¥ [B, C, H, W]ï¼Œå¾—åˆ° {x.shape}")
        
        # ç¼–ç å¾—åˆ°æ½œåœ¨è¡¨ç¤º
        if x.shape[1] == 1:  # è¾“å…¥æ˜¯å›¾åƒ
            z = self.encoder(x)  # [B, z_channels, 4, 4]
        else:  # è¾“å…¥å·²ç»æ˜¯æ½œåœ¨è¡¨ç¤º
            z = x
            
        # æ‰å¹³åŒ–å¹¶é‡åŒ–
        z_flat = z.flatten(2).transpose(1, 2)  # [B, 16, z_channels]
        distances = torch.cdist(z_flat, self.quantize.weight)
        indices = torch.argmin(distances, dim=-1)  # [B, 16]
        
        # å¤šå°ºåº¦åˆ†å‰²
        tokens_list = []
        flat_indices = indices.flatten(1)  # [B, 16]
        
        start_idx = 0
        for pn in self.patch_nums:
            token_count = pn * pn
            if start_idx + token_count <= flat_indices.shape[1]:
                tokens = flat_indices[:, start_idx:start_idx + token_count]
            else:
                # å¦‚æœä¸å¤Ÿï¼Œé‡å¤æœ€åçš„tokens
                available = flat_indices.shape[1] - start_idx
                if available > 0:
                    tokens = flat_indices[:, start_idx:]
                    # é‡å¤å¡«å……åˆ°æ‰€éœ€é•¿åº¦
                    repeats = (token_count + available - 1) // available
                    tokens = tokens.repeat(1, repeats)[:, :token_count]
                else:
                    # å®Œå…¨æ²¡æœ‰å¯ç”¨çš„ï¼Œä½¿ç”¨0å¡«å……
                    tokens = torch.zeros(B, token_count, dtype=torch.long, device=x.device)
            
            tokens_list.append(tokens)
            start_idx += token_count
            
        return tokens_list
    
    def img_to_idxBl_multiscale(self, z_q: torch.Tensor, indices: torch.Tensor) -> List[torch.Tensor]:
        """
        åŸºäºé‡åŒ–ç‰¹å¾å’Œç´¢å¼•ç”Ÿæˆå¤šå°ºåº¦tokenåˆ—è¡¨ - æ–¹æ¡ˆCå®ç°
        
        æ–¹æ¡ˆCè®¾è®¡ï¼š
        - ç¬¬1å±‚: 1ä¸ªå…¨å±€token (æ•´ä½“åŸºå› è¡¨è¾¾æ¨¡å¼çš„ä»£è¡¨)
        - ç¬¬2å±‚: 196ä¸ªåŸºå› tokens (æ¯ä¸ªåŸºå› çš„è¯¦ç»†è¡¨è¾¾)
        
        Args:
            z_q: é‡åŒ–åçš„ç‰¹å¾ [B, 16, z_channels] (æ¥è‡ª4Ã—4=16çš„spatial tokens)
            indices: é‡åŒ–ç´¢å¼• [B, 16] (4Ã—4ç©ºé—´ä½ç½®çš„é‡åŒ–indices)
            
        Returns:
            tokens_list: [
                [B, 1] - ç¬¬1å±‚å…¨å±€token
                [B, 196] - ç¬¬2å±‚åŸºå› tokens  
            ]
        """
        B = indices.shape[0]
        tokens_list = []
        
        # ğŸ”§ æ–¹æ¡ˆCçš„å…³é”®æ”¹è¿›ï¼šåˆç†çš„å…¨å±€-å±€éƒ¨åˆ†å±‚
        
        # ç¬¬1å±‚ï¼šå…¨å±€token (ä½¿ç”¨æ‰€æœ‰spatial tokensçš„å¹³å‡æˆ–ä»£è¡¨æ€§token)
        if self.patch_nums[0] == 1:
            # ä½¿ç”¨ä¸­å¿ƒä½ç½®çš„tokenä½œä¸ºå…¨å±€ä»£è¡¨ï¼Œæˆ–è€…ä½¿ç”¨å¹³å‡
            if indices.shape[1] >= 4:
                # å¯¹äº4Ã—4çš„indicesï¼Œå–ä¸­å¿ƒ4ä¸ªä½ç½®çš„å¹³å‡ä½œä¸ºå…¨å±€token
                center_indices = indices[:, [5, 6, 9, 10]]  # 4Ã—4ä¸­å¿ƒçš„4ä¸ªä½ç½®
                global_token = torch.mode(center_indices, dim=1)[0].unsqueeze(1)  # [B, 1]
            else:
                # å¦‚æœspatial tokensä¸å¤Ÿï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª
                global_token = indices[:, 0:1]  # [B, 1]
            tokens_list.append(global_token)
        else:
            raise ValueError(f"æ–¹æ¡ˆCç¬¬1å±‚åº”è¯¥æ˜¯1ä¸ªtokenï¼Œå¾—åˆ°: {self.patch_nums[0]}")
            
        # ç¬¬2å±‚ï¼šåŸºå› çº§tokens (éœ€è¦æ‰©å±•16ä¸ªspatial tokensåˆ°196ä¸ªåŸºå› tokens)
        if self.patch_nums[1] == 14:  # 14Â² = 196
            # ğŸ”§ å…³é”®æ”¹è¿›ï¼šå°†16ä¸ªspatial tokensåˆç†æ‰©å±•åˆ°196ä¸ªåŸºå› tokens
            
            # æ–¹æ³•1ï¼šé‡å¤å’Œæ’å€¼æ‰©å±•
            flat_indices = indices.flatten(1)  # [B, 16]
            
            # è®¡ç®—éœ€è¦çš„æ‰©å±•å€æ•°
            target_count = 196
            available_count = flat_indices.shape[1]  # 16
            
            # ä½¿ç”¨é‡å¤+éšæœºæ‰°åŠ¨çš„æ–¹å¼æ‰©å±•åˆ°196ä¸ª
            expansion_factor = target_count // available_count  # 196 // 16 = 12
            remainder = target_count % available_count  # 196 % 16 = 4
            
            expanded_tokens = []
            
            # æ¯ä¸ªspatial tokené‡å¤expansion_factoræ¬¡
            for i in range(available_count):
                token_val = flat_indices[:, i:i+1]  # [B, 1]
                repeated = token_val.repeat(1, expansion_factor)  # [B, 12]
                expanded_tokens.append(repeated)
            
            # å¤„ç†ä½™æ•°ï¼šé¢å¤–é‡å¤å‰remainderä¸ªtokens
            for i in range(remainder):
                token_val = flat_indices[:, i:i+1]  # [B, 1]
                expanded_tokens.append(token_val)
            
            # è¿æ¥æ‰€æœ‰æ‰©å±•çš„tokens
            gene_tokens = torch.cat(expanded_tokens, dim=1)  # [B, 196]
            
            # ğŸ”§ æ·»åŠ è½»å¾®çš„éšæœºæ‰°åŠ¨ï¼Œé¿å…å®Œå…¨é‡å¤
            if self.training:
                # è®­ç»ƒæ—¶æ·»åŠ è½»å¾®æ‰°åŠ¨ (Â±1çš„éšæœºå˜åŒ–)
                noise = torch.randint_like(gene_tokens, low=-1, high=2) 
                gene_tokens = torch.clamp(gene_tokens + noise, min=0, max=self.vocab_size-1)
            
            tokens_list.append(gene_tokens)
        else:
            raise ValueError(f"æ–¹æ¡ˆCç¬¬2å±‚åº”è¯¥æ˜¯196ä¸ªtokensï¼Œå¾—åˆ°: {self.patch_nums[1]**2}")
        
        # éªŒè¯è¾“å‡º
        assert len(tokens_list) == 2, f"æ–¹æ¡ˆCåº”è¯¥è¾“å‡º2å±‚ï¼Œå¾—åˆ°: {len(tokens_list)}"
        assert tokens_list[0].shape[1] == 1, f"ç¬¬1å±‚åº”è¯¥æ˜¯1ä¸ªtokenï¼Œå¾—åˆ°: {tokens_list[0].shape[1]}"
        assert tokens_list[1].shape[1] == 196, f"ç¬¬2å±‚åº”è¯¥æ˜¯196ä¸ªtokensï¼Œå¾—åˆ°: {tokens_list[1].shape[1]}"
        
        return tokens_list
            
    def idxBl_to_img(self, tokens: List[torch.Tensor], same_shape: bool = True, last_one: bool = True) -> torch.Tensor:
        """å¤šå°ºåº¦token indicesè§£ç ä¸ºå›¾åƒ"""
        if last_one and len(tokens) > 0:
            # ä½¿ç”¨æœ€é«˜åˆ†è¾¨ç‡çš„tokens
            indices = tokens[-1]
        else:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæˆ–å¹³å‡
            indices = tokens[0] if tokens else torch.zeros(1, 1, dtype=torch.long)
                
        B = indices.shape[0]
                
        # é‡åŒ–åµŒå…¥
        quantized = self.quantize(indices)  # [B, L, z_channels]
        
        # é‡å¡‘ä¸º4Ã—4ç‰¹å¾å›¾
        if quantized.dim() == 3:
            L = quantized.shape[1]
            side = int(math.sqrt(L))
            if side * side != L:
                side = 4  # é»˜è®¤4Ã—4
                quantized = quantized[:, :side*side]
            
            z = quantized.permute(0, 2, 1).view(B, self.z_channels, side, side)
        else:
            z = quantized.view(B, self.z_channels, 1, 1)
        
        # å¦‚æœä¸æ˜¯4Ã—4ï¼Œæ’å€¼åˆ°4Ã—4
        if z.shape[-1] != 4:
            z = F.interpolate(z, size=(4, 4), mode='bilinear', align_corners=False)
        
        # è§£ç 
        return self.decoder(z)
    
    def get_logits(self, h: torch.Tensor, cond_BD: torch.Tensor) -> torch.Tensor:
        """è·å–è¾“å‡ºlogits"""
        return self.head(self.head_nm(h.float(), cond_BD).float()).float()
    
    def get_recon_weight(self) -> float:
        """
        è·å–å½“å‰çš„é‡å»ºæŸå¤±æƒé‡
        
        Returns:
            å½“å‰çš„é‡å»ºæŸå¤±æƒé‡
        """
        if self.progressive_training:
            # æ¸è¿›å¼è®­ç»ƒï¼šæƒé‡é€æ¸å¢åŠ 
            progress = min(1.0, self.current_step / self.warmup_steps)
            weight = self.min_recon_weight + progress * (self.max_recon_weight - self.min_recon_weight)
        else:
            # å›ºå®šæƒé‡
            weight = self.max_recon_weight
            
        return weight
    
    def get_next_autoregressive_input(self, si: int, total_stages: int, f_hat: torch.Tensor, h_BChw: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è·å–ä¸‹ä¸€ä¸ªè‡ªå›å½’è¾“å…¥ - VARæ ¸å¿ƒæ–¹æ³•
        
        Args:
            si: å½“å‰é˜¶æ®µç´¢å¼•
            total_stages: æ€»é˜¶æ®µæ•°
            f_hat: å½“å‰é‡å»ºçš„ç‰¹å¾å›¾ [B, C, H, W]
            h_BChw: å½“å‰é˜¶æ®µçš„ç‰¹å¾ [B, C, h, w]
            
        Returns:
            updated_f_hat: æ›´æ–°åçš„ç‰¹å¾å›¾
            next_input: ä¸‹ä¸€é˜¶æ®µçš„è¾“å…¥ (å¦‚æœä¸æ˜¯æœ€åé˜¶æ®µ)
        """
        B, Cvae = h_BChw.shape[:2]
        pn = self.patch_nums[si]
        
        # å°†å½“å‰é˜¶æ®µçš„ç‰¹å¾æ’å…¥åˆ°æ­£ç¡®ä½ç½®
        if si == 0:
            # ç¬¬ä¸€é˜¶æ®µï¼šåˆå§‹åŒ–f_hat
            target_size = self.patch_nums[-1]  # æœ€å¤§å°ºå¯¸
            f_hat = torch.zeros(B, Cvae, target_size, target_size, device=h_BChw.device, dtype=h_BChw.dtype)
            
            # å°†1x1ç‰¹å¾æ”¾åˆ°å·¦ä¸Šè§’
            f_hat[:, :, :pn, :pn] = h_BChw
        else:
            # åç»­é˜¶æ®µï¼šå°†ç‰¹å¾æ’å…¥åˆ°å¯¹åº”ä½ç½®
            # ä½¿ç”¨åŒçº¿æ€§æ’å€¼å°†ç‰¹å¾æ”¾ç½®åˆ°æ­£ç¡®çš„ç©ºé—´ä½ç½®
            target_size = self.patch_nums[-1]
            scale_factor = target_size // pn
            
            # ä¸Šé‡‡æ ·åˆ°ç›®æ ‡å°ºå¯¸
            h_upsampled = F.interpolate(h_BChw, scale_factor=scale_factor, mode='nearest')
            
            # æ›´æ–°f_hatçš„å¯¹åº”åŒºåŸŸ
            start_h = start_w = 0
            if si > 0:
                # è®¡ç®—å½“å‰é˜¶æ®µåº”è¯¥å¡«å……çš„åŒºåŸŸ
                prev_size = self.patch_nums[si-1] if si > 0 else 0
                start_h = start_w = prev_size
            
            end_h = min(start_h + h_upsampled.shape[2], target_size)
            end_w = min(start_w + h_upsampled.shape[3], target_size)
            
            f_hat[:, :, start_h:end_h, start_w:end_w] = h_upsampled[:, :, :end_h-start_h, :end_w-start_w]
        
        if si == total_stages - 1:
            # æœ€åé˜¶æ®µï¼šè¿”å›æœ€ç»ˆç»“æœ
            return f_hat, None
        else:
            # ä¸­é—´é˜¶æ®µï¼šå‡†å¤‡ä¸‹ä¸€é˜¶æ®µçš„è¾“å…¥
            next_pn = self.patch_nums[si + 1]
            
            # ä»f_hatä¸­æå–ä¸‹ä¸€é˜¶æ®µéœ€è¦çš„åŒºåŸŸ
            next_region = f_hat[:, :, :next_pn, :next_pn]
            
            return f_hat, next_region
    
    def forward_training(
        self,
        gene_expression: torch.Tensor,
        histology_features: torch.Tensor,
        class_labels: Optional[torch.Tensor] = None,
        show_details: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        VARè®­ç»ƒå‰å‘ä¼ æ’­ - æ–¹æ¡ˆCé‡æ„ç‰ˆæœ¬
        
        å®Œå…¨å¯¹é½åŸå§‹VARçš„teacher forcingæœºåˆ¶ï¼š
        - è¾“å…¥ï¼šç¬¬1å±‚çš„ç‰¹å¾
        - ç›®æ ‡ï¼šæ‰€æœ‰å±‚çš„tokens
        - è®­ç»ƒï¼šé¢„æµ‹ç¬¬2å±‚çš„196ä¸ªåŸºå› tokens
        
        Args:
            gene_expression: åŸºå› è¡¨è¾¾ [B, num_genes]
            histology_features: ç»„ç»‡å­¦ç‰¹å¾ [B, histology_dim]
            class_labels: ç±»åˆ«æ ‡ç­¾ [B] (å¯é€‰)
            show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            åŒ…å«æŸå¤±å’ŒæŒ‡æ ‡çš„å­—å…¸
        """
        B = gene_expression.shape[0]
        
        # 1. åŸºå› è¡¨è¾¾ â†’ ä¼ªå›¾åƒ â†’ VQVAEç¼–ç  â†’ å¤šå°ºåº¦tokenåŒ–
        with torch.no_grad():
            pseudo_images = self.gene_adapter(gene_expression)  # [B, 1, 64, 64]
        
        # VQVAEç¼–ç å¾—åˆ°æ½œåœ¨è¡¨ç¤º
        z = self.encoder(pseudo_images)  # [B, z_channels, 4, 4]
        z_flat = z.flatten(2).transpose(1, 2)  # [B, 16, z_channels]
        
        # é‡åŒ–å¾—åˆ°ç¦»æ•£tokens
        distances = torch.cdist(z_flat, self.quantize.weight)  # [B, 16, vocab_size]
        indices = torch.argmin(distances, dim=-1)  # [B, 16]
        z_q = self.quantize(indices)  # [B, 16, z_channels]
        
        # å¤šå°ºåº¦tokenåŒ–
        tokens_list = self.img_to_idxBl_multiscale(z_q, indices)
        
        if show_details:
            print(f"ğŸ§¬ æ–¹æ¡ˆCè®­ç»ƒ (æ­¥éª¤ {self._step_count + 1}):")
            print(f"   å¤šå°ºåº¦tokens: {[t.shape for t in tokens_list]}")
        
        # 2. ğŸ”§ åŸå§‹VARçš„teacher forcingé€»è¾‘
        # è·å–ç›®æ ‡tokensï¼šè¿æ¥æ‰€æœ‰å±‚çš„tokens
        gt_BL = torch.cat(tokens_list, dim=1)  # [B, L] = [B, 1+196] = [B, 197]
        
        # è·å–teacher forcingè¾“å…¥ï¼šå‰N-1å±‚çš„ç‰¹å¾ç”¨äºé¢„æµ‹æ‰€æœ‰Nå±‚
        if len(tokens_list) > 1:
            # æ–¹æ¡ˆCï¼šä½¿ç”¨ç¬¬1å±‚é¢„æµ‹ç¬¬2å±‚
            x_BLCv_wo_first_l = self.idxBl_to_var_input(tokens_list)  # [B, 196, z_channels]
        else:
            x_BLCv_wo_first_l = None
        
        # 3. æ¡ä»¶å¤„ç†
        if class_labels is None:
            class_labels = torch.zeros(B, dtype=torch.long, device=gene_expression.device)
        
        # 4. ğŸ”§ ä½¿ç”¨åŸå§‹VARçš„forwardæ–¹æ³•
        # è¿™é‡Œå®Œå…¨å¯¹é½åŸå§‹VARçš„è®­ç»ƒé€»è¾‘
        with torch.cuda.amp.autocast(enabled=False):
            # Class dropout (åŸå§‹VARçš„æ¡ä»¶dropoutæœºåˆ¶)
            if self.training and hasattr(self, 'cond_drop_rate'):
                drop_mask = torch.rand(B, device=class_labels.device) < getattr(self, 'cond_drop_rate', 0.1)
                class_labels = torch.where(drop_mask, self.num_classes, class_labels)
            
            # Class embedding
            sos = cond_BD = self.class_emb(class_labels)  # [B, embed_dim]
            
            # èµ·å§‹embeddingï¼šç¬¬1å±‚ä½¿ç”¨pos_start
            sos = sos.unsqueeze(1).expand(B, self.first_l, -1) + self.pos_start.expand(B, -1, -1)
            
            # æ„å»ºå®Œæ•´è¾“å…¥åºåˆ—
            if self.prog_si == 0:
                # Progressive trainingçš„ç¬¬ä¸€é˜¶æ®µï¼šåªæœ‰èµ·å§‹tokens
                x_BLC = sos
            else:
                # æ­£å¸¸è®­ç»ƒï¼šèµ·å§‹tokens + teacher forcingè¾“å…¥
                if x_BLCv_wo_first_l is not None:
                    teacher_input = self.word_embed(x_BLCv_wo_first_l.float())  # [B, 196, embed_dim]
                    x_BLC = torch.cat((sos, teacher_input), dim=1)  # [B, 197, embed_dim]
                else:
                    x_BLC = sos
            
            # Progressive trainingèŒƒå›´
            bg, ed = self.begin_ends[self.prog_si] if self.prog_si >= 0 else (0, self.L)
            
            # æ·»åŠ ä½ç½®ç¼–ç å’Œå±‚çº§ç¼–ç 
            lvl_pos = self.lvl_embed(self.lvl_1L[:, :ed].expand(B, -1)) + self.pos_1LC[:, :ed]
            x_BLC += lvl_pos
        
        # 5. ğŸ”§ Transformerå‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨causal maskï¼‰
        attn_bias = self.attn_bias_for_masking[:, :, :ed, :ed]
        cond_BD_or_gss = self.shared_ada_lin(cond_BD) if hasattr(self, 'shared_ada_lin') else cond_BD
        
        # ç»„ç»‡å­¦æ¡ä»¶
        histology_condition = self.condition_processor(histology_features)
        
        # Transformer blocks
        for block in self.blocks:
            x_BLC = block(
                x=x_BLC, 
                cond_BD=cond_BD_or_gss, 
                attn_bias=attn_bias,
                histology_condition=histology_condition
            )
        
        # 6. ğŸ”§ è¾“å‡ºlogitså¹¶è®¡ç®—æŸå¤±
        logits_BLV = self.head(self.head_nm(x_BLC.float(), cond_BD)).float()
        
        # ğŸ”§ æ­£ç¡®çš„VARæŸå¤±è®¡ç®—ï¼šé¢„æµ‹æ‰€æœ‰ä½ç½®çš„tokens
        target_BL = gt_BL[:, :ed]  # æˆªå–åˆ°å½“å‰è®­ç»ƒèŒƒå›´
        var_loss = F.cross_entropy(
            logits_BLV.view(-1, self.vocab_size),
            target_BL.view(-1)
        )
        
        # 7. é‡å»ºæŸå¤±ï¼ˆç”¨äºè¯„ä¼°ï¼Œä¸å‚ä¸ä¸»è¦è®­ç»ƒï¼‰
        with torch.no_grad():
            # é¢„æµ‹çš„tokens
            pred_tokens = logits_BLV.argmax(dim=-1)  # [B, L]
            
            # ä½¿ç”¨ç¬¬2å±‚çš„é¢„æµ‹tokensé‡å»ºåŸºå› è¡¨è¾¾
            if pred_tokens.shape[1] >= 197:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„tokens
                gene_tokens = pred_tokens[:, 1:197]  # ç¬¬2å±‚çš„196ä¸ªtokens
                # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å®ç°ä»tokensåˆ°åŸºå› è¡¨è¾¾çš„é‡å»º
                # ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨MSEå¯¹æ¯”åŸå§‹åŸºå› è¡¨è¾¾
                gene_recon = gene_expression  # å ä½ç¬¦
                recon_loss = F.mse_loss(gene_recon, gene_expression)
            else:
                recon_loss = torch.tensor(0.0, device=gene_expression.device)
        
        # 8. æ€»æŸå¤±
        recon_weight = self.get_recon_weight() if hasattr(self, 'get_recon_weight') else 0.1
        total_loss = var_loss  # ä¸»è¦ä½¿ç”¨VARæŸå¤±ï¼Œé‡å»ºæŸå¤±ä»…ç”¨äºç›‘æ§
        
        if show_details:
            print(f"   VARæŸå¤±: {var_loss.item():.6f}")
            print(f"   é‡å»ºæŸå¤±: {recon_loss.item():.6f}")
            print(f"   ç›®æ ‡tokenså½¢çŠ¶: {target_BL.shape}")
            print(f"   é¢„æµ‹logitså½¢çŠ¶: {logits_BLV.shape}")
        
        self._step_count += 1
        
        return {
            'loss': total_loss,
            'var_loss': var_loss,
            'recon_loss': recon_loss,
            'predictions': gene_expression,  # æš‚æ—¶è¿”å›åŸå§‹å€¼
            'targets': gene_expression,
            'pred_tokens': pred_tokens.detach()
        }

    @torch.no_grad()
    def autoregressive_infer_cfg(
        self, 
        B: int, 
        label_B: Optional[torch.Tensor] = None,
        g_seed: Optional[int] = None,
        cfg: float = 1.5,
        top_k: int = 50,
        top_p: float = 0.9,
        histology_condition: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        VARè‡ªå›å½’æ¨ç† - å®Œæ•´å®ç°ç‰ˆæœ¬
        
        Args:
            B: æ‰¹æ¬¡å¤§å°
            label_B: ç±»åˆ«æ ‡ç­¾ [B]
            g_seed: éšæœºç§å­
            cfg: Classifier-free guidance scale
            top_k: Top-ké‡‡æ ·
            top_p: Top-pé‡‡æ ·
            histology_condition: ç»„ç»‡å­¦æ¡ä»¶ [B, histology_dim]
            
        Returns:
            ç”Ÿæˆçš„å›¾åƒ [B, 1, 64, 64]
        """
        device = next(self.parameters()).device
        
        # è®¾ç½®éšæœºç§å­
        if g_seed is not None:
            self.rng.manual_seed(g_seed)
            rng = self.rng
        else:
            rng = None
        
        # å¤„ç†æ ‡ç­¾
        if label_B is None:
            label_B = torch.zeros(B, dtype=torch.long, device=device)
        elif isinstance(label_B, int):
            label_B = torch.full((B,), label_B, dtype=torch.long, device=device)
        
        # CFGéœ€è¦åŒå€batchï¼šconditional + unconditional
        # Conditionalä½¿ç”¨çœŸå®æ ‡ç­¾ï¼Œunconditionalä½¿ç”¨ç‰¹æ®Šçš„nullæ ‡ç­¾
        null_label = self.num_classes  # ä½¿ç”¨æœ€åä¸€ä¸ªä½œä¸ºnullç±»åˆ«
        label_cond = label_B
        label_uncond = torch.full_like(label_B, null_label)
        labels_cfg = torch.cat([label_cond, label_uncond], dim=0)  # [2B]
        
        # Class embeddings
        cond_BD = self.class_emb(labels_cfg)  # [2B, embed_dim]
        
        # ç»„ç»‡å­¦æ¡ä»¶
        if histology_condition is not None:
            if histology_condition.shape[0] != B:
                raise ValueError(f"ç»„ç»‡å­¦æ¡ä»¶æ‰¹æ¬¡å¤§å° {histology_condition.shape[0]} ä¸ B={B} ä¸åŒ¹é…")
            histology_condition = self.condition_processor(histology_condition)  # [B, embed_dim]
            # å¯¹äºCFGï¼Œä¹Ÿéœ€è¦åŒå€ï¼šconditionalç”¨çœŸå®æ¡ä»¶ï¼Œunconditionalç”¨é›¶å‘é‡
            zero_histology = torch.zeros_like(histology_condition)
            histology_cfg = torch.cat([histology_condition, zero_histology], dim=0)  # [2B, embed_dim]
        else:
            histology_cfg = None
        
        # å¯ç”¨KVç¼“å­˜
        for block in self.blocks:
            if hasattr(block.attn, 'kv_caching'):
                block.attn.kv_caching(True)
        
        # åˆå§‹åŒ–
        lvl_pos = self.lvl_embed(self.lvl_1L) + self.pos_1LC  # [1, L, embed_dim]
        f_hat = torch.zeros(B, self.z_channels, self.patch_nums[-1], self.patch_nums[-1], device=device)
        cur_L = 0
        
        # é€é˜¶æ®µç”Ÿæˆ
        for si, pn in enumerate(self.patch_nums):
            ratio = si / self.num_stages_minus_1 if self.num_stages_minus_1 > 0 else 1.0
            cur_L_start = cur_L
            cur_L += pn * pn
            
            if si == 0:
                # ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨pos_start
                next_token_map = cond_BD.unsqueeze(1).expand(2 * B, self.first_l, -1)
                next_token_map = next_token_map + self.pos_start.expand(2 * B, -1, -1)
                next_token_map = next_token_map + lvl_pos[:, :self.first_l].expand(2 * B, -1, -1)
            else:
                # åç»­é˜¶æ®µï¼šä»f_hatè·å–è¾“å…¥
                # æå–å½“å‰é˜¶æ®µéœ€è¦çš„åŒºåŸŸ
                current_region = f_hat[:, :, :pn, :pn]  # [B, z_channels, pn, pn]
                current_tokens = current_region.flatten(2).transpose(1, 2)  # [B, pn*pn, z_channels]
                
                # Word embedding
                next_token_map = self.word_embed(current_tokens)  # [B, pn*pn, embed_dim]
                
                # ä½ç½®ç¼–ç å’Œå±‚çº§ç¼–ç 
                pos_embed = lvl_pos[:, cur_L_start:cur_L]  # [1, pn*pn, embed_dim]
                next_token_map = next_token_map + pos_embed
                
                # CFGï¼šåŒå€batch
                next_token_map = next_token_map.repeat(2, 1, 1)  # [2B, pn*pn, embed_dim]
            
            # Transformer forward
            x = next_token_map
            for block in self.blocks:
                x = block(
                    x=x, 
                    cond_BD=cond_BD, 
                    attn_bias=None,  # æ¨ç†æ—¶ä¸ä½¿ç”¨maskï¼ˆå› ä¸ºæœ‰KVç¼“å­˜ï¼‰
                    histology_condition=histology_cfg
                )
            
            # è·å–logits
            logits_BlV = self.head(self.head_nm(x, cond_BD))  # [2B, L_current, vocab_size]
            
            # CFG
            if cfg != 1.0:
                t = cfg * ratio
                logits_cond = logits_BlV[:B]
                logits_uncond = logits_BlV[B:]
                logits_BlV = (1 + t) * logits_cond - t * logits_uncond
            else:
                logits_BlV = logits_BlV[:B]
            
            # é‡‡æ ·
            if si < len(self.patch_nums) - 1:
                # ä¸­é—´é˜¶æ®µï¼šåªå¯¹å½“å‰æ–°å¢çš„tokensé‡‡æ ·
                current_logits = logits_BlV[:, -pn*pn:]  # [B, pn*pn, vocab_size]
            else:
                # æœ€åé˜¶æ®µï¼šå¯¹æ‰€æœ‰tokensé‡‡æ ·
                current_logits = logits_BlV
            
            # Top-k Top-pé‡‡æ ·
            idx_Bl = self.sample_with_top_k_top_p(current_logits, top_k=top_k, top_p=top_p, rng=rng)
            
            # é‡åŒ–
            h_BChw = self.quantize(idx_Bl)  # [B, pn*pn, z_channels]
            h_BChw = h_BChw.transpose(1, 2).reshape(B, self.z_channels, pn, pn)  # [B, z_channels, pn, pn]
            
            # æ›´æ–°f_hat
            f_hat, next_input = self.get_next_autoregressive_input(si, len(self.patch_nums), f_hat, h_BChw)
        
        # ç¦ç”¨KVç¼“å­˜
        for block in self.blocks:
            if hasattr(block.attn, 'kv_caching'):
                block.attn.kv_caching(False)
        
        # æœ€ç»ˆè§£ç 
        final_img = self.decoder(f_hat)  # [B, 1, target_size, target_size]
        
        # ç¡®ä¿è¾“å‡ºå°ºå¯¸æ­£ç¡®
        if final_img.shape[-2:] != (self.image_size, self.image_size):
            final_img = F.interpolate(
                final_img, 
                size=(self.image_size, self.image_size), 
                mode='bilinear', 
                align_corners=False
            )
        
        # å½’ä¸€åŒ–åˆ°[0, 1]
        final_img = (final_img + 1) * 0.5
        final_img = torch.clamp(final_img, 0, 1)
        
        return final_img
    
    def sample_with_top_k_top_p(self, logits: torch.Tensor, top_k: int = 0, top_p: float = 0.0, rng=None) -> torch.Tensor:
        """
        Top-k Top-pé‡‡æ ·
        
        Args:
            logits: [B, L, vocab_size]
            top_k: Top-ké™åˆ¶
            top_p: Top-pé™åˆ¶  
            rng: éšæœºæ•°ç”Ÿæˆå™¨
            
        Returns:
            é‡‡æ ·ç»“æœ [B, L]
        """
        B, L, V = logits.shape
        
        # Top-kè¿‡æ»¤
        if top_k > 0:
            top_k = min(top_k, V)
            topk_values, topk_indices = torch.topk(logits, top_k, dim=-1)
            # åˆ›å»ºmask
            mask = torch.full_like(logits, -torch.inf)
            mask.scatter_(-1, topk_indices, topk_values)
            logits = mask
        
        # Top-pè¿‡æ»¤
        if top_p > 0.0 and top_p < 1.0:
            sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
            
            # åˆ›å»ºmaskï¼šä¿ç•™ç´¯ç§¯æ¦‚ç‡å°äºtop_pçš„tokens
            sorted_indices_to_remove = cumulative_probs > top_p
            # ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€ä¸ªtoken
            sorted_indices_to_remove[..., 0] = False
            
            # åº”ç”¨mask
            sorted_logits[sorted_indices_to_remove] = -torch.inf
            
            # æ¢å¤åŸå§‹é¡ºåº
            logits = sorted_logits.gather(-1, sorted_indices.argsort(-1))
        
        # å¤šé¡¹å¼é‡‡æ ·
        probs = F.softmax(logits, dim=-1)
        
        if rng is not None:
            # ä½¿ç”¨æŒ‡å®šçš„éšæœºæ•°ç”Ÿæˆå™¨
            samples = torch.multinomial(probs.view(-1, V), 1, generator=rng).view(B, L)
        else:
            samples = torch.multinomial(probs.view(-1, V), 1).view(B, L)
        
        return samples
    
    def forward_inference(
        self,
        histology_features: torch.Tensor,
        class_labels: Optional[torch.Tensor] = None,
        cfg_scale: float = 1.5,
        top_k: int = 50,
        top_p: float = 0.9,
        temperature: float = 1.0,
        num_samples: int = 1
    ) -> Dict[str, torch.Tensor]:
        """æ¨ç†æ¨¡å¼å‰å‘ä¼ æ’­"""
        B = histology_features.shape[0]
        device = histology_features.device
        
        # ç”Ÿæˆå›¾åƒ
        generated_images = self.autoregressive_infer_cfg(
            B=B,
            label_B=class_labels,
                cfg=cfg_scale,
                top_k=top_k,
                top_p=top_p,
            histology_condition=histology_features
            )
        
        # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡®
        if generated_images.shape[-2:] != (self.image_size, self.image_size):
            generated_images = F.interpolate(
                generated_images,
                size=(self.image_size, self.image_size),
                mode='bilinear',
                align_corners=False
            )
        
        if generated_images.shape[1] != 1:
                generated_images = generated_images.mean(dim=1, keepdim=True)
        
        # è½¬æ¢ä¸ºåŸºå› è¡¨è¾¾
        predicted_genes = self.gene_adapter.pseudo_image_to_genes(generated_images)
        
        return {
            'predicted_expression': predicted_genes,
            'predictions': predicted_genes,
            'generated_images': generated_images
        }
    
    def forward(self, **inputs) -> Dict[str, torch.Tensor]:
        """ä¸»å‰å‘ä¼ æ’­å…¥å£"""
        mode = inputs.get('mode', 'training')
        
        if mode == 'training' or 'gene_expression' in inputs:
            # è®­ç»ƒæ¨¡å¼
            return self.forward_training(
                gene_expression=inputs['gene_expression'],
                histology_features=inputs['histology_features'],
                class_labels=inputs.get('class_labels'),
                show_details=inputs.get('show_details', False)
            )
        else:
            # æ¨ç†æ¨¡å¼
            return self.forward_inference(
                histology_features=inputs['histology_features'],
                class_labels=inputs.get('class_labels'),
                cfg_scale=inputs.get('cfg_scale', 1.5),
                top_k=inputs.get('top_k', 50),
                top_p=inputs.get('top_p', 0.9),
                temperature=inputs.get('temperature', 1.0),
                num_samples=inputs.get('num_samples', 1)
            ) 
    
    def idxBl_to_var_input(self, tokens_list: List[torch.Tensor]) -> torch.Tensor:
        """
        å°†å¤šå°ºåº¦tokenåˆ—è¡¨è½¬æ¢ä¸ºVARçš„teacher forcingè¾“å…¥ - åŸå§‹VARå®ç°
        
        åŸå§‹VARé€»è¾‘ï¼š
        - è¾“å…¥ï¼šå‰N-1ä¸ªå°ºåº¦çš„ç‰¹å¾
        - ç›®æ ‡ï¼šæ‰€æœ‰Nä¸ªå°ºåº¦çš„tokens
        - Teacher forcingï¼šç”¨å‰é¢çš„çœŸå®tokensé¢„æµ‹åé¢çš„tokens
        
        æ–¹æ¡ˆCé€‚é…ï¼š
        - è¾“å…¥ï¼šç¬¬1å±‚(å…¨å±€)çš„ç‰¹å¾ [B, z_channels]
        - ç›®æ ‡ï¼šç¬¬1å±‚+ç¬¬2å±‚çš„æ‰€æœ‰tokens [B, 197]
        - è®­ç»ƒï¼šç¬¬1å±‚é¢„æµ‹ç¬¬2å±‚çš„196ä¸ªåŸºå› tokens
        
        Args:
            tokens_list: [
                [B, 1] - ç¬¬1å±‚å…¨å±€token
                [B, 196] - ç¬¬2å±‚åŸºå› tokens
            ]
            
        Returns:
            teacher_forcing_input: [B, L-first_l, z_channels] = [B, 196, z_channels]
        """
        if len(tokens_list) != 2:
            raise ValueError(f"æ–¹æ¡ˆCéœ€è¦2å±‚tokensï¼Œå¾—åˆ°: {len(tokens_list)}")
        
        B = tokens_list[0].shape[0]
        
        # ğŸ”§ æ–¹æ¡ˆCçš„teacher forcingé€»è¾‘ï¼š
        # - ç¬¬1å±‚(å…¨å±€token)ä½œä¸ºèµ·å§‹ï¼Œä¸éœ€è¦teacher forcingè¾“å…¥
        # - ç¬¬2å±‚(åŸºå› tokens)éœ€è¦åŸºäºç¬¬1å±‚çš„ç‰¹å¾æ¥é¢„æµ‹
        
        # è·å–ç¬¬1å±‚çš„token embeddingä½œä¸ºç¬¬2å±‚çš„è¾“å…¥
        global_tokens = tokens_list[0]  # [B, 1]
        
        # å°†ç¬¬1å±‚tokenè½¬æ¢ä¸ºç‰¹å¾ç©ºé—´
        global_features = self.quantize(global_tokens)  # [B, 1, z_channels]
        
        # ğŸ”§ æ–¹æ¡ˆCæ ¸å¿ƒï¼šç¬¬2å±‚éœ€è¦196ä¸ªè¾“å…¥ï¼Œæ¯ä¸ªéƒ½åŸºäºå…¨å±€ç‰¹å¾
        # å°†å…¨å±€ç‰¹å¾æ‰©å±•åˆ°196ä¸ªä½ç½®ï¼Œä¸ºç¬¬2å±‚æä¾›teacher forcingè¾“å…¥
        expanded_features = global_features.repeat(1, 196, 1)  # [B, 196, z_channels]
        
        # ğŸ”§ æ·»åŠ ä½ç½®ä¿¡æ¯ï¼Œè®©æ¯ä¸ªåŸºå› ä½ç½®éƒ½æœ‰ç‹¬ç‰¹çš„ç‰¹å¾
        if hasattr(self, 'gene_position_embed'):
            # å¦‚æœæœ‰åŸºå› ä½ç½®ç¼–ç ï¼Œæ·»åŠ åˆ°ç‰¹å¾ä¸­
            position_embed = self.gene_position_embed.weight.unsqueeze(0)  # [1, 196, z_channels]
            expanded_features = expanded_features + position_embed
        else:
            # ç®€å•çš„ä½ç½®æ‰°åŠ¨ï¼Œé¿å…æ‰€æœ‰åŸºå› ä½ç½®å®Œå…¨ç›¸åŒ
            position_noise = torch.randn_like(expanded_features) * 0.01
            expanded_features = expanded_features + position_noise
        
        return expanded_features 