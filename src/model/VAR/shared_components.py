"""
å…±äº«ç»„ä»¶æ¨¡å— - ä¸¤é˜¶æ®µVAR-STçš„æ ¸å¿ƒç»„ä»¶

åŒ…å«ï¼š
1. SharedVectorQuantizer: ç¬¦åˆVARåŸå§‹è®¾è®¡çš„å•ä¸€å…±äº«codebook
2. MultiScaleEncoder: ç”Ÿç‰©å­¦å¤šå°ºåº¦ç¼–ç å™¨
3. MultiScaleDecoder: å¯¹åº”çš„è§£ç å™¨  
4. ResidualReconstructor: æ®‹å·®é‡å»ºç­–ç•¥

ä¸¥æ ¼éµå¾ªVARåŸå§‹è®¾è®¡ï¼š
- å•ä¸€å…±äº«codebookï¼Œè¯æ±‡è¡¨å¤§å°4096
- æ‰€æœ‰å°ºåº¦ç¼–ç å™¨è¾“å‡ºç»Ÿä¸€128ç»´
- ç”Ÿç‰©å­¦å¤šå°ºåº¦ï¼šGlobal(1) â†’ Pathway(8) â†’ Module(32) â†’ Individual(200)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Tuple, Dict, List


class SharedVectorQuantizer(nn.Module):
    """
    å…±äº«å‘é‡é‡åŒ–å™¨ - ä¸¥æ ¼éµå¾ªVARåŸå§‹è®¾è®¡
    
    ç‰¹æ€§ï¼š
    - å•ä¸€å…±äº«codebookï¼Œæ‰€æœ‰å°ºåº¦ä½¿ç”¨åŒä¸€è¯æ±‡è¡¨
    - è¯æ±‡è¡¨å¤§å°4096ï¼ˆä¸VARä¸€è‡´ï¼‰
    - åµŒå…¥ç»´åº¦128
    - æ”¯æŒä¸åŒå½¢çŠ¶çš„è¾“å…¥å¼ é‡
    """
    
    def __init__(
        self,
        vocab_size: int = 4096,
        embed_dim: int = 128,
        beta: float = 0.25
    ):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.beta = beta
        
        # å…±äº«codebook - VARæ ¸å¿ƒè®¾è®¡
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        nn.init.uniform_(self.embedding.weight, -1/vocab_size, 1/vocab_size)
        
        print(f"ğŸ”§ SharedVectorQuantizeråˆå§‹åŒ–:")
        print(f"   è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        print(f"   åµŒå…¥ç»´åº¦: {embed_dim}")
        print(f"   Î²å‚æ•°: {beta}")
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [B, N, embed_dim] æˆ– [B, embed_dim]
            
        Returns:
            tokens: ç¦»æ•£tokenç´¢å¼• [B, N] æˆ– [B]
            quantized: é‡åŒ–åçš„ç‰¹å¾ [B, N, embed_dim] æˆ– [B, embed_dim]  
            vq_loss: VQæŸå¤±æ ‡é‡
        """
        input_shape = x.shape
        
        # å¤„ç†ä¸åŒè¾“å…¥å½¢çŠ¶
        if x.dim() == 2:
            # [B, embed_dim] â†’ [B, 1, embed_dim]
            x = x.unsqueeze(1)
            squeeze_output = True
        elif x.dim() == 3:
            # [B, N, embed_dim] ä¿æŒä¸å˜
            squeeze_output = False
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥ç»´åº¦: {x.shape}")
        
        B, N, D = x.shape
        assert D == self.embed_dim, f"è¾“å…¥ç»´åº¦ {D} ä¸åŒ¹é…åµŒå…¥ç»´åº¦ {self.embed_dim}"
        
        # è®¡ç®—è·ç¦»å¹¶è·å–æœ€è¿‘çš„codebook entry
        flat_x = x.view(-1, D)  # [B*N, D]
        
        # è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»
        distances = torch.cdist(flat_x, self.embedding.weight)  # [B*N, vocab_size]
        tokens_flat = torch.argmin(distances, dim=1)  # [B*N]
        tokens = tokens_flat.view(B, N)  # [B, N]
        
        # è·å–é‡åŒ–ç‰¹å¾
        quantized = self.embedding(tokens)  # [B, N, embed_dim]
        
        # è®¡ç®—VQæŸå¤±
        commitment_loss = F.mse_loss(quantized.detach(), x)
        embedding_loss = F.mse_loss(quantized, x.detach())
        vq_loss = embedding_loss + self.beta * commitment_loss
        
        # ç›´é€šä¼°è®¡å™¨
        quantized = x + (quantized - x).detach()
        
        # æ ¹æ®è¾“å…¥å½¢çŠ¶è°ƒæ•´è¾“å‡º
        if squeeze_output:
            tokens = tokens.squeeze(1)  # [B, 1] â†’ [B]
            quantized = quantized.squeeze(1)  # [B, 1, embed_dim] â†’ [B, embed_dim]
        
        return tokens, quantized, vq_loss
    
    def decode(self, tokens: torch.Tensor) -> torch.Tensor:
        """
        ä»tokensè§£ç ä¸ºç‰¹å¾
        
        Args:
            tokens: tokenç´¢å¼• [B, N] æˆ– [B]
            
        Returns:
            ç‰¹å¾ [B, N, embed_dim] æˆ– [B, embed_dim]
        """
        return self.embedding(tokens)


class GlobalEncoder(nn.Module):
    """Globalå±‚ç¼–ç å™¨: [B, 1] â†’ [B, 1, 128]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        self.embed_dim = embed_dim
        
        self.encoder = nn.Sequential(
            nn.Linear(1, 64),
            nn.ReLU(),
            nn.Linear(64, embed_dim)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [B, 1] - Globalç‰¹å¾
        Returns:
            [B, 1, 128] - ç¼–ç åç‰¹å¾
        """
        B = x.shape[0]
        encoded = self.encoder(x)  # [B, 1] â†’ [B, 128]
        return encoded.view(B, 1, self.embed_dim)  # [B, 1, 128]


class PathwayEncoder(nn.Module):
    """Pathwayå±‚ç¼–ç å™¨: [B, 8] â†’ [B, 8, 128]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        self.embed_dim = embed_dim
        
        self.encoder = nn.Sequential(
            nn.Linear(1, 64),  # æ¯ä¸ªpathwayç‹¬ç«‹ç¼–ç 
            nn.ReLU(),
            nn.Linear(64, embed_dim)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [B, 8] - Pathwayç‰¹å¾
        Returns:
            [B, 8, 128] - ç¼–ç åç‰¹å¾
        """
        B, N = x.shape
        x_expanded = x.unsqueeze(-1)  # [B, 8, 1]
        encoded = self.encoder(x_expanded)  # [B, 8, 128]
        return encoded


class ModuleEncoder(nn.Module):
    """Moduleå±‚ç¼–ç å™¨: [B, 32] â†’ [B, 32, 128]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        self.embed_dim = embed_dim
        
        self.encoder = nn.Sequential(
            nn.Linear(1, 96),  # æ¯ä¸ªmoduleç‹¬ç«‹ç¼–ç 
            nn.ReLU(),
            nn.Linear(96, embed_dim)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [B, 32] - Moduleç‰¹å¾
        Returns:
            [B, 32, 128] - ç¼–ç åç‰¹å¾
        """
        B, N = x.shape
        x_expanded = x.unsqueeze(-1)  # [B, 32, 1]
        encoded = self.encoder(x_expanded)  # [B, 32, 128]
        return encoded


class IndividualEncoder(nn.Module):
    """Individualå±‚ç¼–ç å™¨: [B, 200] â†’ [B, 200, 128]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        self.embed_dim = embed_dim
        
        self.encoder = nn.Sequential(
            nn.Linear(1, 256),  # æ¯ä¸ªåŸºå› ç‹¬ç«‹ç¼–ç 
            nn.ReLU(),
            nn.Linear(256, embed_dim)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [B, 200] - Individualç‰¹å¾
        Returns:
            [B, 200, 128] - ç¼–ç åç‰¹å¾
        """
        B, N = x.shape
        x_expanded = x.unsqueeze(-1)  # [B, 200, 1]
        encoded = self.encoder(x_expanded)  # [B, 200, 128]
        return encoded


class GlobalDecoder(nn.Module):
    """Globalå±‚è§£ç å™¨: [B, 1, 128] â†’ [B, 1]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        
        self.decoder = nn.Sequential(
            nn.Linear(embed_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [B, 1, 128] - é‡åŒ–ç‰¹å¾
        Returns:
            [B, 1] - é‡å»ºç‰¹å¾
        """
        B = x.shape[0]
        x_flat = x.view(B, -1)  # [B, 128]
        decoded = self.decoder(x_flat)  # [B, 1]
        return decoded


class PathwayDecoder(nn.Module):
    """Pathwayå±‚è§£ç å™¨: [B, 8, 128] â†’ [B, 8]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        
        self.decoder = nn.Sequential(
            nn.Linear(embed_dim, 64),
            nn.ReLU(), 
            nn.Linear(64, 1)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [B, 8, 128] - é‡åŒ–ç‰¹å¾
        Returns:
            [B, 8] - é‡å»ºç‰¹å¾
        """
        B, N, D = x.shape
        decoded = self.decoder(x)  # [B, 8, 1]
        return decoded.squeeze(-1)  # [B, 8]


class ModuleDecoder(nn.Module):
    """Moduleå±‚è§£ç å™¨: [B, 32, 128] â†’ [B, 32]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        
        self.decoder = nn.Sequential(
            nn.Linear(embed_dim, 96),
            nn.ReLU(),
            nn.Linear(96, 1)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [B, 32, 128] - é‡åŒ–ç‰¹å¾
        Returns:
            [B, 32] - é‡å»ºç‰¹å¾
        """
        B, N, D = x.shape
        decoded = self.decoder(x)  # [B, 32, 1]
        return decoded.squeeze(-1)  # [B, 32]


class IndividualDecoder(nn.Module):
    """Individualå±‚è§£ç å™¨: [B, 200, 128] â†’ [B, 200]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        
        self.decoder = nn.Sequential(
            nn.Linear(embed_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [B, 200, 128] - é‡åŒ–ç‰¹å¾
        Returns:
            [B, 200] - é‡å»ºç‰¹å¾
        """
        B, N, D = x.shape
        decoded = self.decoder(x)  # [B, 200, 1]
        return decoded.squeeze(-1)  # [B, 200]


class ResidualReconstructor(nn.Module):
    """
    æ®‹å·®é‡å»ºå™¨ - å®ç°ç”Ÿç‰©å­¦å¤šå°ºåº¦çš„æ®‹å·®é‡å»ºç­–ç•¥
    
    é‡å»ºé¡ºåºï¼š
    1. Globalé‡å»º â†’ å¹¿æ’­åˆ°200ç»´
    2. Pathwayé‡å»º â†’ å¹¿æ’­åˆ°200ç»´ï¼ŒåŠ åˆ°GlobalåŸºç¡€ä¸Š
    3. Moduleé‡å»º â†’ å¹¿æ’­åˆ°200ç»´ï¼ŒåŠ åˆ°å‰ä¸¤å±‚åŸºç¡€ä¸Š  
    4. Individualé‡å»º â†’ ç›´æ¥200ç»´ï¼Œæœ€ç»ˆç»†èŠ‚
    """
    
    def __init__(self):
        super().__init__()
        
    def forward(
        self, 
        global_recon: torch.Tensor,      # [B, 1]
        pathway_recon: torch.Tensor,     # [B, 8] 
        module_recon: torch.Tensor,      # [B, 32]
        individual_recon: torch.Tensor   # [B, 200]
    ) -> Dict[str, torch.Tensor]:
        """
        æ®‹å·®é‡å»º
        
        Args:
            global_recon: Globalå±‚é‡å»º [B, 1]
            pathway_recon: Pathwayå±‚é‡å»º [B, 8]
            module_recon: Moduleå±‚é‡å»º [B, 32]
            individual_recon: Individualå±‚é‡å»º [B, 200]
            
        Returns:
            åŒ…å«å„å±‚é‡å»ºå’Œæœ€ç»ˆé‡å»ºçš„å­—å…¸
        """
        B = global_recon.shape[0]
        
        # 1. Globalå±‚å¹¿æ’­
        global_broadcast = global_recon.expand(B, 200)  # [B, 1] â†’ [B, 200]
        
        # 2. Pathwayå±‚å¹¿æ’­ï¼ˆ8ä¸ªé€šè·¯ï¼Œæ¯ä¸ªå¯¹åº”25ä¸ªåŸºå› ï¼‰
        pathway_broadcast = pathway_recon.repeat_interleave(25, dim=1)  # [B, 8] â†’ [B, 200]
        
        # 3. Moduleå±‚å¹¿æ’­ï¼ˆ32ä¸ªæ¨¡å—ï¼Œæ¯ä¸ªå¯¹åº”6.25ä¸ªåŸºå› ï¼Œéœ€è¦å¤„ç†éæ•´æ•°ï¼‰
        # ä½¿ç”¨çº¿æ€§æ’å€¼çš„æ–¹å¼å¤„ç†éæ•´æ•°å€çš„æ˜ å°„
        module_expanded = F.interpolate(
            module_recon.unsqueeze(1),  # [B, 32] â†’ [B, 1, 32]
            size=200,
            mode='linear',
            align_corners=False
        ).squeeze(1)  # [B, 1, 200] â†’ [B, 200]
        
        # 4. æ®‹å·®ç´¯ç§¯é‡å»º
        cumulative_recon = global_broadcast.clone()
        cumulative_recon = cumulative_recon + pathway_broadcast
        cumulative_recon = cumulative_recon + module_expanded
        final_recon = cumulative_recon + individual_recon
        
        return {
            'global_broadcast': global_broadcast,
            'pathway_broadcast': pathway_broadcast, 
            'module_broadcast': module_expanded,
            'individual_contribution': individual_recon,
            'cumulative_without_individual': cumulative_recon,
            'final_reconstruction': final_recon
        }


class MultiScaleDecomposer(nn.Module):
    """
    å¤šå°ºåº¦åˆ†è§£å™¨ - å°†200ç»´åŸºå› è¡¨è¾¾åˆ†è§£ä¸ºç”Ÿç‰©å­¦å¤šå°ºåº¦
    
    åˆ†è§£ç­–ç•¥ï¼š
    - Global(1): æ‰€æœ‰åŸºå› çš„å¹³å‡å€¼ï¼Œä»£è¡¨æ•´ä½“è½¬å½•æ´»è·ƒåº¦
    - Pathway(8): å°†200ä¸ªåŸºå› åˆ†ä¸º8ç»„ï¼Œæ¯ç»„25ä¸ªåŸºå› çš„å¹³å‡å€¼
    - Module(32): å°†200ä¸ªåŸºå› åˆ†ä¸º32ç»„ï¼Œæ¯ç»„6.25ä¸ªåŸºå› çš„å¹³å‡å€¼  
    - Individual(200): ä¿æŒåŸå§‹å•åŸºå› åˆ†è¾¨ç‡
    """
    
    def __init__(self):
        super().__init__()
        
    def forward(self, gene_expression: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        å¤šå°ºåº¦åˆ†è§£
        
        Args:
            gene_expression: [B, 200] - åŸå§‹åŸºå› è¡¨è¾¾
            
        Returns:
            åŒ…å«å„å°ºåº¦ç‰¹å¾çš„å­—å…¸
        """
        B, num_genes = gene_expression.shape
        assert num_genes == 200, f"æœŸæœ›200ä¸ªåŸºå› ï¼Œå¾—åˆ°{num_genes}"
        
        # Globalå±‚ï¼šæ•´ä½“å¹³å‡
        global_features = gene_expression.mean(dim=1, keepdim=True)  # [B, 1]
        
        # Pathwayå±‚ï¼š8ä¸ªç”Ÿç‰©å­¦é€šè·¯
        pathway_features = F.adaptive_avg_pool1d(
            gene_expression.unsqueeze(1), 8
        ).squeeze(1)  # [B, 200] â†’ [B, 1, 200] â†’ [B, 1, 8] â†’ [B, 8]
        
        # Moduleå±‚ï¼š32ä¸ªåŠŸèƒ½æ¨¡å—
        module_features = F.adaptive_avg_pool1d(
            gene_expression.unsqueeze(1), 32
        ).squeeze(1)  # [B, 200] â†’ [B, 1, 200] â†’ [B, 1, 32] â†’ [B, 32]
        
        # Individualå±‚ï¼šä¿æŒåŸå§‹åˆ†è¾¨ç‡
        individual_features = gene_expression.clone()  # [B, 200]
        
        return {
            'global': global_features,      # [B, 1]
            'pathway': pathway_features,    # [B, 8]
            'module': module_features,      # [B, 32]
            'individual': individual_features  # [B, 200]
        } 