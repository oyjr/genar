"""
å…±äº«ç»„ä»¶æ¨¡å— - ä¸¤é˜¶æ®µVAR-STçš„æ ¸å¿ƒç»„ä»¶

ğŸ”§ ä¸»è¦ç‰¹æ€§ï¼š
1. æ”¹è¿›codebookåˆå§‹åŒ– (Xavier â†’ std=0.02)
2. å¢å¤§commitment lossæƒé‡ (0.25 â†’ 0.5)
3. æ·»åŠ EMAæ›´æ–°æ”¯æŒ
4. æ·»åŠ codebookåˆ©ç”¨ç‡ç›‘æ§
5. ç¼–ç å™¨æ·»åŠ LayerNormç¨³å®šè®­ç»ƒ

ä¸¥æ ¼éµå¾ªVARåŸå§‹è®¾è®¡ï¼š
- å•ä¸€å…±äº«codebookï¼Œè¯æ±‡è¡¨å¤§å°4096
- æ‰€æœ‰å°ºåº¦ç¼–ç å™¨è¾“å‡ºç»Ÿä¸€128ç»´
- ç”Ÿç‰©å­¦å¤šå°ºåº¦ï¼šGlobal(1) â†’ Pathway(8) â†’ Module(32) â†’ Individual(200)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Tuple, Dict, List


class SharedVectorQuantizer(nn.Module):
    """
    å…±äº«å‘é‡é‡åŒ–å™¨ - æ”¹è¿›ç‰ˆï¼Œè§£å†³Codebook Collapseé—®é¢˜
    
    ğŸ”§ å…³é”®æ”¹è¿›ï¼š
    - æ›´å¤§çš„åˆå§‹åŒ–èŒƒå›´ï¼Œé¿å…codebookå‘é‡è¿‡äºç›¸ä¼¼
    - å¢å¤§commitment lossæƒé‡ï¼Œå¼ºåŒ–ç¼–ç å™¨å­¦ä¹ 
    - æ”¯æŒEMAæ›´æ–°ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
    - æ·»åŠ codebookåˆ©ç”¨ç‡ç›‘æ§
    """
    
    def __init__(
        self,
        vocab_size: int = 4096,
        embed_dim: int = 128,
        beta: float = 0.5,  # ğŸ”§ å¢å¤§commitment lossæƒé‡
        use_ema: bool = True,  # ğŸ†• å¯ç”¨EMAæ›´æ–°
        ema_decay: float = 0.99,
        epsilon: float = 1e-5
    ):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.beta = beta
        self.use_ema = use_ema
        self.ema_decay = ema_decay
        self.epsilon = epsilon
        
        # ğŸ”§ æ”¹è¿›çš„codebookåˆå§‹åŒ–
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        # ä½¿ç”¨æ›´å¤§çš„åˆå§‹åŒ–èŒƒå›´ï¼Œç¡®ä¿codebookå‘é‡å¤šæ ·æ€§
        std = 0.02  # æ¯”åŸæ¥çš„1/vocab_size=0.0002å¤§100å€
        nn.init.normal_(self.embedding.weight, mean=0, std=std)
        
        # ğŸ†• EMAç»Ÿè®¡
        if use_ema:
            self.register_buffer('cluster_size', torch.zeros(vocab_size))
            self.register_buffer('embed_avg', self.embedding.weight.data.clone())
        
        # ğŸ†• åˆ©ç”¨ç‡ç»Ÿè®¡
        self.register_buffer('usage_count', torch.zeros(vocab_size))
        
        # Initialization complete
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­ - æ”¹è¿›ç‰ˆ"""
        input_shape = x.shape
        
        # å¤„ç†ä¸åŒè¾“å…¥å½¢çŠ¶
        if x.dim() == 2:
            x = x.unsqueeze(1)
            squeeze_output = True
        elif x.dim() == 3:
            squeeze_output = False
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥ç»´åº¦: {x.shape}")
        
        B, N, D = x.shape
        assert D == self.embed_dim, f"è¾“å…¥ç»´åº¦ {D} ä¸åŒ¹é…åµŒå…¥ç»´åº¦ {self.embed_dim}"
        
        # è®¡ç®—è·ç¦»å¹¶è·å–æœ€è¿‘çš„codebook entry
        flat_x = x.view(-1, D)  # [B*N, D]
        
        # è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»
        distances = torch.cdist(flat_x, self.embedding.weight)  # [B*N, vocab_size]
        tokens_flat = torch.argmin(distances, dim=1)  # [B*N]
        tokens = tokens_flat.view(B, N)  # [B, N]
        
        # è·å–é‡åŒ–ç‰¹å¾
        quantized = self.embedding(tokens)  # [B, N, embed_dim]
        
        # ğŸ†• æ›´æ–°ä½¿ç”¨ç»Ÿè®¡
        with torch.no_grad():
            token_counts = torch.bincount(tokens_flat, minlength=self.vocab_size).float()
            self.usage_count.add_(token_counts)
        
        # ğŸ†• EMAæ›´æ–°ï¼ˆåªåœ¨è®­ç»ƒæ—¶ï¼‰
        if self.training and self.use_ema:
            self._ema_update(flat_x, tokens_flat)
        
        # ğŸ”§ æ”¹è¿›çš„VQæŸå¤± - æ›´é«˜çš„commitment weight
        commitment_loss = F.mse_loss(quantized.detach(), x)
        embedding_loss = F.mse_loss(quantized, x.detach())
        vq_loss = embedding_loss + self.beta * commitment_loss
        
        # ç›´é€šä¼°è®¡å™¨
        quantized = x + (quantized - x).detach()
        
        # æ ¹æ®è¾“å…¥å½¢çŠ¶è°ƒæ•´è¾“å‡º
        if squeeze_output:
            tokens = tokens.squeeze(1)  # [B, 1] â†’ [B]
            quantized = quantized.squeeze(1)  # [B, 1, embed_dim] â†’ [B, embed_dim]
        
        return tokens, quantized, vq_loss
    
    def _ema_update(self, flat_x: torch.Tensor, tokens_flat: torch.Tensor):
        """ğŸ†• EMAæ›´æ–°codebook - ä¿®å¤å†…å­˜æ³„æ¼"""
        with torch.no_grad():  # ğŸ”§ ç¡®ä¿æ•´ä¸ªEMAæ›´æ–°è¿‡ç¨‹ä¸ä¿ç•™è®¡ç®—å›¾
            # è®¡ç®—æ¯ä¸ªtokençš„ä½¿ç”¨æ¬¡æ•°
            token_counts = torch.bincount(tokens_flat, minlength=self.vocab_size).float()
            
            # æ›´æ–°cluster size
            self.cluster_size.mul_(self.ema_decay).add_(token_counts, alpha=1 - self.ema_decay)
            
            # è®¡ç®—æ¯ä¸ªtokenå¯¹åº”çš„ç‰¹å¾å¹³å‡å€¼
            embed_sum = torch.zeros_like(self.embed_avg)
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿flat_xä¸ä¿ç•™è®¡ç®—å›¾
            flat_x_detached = flat_x.detach()
            embed_sum.index_add_(0, tokens_flat, flat_x_detached)
            
            # æ›´æ–°embedding average
            self.embed_avg.mul_(self.ema_decay).add_(embed_sum, alpha=1 - self.ema_decay)
            
            # æ›´æ–°embeddingæƒé‡
            cluster_size = self.cluster_size + self.epsilon
            embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)
            self.embedding.weight.data.copy_(embed_normalized)
    
    def get_codebook_utilization(self) -> float:
        """ğŸ†• è·å–codebookåˆ©ç”¨ç‡"""
        used_codes = (self.usage_count > 0).sum().item()
        return used_codes / self.vocab_size
    
    def decode(self, tokens: torch.Tensor) -> torch.Tensor:
        """ä»tokensè§£ç ä¸ºç‰¹å¾"""
        return self.embedding(tokens)


# å…¶ä»–ç»„ä»¶ä¿æŒä¸å˜ï¼Œåªæ·»åŠ æ”¹è¿›çš„ç¼–ç å™¨ï¼ˆå¯é€‰ï¼‰
class GlobalEncoder(nn.Module):
    """Globalå±‚ç¼–ç å™¨: [B, 1] â†’ [B, 1, 128]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        self.embed_dim = embed_dim
        
        # ğŸ”§ å¯é€‰æ”¹è¿›ï¼šæ·»åŠ LayerNorm
        self.encoder = nn.Sequential(
            nn.Linear(1, 64),
            nn.LayerNorm(64),  # ğŸ†• ç¨³å®šè®­ç»ƒ
            nn.ReLU(),
            nn.Linear(64, embed_dim)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B = x.shape[0]
        encoded = self.encoder(x)  # [B, 1] â†’ [B, 128]
        return encoded.view(B, 1, self.embed_dim)


class PathwayEncoder(nn.Module):
    """Pathwayå±‚ç¼–ç å™¨: [B, 8] â†’ [B, 8, 128]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        self.embed_dim = embed_dim
        
        self.encoder = nn.Sequential(
            nn.Linear(1, 64),
            nn.LayerNorm(64),  # ğŸ†• ç¨³å®šè®­ç»ƒ
            nn.ReLU(),
            nn.Linear(64, embed_dim)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N = x.shape
        x_expanded = x.unsqueeze(-1)  # [B, 8, 1]
        encoded = self.encoder(x_expanded)  # [B, 8, 128]
        return encoded


class ModuleEncoder(nn.Module):
    """Moduleå±‚ç¼–ç å™¨: [B, 32] â†’ [B, 32, 128]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        self.embed_dim = embed_dim
        
        self.encoder = nn.Sequential(
            nn.Linear(1, 96),
            nn.LayerNorm(96),  # ğŸ†• ç¨³å®šè®­ç»ƒ
            nn.ReLU(),
            nn.Linear(96, embed_dim)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N = x.shape
        x_expanded = x.unsqueeze(-1)  # [B, 32, 1]
        encoded = self.encoder(x_expanded)  # [B, 32, 128]
        return encoded


class IndividualEncoder(nn.Module):
    """Individualå±‚ç¼–ç å™¨: [B, 200] â†’ [B, 200, 128]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        self.embed_dim = embed_dim
        
        self.encoder = nn.Sequential(
            nn.Linear(1, 256),
            nn.LayerNorm(256),  # ğŸ†• ç¨³å®šè®­ç»ƒ
            nn.ReLU(),
            nn.Linear(256, embed_dim)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N = x.shape
        x_expanded = x.unsqueeze(-1)  # [B, 200, 1]
        encoded = self.encoder(x_expanded)  # [B, 200, 128]
        return encoded


# è§£ç å™¨ä¿æŒä¸å˜...
class GlobalDecoder(nn.Module):
    """Globalå±‚è§£ç å™¨: [B, 1, 128] â†’ [B, 1]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        
        self.decoder = nn.Sequential(
            nn.Linear(embed_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B = x.shape[0]
        x_flat = x.view(B, -1)  # [B, 128]
        decoded = self.decoder(x_flat)  # [B, 1]
        return decoded


class PathwayDecoder(nn.Module):
    """Pathwayå±‚è§£ç å™¨: [B, 8, 128] â†’ [B, 8]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        
        self.decoder = nn.Sequential(
            nn.Linear(embed_dim, 64),
            nn.ReLU(), 
            nn.Linear(64, 1)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N, D = x.shape
        decoded = self.decoder(x)  # [B, 8, 1]
        return decoded.squeeze(-1)  # [B, 8]


class ModuleDecoder(nn.Module):
    """Moduleå±‚è§£ç å™¨: [B, 32, 128] â†’ [B, 32]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        
        self.decoder = nn.Sequential(
            nn.Linear(embed_dim, 96),
            nn.ReLU(),
            nn.Linear(96, 1)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N, D = x.shape
        decoded = self.decoder(x)  # [B, 32, 1]
        return decoded.squeeze(-1)  # [B, 32]


class IndividualDecoder(nn.Module):
    """Individualå±‚è§£ç å™¨: [B, 200, 128] â†’ [B, 200]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        
        self.decoder = nn.Sequential(
            nn.Linear(embed_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N, D = x.shape
        decoded = self.decoder(x)  # [B, 200, 1]
        return decoded.squeeze(-1)  # [B, 200]


class ResidualReconstructor(nn.Module):
    """æ®‹å·®é‡å»ºå™¨ - ä¸å˜"""
    
    def __init__(self):
        super().__init__()
        
    def forward(
        self, 
        global_recon: torch.Tensor,
        pathway_recon: torch.Tensor,
        module_recon: torch.Tensor,
        individual_recon: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        B = global_recon.shape[0]
        
        # 1. Globalå±‚å¹¿æ’­
        global_broadcast = global_recon.expand(B, 200)  # [B, 1] â†’ [B, 200]
        
        # 2. Pathwayå±‚å¹¿æ’­
        pathway_broadcast = pathway_recon.repeat_interleave(25, dim=1)  # [B, 8] â†’ [B, 200]
        
        # 3. Moduleå±‚å¹¿æ’­
        module_expanded = F.interpolate(
            module_recon.unsqueeze(1),  # [B, 32] â†’ [B, 1, 32]
            size=200,
            mode='linear',
            align_corners=False
        ).squeeze(1)  # [B, 1, 200] â†’ [B, 200]
        
        # 4. æ®‹å·®ç´¯ç§¯é‡å»º
        cumulative_recon = global_broadcast.clone()
        cumulative_recon = cumulative_recon + pathway_broadcast
        cumulative_recon = cumulative_recon + module_expanded
        final_recon = cumulative_recon + individual_recon
        
        return {
            'global_broadcast': global_broadcast,
            'pathway_broadcast': pathway_broadcast, 
            'module_broadcast': module_expanded,
            'individual_contribution': individual_recon,
            'cumulative_without_individual': cumulative_recon,
            'final_reconstruction': final_recon
        }


class MultiScaleDecomposer(nn.Module):
    """å¤šå°ºåº¦åˆ†è§£å™¨ - ä¸å˜"""
    
    def __init__(self):
        super().__init__()
        
    def forward(self, gene_expression: torch.Tensor) -> Dict[str, torch.Tensor]:
        B, num_genes = gene_expression.shape
        assert num_genes == 200, f"æœŸæœ›200ä¸ªåŸºå› ï¼Œå¾—åˆ°{num_genes}"
        
        # Globalå±‚ï¼šæ•´ä½“å¹³å‡
        global_features = gene_expression.mean(dim=1, keepdim=True)  # [B, 1]
        
        # Pathwayå±‚ï¼š8ä¸ªç”Ÿç‰©å­¦é€šè·¯
        pathway_features = F.adaptive_avg_pool1d(
            gene_expression.unsqueeze(1), 8
        ).squeeze(1)  # [B, 8]
        
        # Moduleå±‚ï¼š32ä¸ªåŠŸèƒ½æ¨¡å—
        module_features = F.adaptive_avg_pool1d(
            gene_expression.unsqueeze(1), 32
        ).squeeze(1)  # [B, 32]
        
        # Individualå±‚ï¼šä¿æŒåŸå§‹åˆ†è¾¨ç‡
        individual_features = gene_expression.clone()  # [B, 200]
        
        return {
            'global': global_features,
            'pathway': pathway_features,
            'module': module_features,
            'individual': individual_features
        } 
