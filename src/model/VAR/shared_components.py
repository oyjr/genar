"""
ÂÖ±‰∫´ÁªÑ‰ª∂Ê®°Âùó - ‰∏§Èò∂ÊÆµVAR-STÁöÑÊ†∏ÂøÉÁªÑ‰ª∂

üîß ‰∏ªË¶ÅÁâπÊÄßÔºö
1. ÊîπËøõcodebookÂàùÂßãÂåñ (Xavier ‚Üí std=0.02)
2. Â¢ûÂ§ßcommitment lossÊùÉÈáç (0.25 ‚Üí 0.5)
3. Ê∑ªÂä†EMAÊõ¥Êñ∞ÊîØÊåÅ
4. Ê∑ªÂä†codebookÂà©Áî®ÁéáÁõëÊéß
5. ÁºñÁ†ÅÂô®Ê∑ªÂä†LayerNormÁ®≥ÂÆöËÆ≠ÁªÉ

‰∏•Ê†ºÈÅµÂæ™VARÂéüÂßãËÆæËÆ°Ôºö
- Âçï‰∏ÄÂÖ±‰∫´codebookÔºåËØçÊ±áË°®Â§ßÂ∞è4096
- ÊâÄÊúâÂ∞∫Â∫¶ÁºñÁ†ÅÂô®ËæìÂá∫Áªü‰∏Ä128Áª¥
- ÁîüÁâ©Â≠¶Â§öÂ∞∫Â∫¶ÔºöGlobal(1) ‚Üí Pathway(8) ‚Üí Module(32) ‚Üí Individual(200)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Tuple, Dict, List


class SharedVectorQuantizer(nn.Module):
    """
    ÂÖ±‰∫´ÂêëÈáèÈáèÂåñÂô® - ÊîπËøõÁâàÔºåËß£ÂÜ≥Codebook CollapseÈóÆÈ¢ò
    
    üîß ÂÖ≥ÈîÆÊîπËøõÔºö
    - Êõ¥Â§ßÁöÑÂàùÂßãÂåñËåÉÂõ¥ÔºåÈÅøÂÖçcodebookÂêëÈáèËøá‰∫éÁõ∏‰ºº
    - Â¢ûÂ§ßcommitment lossÊùÉÈáçÔºåÂº∫ÂåñÁºñÁ†ÅÂô®Â≠¶‰π†
    - ÊîØÊåÅEMAÊõ¥Êñ∞ÔºåÊèêÈ´òËÆ≠ÁªÉÁ®≥ÂÆöÊÄß
    - Ê∑ªÂä†codebookÂà©Áî®ÁéáÁõëÊéß
    """
    
    def __init__(
        self,
        vocab_size: int = 4096,
        embed_dim: int = 128,
        beta: float = 0.5,  # üîß Â¢ûÂ§ßcommitment lossÊùÉÈáç
        use_ema: bool = True,  # üÜï ÂêØÁî®EMAÊõ¥Êñ∞
        ema_decay: float = 0.99,
        epsilon: float = 1e-5
    ):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.beta = beta
        self.use_ema = use_ema
        self.ema_decay = ema_decay
        self.epsilon = epsilon
        
        # üîß ÊîπËøõÁöÑcodebookÂàùÂßãÂåñ
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        # ‰ΩøÁî®Êõ¥Â§ßÁöÑÂàùÂßãÂåñËåÉÂõ¥ÔºåÁ°Æ‰øùcodebookÂêëÈáèÂ§öÊ†∑ÊÄß
        std = 0.02  # ÊØîÂéüÊù•ÁöÑ1/vocab_size=0.0002Â§ß100ÂÄç
        nn.init.normal_(self.embedding.weight, mean=0, std=std)
        
        # üÜï EMAÁªüËÆ°
        if use_ema:
            self.register_buffer('cluster_size', torch.zeros(vocab_size))
            self.register_buffer('embed_avg', self.embedding.weight.data.clone())
        
        # üÜï Âà©Áî®ÁéáÁªüËÆ°
        self.register_buffer('usage_count', torch.zeros(vocab_size))
        
        # Initialization complete
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """ÂâçÂêë‰º†Êí≠ - ÊîπËøõÁâà"""
        input_shape = x.shape
        
        # Â§ÑÁêÜ‰∏çÂêåËæìÂÖ•ÂΩ¢Áä∂
        if x.dim() == 2:
            x = x.unsqueeze(1)
            squeeze_output = True
        elif x.dim() == 3:
            squeeze_output = False
        else:
            raise ValueError(f"‰∏çÊîØÊåÅÁöÑËæìÂÖ•Áª¥Â∫¶: {x.shape}")
        
        B, N, D = x.shape
        assert D == self.embed_dim, f"ËæìÂÖ•Áª¥Â∫¶ {D} ‰∏çÂåπÈÖçÂµåÂÖ•Áª¥Â∫¶ {self.embed_dim}"
        
        # ËÆ°ÁÆóË∑ùÁ¶ªÂπ∂Ëé∑ÂèñÊúÄËøëÁöÑcodebook entry
        flat_x = x.view(-1, D)  # [B*N, D]
        
        # ËÆ°ÁÆóÊ¨ßÂá†ÈáåÂæóË∑ùÁ¶ª
        distances = torch.cdist(flat_x, self.embedding.weight)  # [B*N, vocab_size]
        tokens_flat = torch.argmin(distances, dim=1)  # [B*N]
        tokens = tokens_flat.view(B, N)  # [B, N]
        
        # Ëé∑ÂèñÈáèÂåñÁâπÂæÅ
        quantized = self.embedding(tokens)  # [B, N, embed_dim]
        
        # üÜï Êõ¥Êñ∞‰ΩøÁî®ÁªüËÆ°
        with torch.no_grad():
            token_counts = torch.bincount(tokens_flat, minlength=self.vocab_size).float()
            self.usage_count.add_(token_counts)
        
        # üÜï EMAÊõ¥Êñ∞ÔºàÂè™Âú®ËÆ≠ÁªÉÊó∂Ôºâ
        if self.training and self.use_ema:
            self._ema_update(flat_x, tokens_flat)
        
        # üîß ÊîπËøõÁöÑVQÊçüÂ§± - Êõ¥È´òÁöÑcommitment weight
        commitment_loss = F.mse_loss(quantized.detach(), x)
        embedding_loss = F.mse_loss(quantized, x.detach())
        vq_loss = embedding_loss + self.beta * commitment_loss
        
        # Áõ¥ÈÄö‰º∞ËÆ°Âô®
        quantized = x + (quantized - x).detach()
        
        # Ê†πÊçÆËæìÂÖ•ÂΩ¢Áä∂Ë∞ÉÊï¥ËæìÂá∫
        if squeeze_output:
            tokens = tokens.squeeze(1)  # [B, 1] ‚Üí [B]
            quantized = quantized.squeeze(1)  # [B, 1, embed_dim] ‚Üí [B, embed_dim]
        
        return tokens, quantized, vq_loss
    
    def _ema_update(self, flat_x: torch.Tensor, tokens_flat: torch.Tensor):
        """üÜï EMAÊõ¥Êñ∞codebook - ‰øÆÂ§çÂÜÖÂ≠òÊ≥ÑÊºè"""
        with torch.no_grad():  # üîß Á°Æ‰øùÊï¥‰∏™EMAÊõ¥Êñ∞ËøáÁ®ã‰∏ç‰øùÁïôËÆ°ÁÆóÂõæ
            # ËÆ°ÁÆóÊØè‰∏™tokenÁöÑ‰ΩøÁî®Ê¨°Êï∞
            token_counts = torch.bincount(tokens_flat, minlength=self.vocab_size).float()
            
            # Êõ¥Êñ∞cluster size
            self.cluster_size.mul_(self.ema_decay).add_(token_counts, alpha=1 - self.ema_decay)
            
            # ËÆ°ÁÆóÊØè‰∏™tokenÂØπÂ∫îÁöÑÁâπÂæÅÂπ≥ÂùáÂÄº
            embed_sum = torch.zeros_like(self.embed_avg)
            # üîß ÂÖ≥ÈîÆ‰øÆÂ§çÔºöÁ°Æ‰øùflat_x‰∏ç‰øùÁïôËÆ°ÁÆóÂõæ
            flat_x_detached = flat_x.detach()
            embed_sum.index_add_(0, tokens_flat, flat_x_detached)
            
            # Êõ¥Êñ∞embedding average
            self.embed_avg.mul_(self.ema_decay).add_(embed_sum, alpha=1 - self.ema_decay)
            
            # Êõ¥Êñ∞embeddingÊùÉÈáç
            cluster_size = self.cluster_size + self.epsilon
            embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)
            self.embedding.weight.data.copy_(embed_normalized)
    
    def get_codebook_utilization(self) -> float:
        """üÜï Ëé∑ÂèñcodebookÂà©Áî®Áéá"""
        used_codes = (self.usage_count > 0).sum().item()
        return used_codes / self.vocab_size
    
    def decode(self, tokens: torch.Tensor) -> torch.Tensor:
        """‰ªétokensËß£Á†Å‰∏∫ÁâπÂæÅ"""
        return self.embedding(tokens)


# ÂÖ∂‰ªñÁªÑ‰ª∂‰øùÊåÅ‰∏çÂèòÔºåÂè™Ê∑ªÂä†ÊîπËøõÁöÑÁºñÁ†ÅÂô®ÔºàÂèØÈÄâÔºâ
class GlobalEncoder(nn.Module):
    """GlobalÂ±ÇÁºñÁ†ÅÂô®: [B, 1] ‚Üí [B, 1, 128]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        self.embed_dim = embed_dim
        
        # üîß ÂèØÈÄâÊîπËøõÔºöÊ∑ªÂä†LayerNorm
        self.encoder = nn.Sequential(
            nn.Linear(1, 64),
            nn.LayerNorm(64),  # üÜï Á®≥ÂÆöËÆ≠ÁªÉ
            nn.ReLU(),
            nn.Linear(64, embed_dim)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B = x.shape[0]
        encoded = self.encoder(x)  # [B, 1] ‚Üí [B, 128]
        return encoded.view(B, 1, self.embed_dim)


class PathwayEncoder(nn.Module):
    """PathwayÂ±ÇÁºñÁ†ÅÂô®: [B, 8] ‚Üí [B, 8, 128]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        self.embed_dim = embed_dim
        
        self.encoder = nn.Sequential(
            nn.Linear(1, 64),
            nn.LayerNorm(64),  # üÜï Á®≥ÂÆöËÆ≠ÁªÉ
            nn.ReLU(),
            nn.Linear(64, embed_dim)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N = x.shape
        x_expanded = x.unsqueeze(-1)  # [B, 8, 1]
        encoded = self.encoder(x_expanded)  # [B, 8, 128]
        return encoded


class ModuleEncoder(nn.Module):
    """ModuleÂ±ÇÁºñÁ†ÅÂô®: [B, 32] ‚Üí [B, 32, 128]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        self.embed_dim = embed_dim
        
        self.encoder = nn.Sequential(
            nn.Linear(1, 96),
            nn.LayerNorm(96),  # üÜï Á®≥ÂÆöËÆ≠ÁªÉ
            nn.ReLU(),
            nn.Linear(96, embed_dim)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N = x.shape
        x_expanded = x.unsqueeze(-1)  # [B, 32, 1]
        encoded = self.encoder(x_expanded)  # [B, 32, 128]
        return encoded


class IndividualEncoder(nn.Module):
    """IndividualÂ±ÇÁºñÁ†ÅÂô®: [B, 200] ‚Üí [B, 200, 128]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        self.embed_dim = embed_dim
        
        self.encoder = nn.Sequential(
            nn.Linear(1, 256),
            nn.LayerNorm(256),  # üÜï Á®≥ÂÆöËÆ≠ÁªÉ
            nn.ReLU(),
            nn.Linear(256, embed_dim)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N = x.shape
        x_expanded = x.unsqueeze(-1)  # [B, 200, 1]
        encoded = self.encoder(x_expanded)  # [B, 200, 128]
        return encoded


# Ëß£Á†ÅÂô®‰øùÊåÅ‰∏çÂèò...
class GlobalDecoder(nn.Module):
    """GlobalÂ±ÇËß£Á†ÅÂô®: [B, 1, 128] ‚Üí [B, 1]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        
        self.decoder = nn.Sequential(
            nn.Linear(embed_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B = x.shape[0]
        x_flat = x.view(B, -1)  # [B, 128]
        decoded = self.decoder(x_flat)  # [B, 1]
        return decoded


class PathwayDecoder(nn.Module):
    """PathwayÂ±ÇËß£Á†ÅÂô®: [B, 8, 128] ‚Üí [B, 8]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        
        self.decoder = nn.Sequential(
            nn.Linear(embed_dim, 64),
            nn.ReLU(), 
            nn.Linear(64, 1)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N, D = x.shape
        decoded = self.decoder(x)  # [B, 8, 1]
        return decoded.squeeze(-1)  # [B, 8]


class ModuleDecoder(nn.Module):
    """ModuleÂ±ÇËß£Á†ÅÂô®: [B, 32, 128] ‚Üí [B, 32]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        
        self.decoder = nn.Sequential(
            nn.Linear(embed_dim, 96),
            nn.ReLU(),
            nn.Linear(96, 1)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N, D = x.shape
        decoded = self.decoder(x)  # [B, 32, 1]
        return decoded.squeeze(-1)  # [B, 32]


class IndividualDecoder(nn.Module):
    """IndividualÂ±ÇËß£Á†ÅÂô®: [B, 200, 128] ‚Üí [B, 200]"""
    
    def __init__(self, embed_dim: int = 128):
        super().__init__()
        
        self.decoder = nn.Sequential(
            nn.Linear(embed_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N, D = x.shape
        decoded = self.decoder(x)  # [B, 200, 1]
        return decoded.squeeze(-1)  # [B, 200]


class ResidualReconstructor(nn.Module):
    """ÊÆãÂ∑ÆÈáçÂª∫Âô® - ‰∏çÂèò"""
    
    def __init__(self):
        super().__init__()
        
    def forward(
        self, 
        global_recon: torch.Tensor,
        pathway_recon: torch.Tensor,
        module_recon: torch.Tensor,
        individual_recon: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        B = global_recon.shape[0]
        
        # 1. GlobalÂ±ÇÂπøÊí≠
        global_broadcast = global_recon.expand(B, 200)  # [B, 1] ‚Üí [B, 200]
        
        # 2. PathwayÂ±ÇÂπøÊí≠
        pathway_broadcast = pathway_recon.repeat_interleave(25, dim=1)  # [B, 8] ‚Üí [B, 200]
        
        # 3. ModuleÂ±ÇÂπøÊí≠
        module_expanded = F.interpolate(
            module_recon.unsqueeze(1),  # [B, 32] ‚Üí [B, 1, 32]
            size=200,
            mode='linear',
            align_corners=False
        ).squeeze(1)  # [B, 1, 200] ‚Üí [B, 200]
        
        # 4. ÊÆãÂ∑ÆÁ¥ØÁßØÈáçÂª∫
        cumulative_recon = global_broadcast.clone()
        cumulative_recon = cumulative_recon + pathway_broadcast
        cumulative_recon = cumulative_recon + module_expanded
        final_recon = cumulative_recon + individual_recon
        
        return {
            'global_broadcast': global_broadcast,
            'pathway_broadcast': pathway_broadcast, 
            'module_broadcast': module_expanded,
            'individual_contribution': individual_recon,
            'cumulative_without_individual': cumulative_recon,
            'final_reconstruction': final_recon
        }


class MultiScaleDecomposer(nn.Module):
    """Â§öÂ∞∫Â∫¶ÂàÜËß£Âô® - ‰∏çÂèò"""
    
    def __init__(self):
        super().__init__()
        
    def forward(self, gene_expression: torch.Tensor) -> Dict[str, torch.Tensor]:
        B, num_genes = gene_expression.shape
        assert num_genes == 200, f"ÊúüÊúõ200‰∏™Âü∫Âõ†ÔºåÂæóÂà∞{num_genes}"
        
        # GlobalÂ±ÇÔºöÊï¥‰ΩìÂπ≥Âùá
        global_features = gene_expression.mean(dim=1, keepdim=True)  # [B, 1]
        
        # PathwayÂ±ÇÔºö8‰∏™ÁîüÁâ©Â≠¶ÈÄöË∑Ø
        pathway_features = F.adaptive_avg_pool1d(
            gene_expression.unsqueeze(1), 8
        ).squeeze(1)  # [B, 8]
        
        # ModuleÂ±ÇÔºö32‰∏™ÂäüËÉΩÊ®°Âùó
        module_features = F.adaptive_avg_pool1d(
            gene_expression.unsqueeze(1), 32
        ).squeeze(1)  # [B, 32]
        
        # IndividualÂ±ÇÔºö‰øùÊåÅÂéüÂßãÂàÜËæ®Áéá
        individual_features = gene_expression.clone()  # [B, 200]
        
        return {
            'global': global_features,
            'pathway': pathway_features,
            'module': module_features,
            'individual': individual_features
        } 
