"""
VAR Transformer for Spatial Transcriptomics

This module implements the autoregressive transformer component of VAR,
adapted for spatial transcriptomics gene expression prediction.

Key Features:
1. Multi-scale autoregressive generation
2. Adaptive layer normalization with conditioning
3. Next-scale prediction for spatial gene expression
4. Complete preservation of VAR transformer architecture

Author: VAR-ST Team
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from functools import partial
from typing import Optional, Tuple, Union, List, Dict, Any

from .var_basic_components import (
    AdaLNSelfAttn, AdaLNBeforeHead, SharedAdaLin, 
    sample_with_top_k_top_p_, gumbel_softmax_with_rng
)


class VAR_ST(nn.Module):
    """
    Vector-quantized Autoregressive transformer for Spatial Transcriptomics
    
    This implements the core VAR transformer that performs autoregressive
    generation of spatial gene expression patterns at multiple scales.
    
    Architecture:
    1. Input: discrete tokens from VQVAE + conditioning (histology features)
    2. Multi-scale autoregressive modeling with adaptive layer norm
    3. Output: next-scale gene expression token predictions
    
    Completely preserves the original VAR transformer design for zero performance loss.
    """
    
    def __init__(
        self,
        vae_embed_dim: int = 256,          # VQVAE embedding dimension
        num_classes: int = 1000,           # Number of condition classes (tissue types, etc.)
        depth: int = 16,                   # Number of transformer blocks
        embed_dim: int = 1024,             # Transformer embedding dimension
        num_heads: int = 16,               # Number of attention heads
        mlp_ratio: float = 4.0,            # MLP expansion ratio
        drop_rate: float = 0.0,            # Dropout rate
        attn_drop_rate: float = 0.0,       # Attention dropout rate
        drop_path_rate: float = 0.0,       # Stochastic depth rate
        norm_eps: float = 1e-6,            # Layer norm epsilon
        shared_aln: bool = False,          # Share adaptive layer norm parameters
        cond_drop_rate: float = 0.1,       # Conditioning dropout rate
        attn_l2_norm: bool = False,        # L2 normalize attention
        patch_nums: Tuple[int, ...] = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16),  # Multi-scale patch numbers
        flash_if_available: bool = True,   # Use flash attention if available
        fused_if_available: bool = True,   # Use fused operations if available
    ):
        """
        Initialize VAR transformer for spatial transcriptomics
        
        Args:
            vae_embed_dim: Dimension of VQVAE embeddings
            num_classes: Number of conditioning classes
            depth: Number of transformer layers
            embed_dim: Transformer hidden dimension
            num_heads: Number of attention heads per layer
            mlp_ratio: Expansion ratio for MLP layers
            drop_rate: General dropout probability
            attn_drop_rate: Attention dropout probability
            drop_path_rate: Stochastic depth probability
            norm_eps: LayerNorm epsilon value
            shared_aln: Whether to share adaptive layer norm parameters
            cond_drop_rate: Conditioning dropout rate for robust training
            attn_l2_norm: Whether to L2 normalize attention weights
            patch_nums: Sequence of patch numbers for multi-scale generation
            flash_if_available: Use flash attention optimization if available
            fused_if_available: Use fused MLP optimization if available
        """
        super().__init__()
        
        # Store architecture parameters
        self.depth = depth
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.num_classes = num_classes
        self.patch_nums = patch_nums
        self.cond_drop_rate = cond_drop_rate
        
        # Multi-scale generation setup
        # Each scale corresponds to a different spatial resolution of gene expression
        self.first_l = patch_nums[0]  # Starting scale
        self.last_l = patch_nums[-1]  # Final scale (highest resolution)
        
        print(f"ğŸš€ åˆå§‹åŒ– VAR-ST Transformer:")
        print(f"  - å±‚æ•°: {depth}")
        print(f"  - åµŒå…¥ç»´åº¦: {embed_dim}")
        print(f"  - æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
        print(f"  - å¤šå°ºåº¦åºåˆ—: {patch_nums}")
        print(f"  - æ¡ä»¶ç±»åˆ«æ•°: {num_classes}")
        
        # Layer normalization
        norm_layer = partial(nn.LayerNorm, eps=norm_eps)
        
        # Class embedding for conditioning (e.g., tissue type, sample metadata)
        self.class_emb = nn.Embedding(num_classes, embed_dim)
        self.pos_start = nn.Parameter(torch.empty(embed_dim))
        
        # Token embeddings for each scale
        # Maps discrete tokens from VQVAE to transformer input embeddings
        self.token_embed = nn.ModuleList([
            nn.Embedding(vae_embed_dim, embed_dim) for _ in patch_nums
        ])
        
        # Positional embeddings for each scale
        # Provides spatial awareness for the transformer at different resolutions
        self.pos_embed = nn.ParameterList([
            nn.Parameter(torch.empty(1, pn * pn, embed_dim)) for pn in patch_nums
        ])
        
        # Adaptive layer norm conditioning
        # This is key to VAR's performance - allows dynamic adaptation based on conditioning
        if shared_aln:
            self.shared_ada_lin = SharedAdaLin(embed_dim, 6 * embed_dim)
        else:
            self.shared_ada_lin = nn.Identity()
        
        # Transformer blocks with adaptive layer normalization
        # Each block can adapt its processing based on the conditioning signal
        drop_path_rates = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]
        self.blocks = nn.ModuleList([
            AdaLNSelfAttn(
                block_idx=block_idx,
                last_drop_p=drop_path_rates[-1],
                embed_dim=embed_dim,
                cond_dim=embed_dim,
                shared_aln=shared_aln,
                norm_layer=norm_layer,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
                drop=drop_rate,
                attn_drop=attn_drop_rate,
                drop_path=drop_path_rates[block_idx],
                attn_l2_norm=attn_l2_norm,
                flash_if_available=flash_if_available,
                fused_if_available=fused_if_available,
            )
            for block_idx in range(depth)
        ])
        
        # Output heads for each scale
        # Predicts the next-scale tokens in the autoregressive sequence
        self.head_nm = AdaLNBeforeHead(embed_dim, embed_dim, norm_layer)
        self.head = nn.ModuleList([
            nn.Linear(embed_dim, vae_embed_dim, bias=False) for _ in patch_nums
        ])
        
        # Initialize parameters
        self.init_weights()
        
        # For inference: cached representations and attention states
        self.prog_si = -1  # Current progress in generation sequence
    
    def init_weights(self):
        """Initialize model parameters following VAR's initialization scheme"""
        # Initialize positional embeddings
        for pos_emb in self.pos_embed:
            nn.init.trunc_normal_(pos_emb, std=0.02)
        nn.init.trunc_normal_(self.pos_start, std=0.02)
        
        # Initialize class embedding
        nn.init.trunc_normal_(self.class_emb.weight, std=0.02)
        
        # Initialize token embeddings
        for token_emb in self.token_embed:
            nn.init.trunc_normal_(token_emb.weight, std=0.02)
        
        # Initialize output heads
        for head in self.head:
            nn.init.trunc_normal_(head.weight, std=0.02)
    
    def get_conditioning(self, B: int, class_labels: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Generate conditioning signal for adaptive layer normalization
        
        This conditioning allows the transformer to adapt its processing
        based on sample-specific information (e.g., tissue type, patient metadata).
        
        Args:
            B: Batch size
            class_labels: [B] - class labels for conditioning (optional)
        
        Returns:
            conditioning: [B, embed_dim] - conditioning vector
        """
        if class_labels is None:
            # Default conditioning (e.g., for unconditional generation)
            class_labels = torch.zeros(B, dtype=torch.long, device=self.class_emb.weight.device)
        
        # Apply conditioning dropout during training for robustness
        if self.training and self.cond_drop_rate > 0:
            mask = torch.rand(B, device=class_labels.device) >= self.cond_drop_rate
            class_labels = class_labels * mask.long()
        
        # Get class embeddings
        conditioning = self.class_emb(class_labels)  # [B, embed_dim]
        
        return conditioning
    
    def forward_for_loss(
        self, 
        gt_indices: List[torch.Tensor],
        class_labels: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Forward pass for training loss computation
        
        Implements teacher-forcing training where the model learns to predict
        the next scale given all previous scales.
        
        Args:
            gt_indices: List of ground truth token indices for each scale
                       Each element: [B, Hi, Wi] where Hi, Wi are spatial dims for scale i
            class_labels: [B] - conditioning class labels
        
        Returns:
            loss: scalar - autoregressive prediction loss
        """
        B = gt_indices[0].shape[0]
        
        # Get conditioning signal
        conditioning = self.get_conditioning(B, class_labels)  # [B, embed_dim]
        
        # Apply shared adaptive linear layer if enabled
        if hasattr(self.shared_ada_lin, 'weight'):
            conditioning = self.shared_ada_lin(conditioning)
        
        # Initialize sequence with start token
        sos = self.pos_start.unsqueeze(0).expand(B, 1, -1)  # [B, 1, embed_dim]
        x = sos
        
        total_loss = 0.0
        losses_per_scale = []
        
        # Process each scale in the multi-scale sequence
        for si, pn in enumerate(self.patch_nums[:-1]):  # Exclude last scale
            # Current scale tokens
            cur_indices = gt_indices[si].view(B, -1).contiguous()  # [B, pn*pn]
            cur_tokens = self.token_embed[si](cur_indices)  # [B, pn*pn, embed_dim]
            cur_pos = self.pos_embed[si].expand(B, -1, -1).contiguous()  # [B, pn*pn, embed_dim]
            cur_tokens = cur_tokens + cur_pos
            cur_tokens = cur_tokens.contiguous()
            
            # Concatenate with previous sequence
            x = torch.cat([x, cur_tokens], dim=1).contiguous()  # [B, seq_len, embed_dim]
            
            # Apply transformer blocks with adaptive conditioning
            for block in self.blocks:
                x = block(x, conditioning, attn_bias=None)
            
            # Predict next scale
            next_pn = self.patch_nums[si + 1]
            next_len = next_pn * next_pn
            
            # Extract representations for next scale prediction
            pred_repr = x[:, -next_len:].contiguous()  # [B, next_len, embed_dim]
            pred_repr = self.head_nm(pred_repr, conditioning)
            logits = self.head[si + 1](pred_repr)  # [B, next_len, vocab_size]
            logits = logits.contiguous()
            
            # Compute cross-entropy loss with ground truth
            gt_next = gt_indices[si + 1].view(B, -1).contiguous()  # [B, next_len]
            scale_loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), gt_next.reshape(-1))
            losses_per_scale.append(scale_loss)
            total_loss += scale_loss
        
        # Average loss
        if len(losses_per_scale) > 0:
            avg_loss = total_loss / len(losses_per_scale)
        else:
            avg_loss = total_loss
        
        return avg_loss
    
    def autoregressive_infer_cfg(
        self,
        B: int,
        class_labels: Optional[torch.Tensor] = None,
        cfg: float = 1.0,
        top_k: int = 0,
        top_p: float = 0.0,
        more_smooth: bool = False,
        rng: Optional[torch.Generator] = None,
    ) -> List[torch.Tensor]:
        """
        Autoregressive inference with classifier-free guidance
        
        Generates spatial gene expression patterns by predicting tokens
        at progressively higher resolutions.
        
        Args:
            B: Batch size
            class_labels: [B] - conditioning class labels
            cfg: Classifier-free guidance scale
            top_k: Top-k sampling parameter
            top_p: Top-p (nucleus) sampling parameter
            more_smooth: Whether to use temperature scaling for smoother generation
            rng: Random number generator for reproducible sampling
        
        Returns:
            generated_indices: List of generated token indices for each scale
        """
        # Setup conditioning for classifier-free guidance
        if cfg != 1.0:
            # Duplicate batch: conditional + unconditional
            if class_labels is not None:
                class_labels_cfg = torch.cat([class_labels, torch.zeros_like(class_labels)], dim=0)
            else:
                class_labels_cfg = torch.cat([
                    torch.zeros(B, dtype=torch.long, device=self.class_emb.weight.device),
                    torch.zeros(B, dtype=torch.long, device=self.class_emb.weight.device)
                ], dim=0)
            B_cfg = B * 2
        else:
            class_labels_cfg = class_labels
            B_cfg = B
        
        # Get conditioning
        conditioning = self.get_conditioning(B_cfg, class_labels_cfg)
        if hasattr(self.shared_ada_lin, 'weight'):
            conditioning = self.shared_ada_lin(conditioning)
        
        # Initialize sequence
        sos = self.pos_start.unsqueeze(0).expand(B_cfg, 1, -1)
        x = sos
        
        generated_indices = []
        
        # Generate each scale autoregressively
        for si, pn in enumerate(self.patch_nums):
            cur_len = pn * pn
            
            if si == 0:
                # First scale: predict from start token
                pred_repr = x
            else:
                # Subsequent scales: use accumulated sequence
                pred_repr = x[:, -cur_len:]
            
            # Apply transformer
            for block in self.blocks:
                x_input = x if si == 0 else torch.cat([x[:, :-cur_len], pred_repr], dim=1)
                pred_repr = block(x_input, conditioning, attn_bias=None)[:, -cur_len:]
            
            # Generate tokens for current scale
            pred_repr = self.head_nm(pred_repr, conditioning)
            logits = self.head[si](pred_repr)  # [B_cfg, cur_len, vocab_size]
            
            # Apply classifier-free guidance
            if cfg != 1.0:
                logits_cond, logits_uncond = logits.chunk(2, dim=0)
                logits = logits_uncond + cfg * (logits_cond - logits_uncond)
            
            # Temperature scaling for smoother generation
            if more_smooth:
                logits = logits / 1.5
            
            # Sample tokens
            if top_k > 0 or top_p > 0:
                indices = sample_with_top_k_top_p_(logits, top_k=top_k, top_p=top_p, rng=rng)
                indices = indices.squeeze(-1).contiguous()  # [B, cur_len]
            else:
                logits_softmax = F.softmax(logits, dim=-1).contiguous()
                indices = torch.multinomial(logits_softmax.reshape(-1, logits.size(-1)), 
                                          num_samples=1, generator=rng).reshape(logits.shape[0], -1).contiguous()
            
            generated_indices.append(indices.reshape(B, pn, pn).contiguous())
            
            # Add generated tokens to sequence for next scale
            if si < len(self.patch_nums) - 1:
                gen_tokens = self.token_embed[si](indices)  # [B, cur_len, embed_dim]
                gen_pos = self.pos_embed[si].expand(B, -1, -1).contiguous()
                gen_tokens = (gen_tokens + gen_pos).contiguous()
                x = torch.cat([x, gen_tokens], dim=1).contiguous()
        
        return generated_indices
    
    def forward(self, indices: List[torch.Tensor], class_labels: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Standard forward pass (delegates to forward_for_loss for training)"""
        return self.forward_for_loss(indices, class_labels)


class VAR_ST_Gene(VAR_ST):
    """
    åŸºå› ç»´åº¦å¤šå°ºåº¦çš„VAR_ST - ä¸“é—¨ç”¨äºåŸºå› è¡¨è¾¾ç”Ÿæˆ
    
    ç»§æ‰¿å®Œæ•´VAR_STçš„æ‰€æœ‰é«˜çº§ç‰¹æ€§:
    - AdaLNè‡ªé€‚åº”å±‚å½’ä¸€åŒ–
    - Classifier-Free Guidance (CFG)
    - æ¡ä»¶ç”Ÿæˆæ§åˆ¶
    - é«˜çº§é‡‡æ ·ç­–ç•¥
    
    é’ˆå¯¹åŸºå› ç»´åº¦å¤šå°ºåº¦çš„å…³é”®ä¿®æ”¹:
    - patch_numså¯¹åº”åŸºå› ç‰¹å¾æ•°é‡ [1, 4, 16, 64, 200]
    - ä½ç½®ç¼–ç é€‚é…åŸºå› ç»´åº¦ (ä¸æ˜¯ç©ºé—´pn*pn)
    - Tokenåºåˆ—å¤„ç†é€‚é…å•spotåŸºå› å‘é‡
    """
    
    def __init__(
        self,
        gene_scales: List[int] = [1, 4, 16, 64, 200],
        vae_embed_dim: int = 8192,          # VQVAEæœ€å¤§codebook size
        num_classes: int = 1000,            # æ¡ä»¶ç±»åˆ«æ•°
        depth: int = 16,                    # Transformerå±‚æ•°
        embed_dim: int = 1024,              # åµŒå…¥ç»´åº¦
        num_heads: int = 16,                # æ³¨æ„åŠ›å¤´æ•°
        **kwargs
    ):
        """
        åˆå§‹åŒ–åŸºå› ç»´åº¦VAR_ST
        
        Args:
            gene_scales: åŸºå› å¤šå°ºåº¦ç‰¹å¾æ•°é‡ [1, 4, 16, 64, 200]
            vae_embed_dim: VQVAEè¯æ±‡è¡¨å¤§å°
            å…¶ä»–å‚æ•°ä¸åŸå§‹VAR_STç›¸åŒ
        """
        # è®¾ç½®åŸºå› å°ºåº¦ä¸ºpatch_nums
        kwargs['patch_nums'] = tuple(gene_scales)
        kwargs['vae_embed_dim'] = vae_embed_dim
        kwargs['num_classes'] = num_classes
        kwargs['depth'] = depth
        kwargs['embed_dim'] = embed_dim
        kwargs['num_heads'] = num_heads
        
        print(f"ğŸ§¬ åˆå§‹åŒ–åŸºå› ç»´åº¦VAR_ST:")
        print(f"  - åŸºå› å°ºåº¦: {gene_scales}")
        print(f"  - è¯æ±‡è¡¨å¤§å°: {vae_embed_dim}")
        print(f"  - æ¡ä»¶ç±»åˆ«æ•°: {num_classes}")
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ– (ä½†ä¼šè¢«ä¸‹é¢çš„ä¿®æ”¹è¦†ç›–)
        super().__init__(**kwargs)
        
        # ğŸ”§ å…³é”®ä¿®æ”¹: é‡å†™ä½ç½®ç¼–ç ä»¥é€‚é…åŸºå› ç»´åº¦
        # åŸå§‹VAR_ST: pos_embedä¸º [1, pn*pn, embed_dim] (ç©ºé—´ç»´åº¦)
        # åŸºå› VAR_ST: pos_embedä¸º [1, 1, embed_dim] (åŸºå› ç»´åº¦ï¼Œæ¯ä¸ªå°ºåº¦1ä¸ªtoken)
        self.pos_embed = nn.ParameterList([
            nn.Parameter(torch.empty(1, 1, embed_dim))  # æ¯ä¸ªå°ºåº¦åªæœ‰1ä¸ªtoken
            for _ in gene_scales
        ])
        
        # é‡æ–°åˆå§‹åŒ–ä½ç½®ç¼–ç 
        for pos_emb in self.pos_embed:
            nn.init.trunc_normal_(pos_emb, std=0.02)
        
        print(f"  - ä½ç½®ç¼–ç é€‚é…: åŸºå› ç»´åº¦ (æ¯ä¸ªå°ºåº¦1ä¸ªtoken)")
        print(f"  - ä½ç½®ç¼–ç å½¢çŠ¶: {[tuple(pos.shape) for pos in self.pos_embed]}")
        
        # å­˜å‚¨åŸºå› å°ºåº¦ä¿¡æ¯
        self.gene_scales = gene_scales
        self.num_gene_scales = len(gene_scales)
        
        print(f"âœ… åŸºå› ç»´åº¦VAR_STåˆå§‹åŒ–å®Œæˆ")
    
    def forward_for_loss(
        self, 
        gt_tokens: List[torch.Tensor],  # ä¿®æ”¹: åŸºå› tokensæ ¼å¼
        class_labels: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        åŸºå› ç»´åº¦çš„è®­ç»ƒå‰å‘ä¼ æ’­
        
        Args:
            gt_tokens: List of gene tokens for each scale
                      æ¯ä¸ªå…ƒç´ : [B, 1] - æ¯ä¸ªæ ·æœ¬æ¯ä¸ªå°ºåº¦1ä¸ªtoken
            class_labels: [B] - æ¡ä»¶ç±»åˆ«æ ‡ç­¾
        
        Returns:
            loss: è‡ªå›å½’é¢„æµ‹æŸå¤±
        """
        B = gt_tokens[0].shape[0]
        
        # è·å–æ¡ä»¶ä¿¡å·
        conditioning = self.get_conditioning(B, class_labels)
        
        # åº”ç”¨å…±äº«è‡ªé€‚åº”çº¿æ€§å±‚
        if hasattr(self.shared_ada_lin, 'weight'):
            conditioning = self.shared_ada_lin(conditioning)
        
        # åˆå§‹åŒ–åºåˆ—
        sos = self.pos_start.unsqueeze(0).expand(B, 1, -1)  # [B, 1, embed_dim]
        x = sos
        
        total_loss = 0.0
        losses_per_scale = []
        
        # ğŸ”§ ä¿®æ”¹: é€‚é…åŸºå› ç»´åº¦çš„tokenå¤„ç†
        for si, gene_scale in enumerate(self.gene_scales[:-1]):  # æ’é™¤æœ€åä¸€ä¸ªå°ºåº¦
            # å½“å‰å°ºåº¦çš„tokens: [B, 1] -> [B]
            cur_tokens = gt_tokens[si].squeeze(-1) if gt_tokens[si].dim() == 2 else gt_tokens[si]
            cur_tokens = cur_tokens.contiguous()  # [B]
            
            # TokenåµŒå…¥: [B] -> [B, 1, embed_dim]
            cur_token_emb = self.token_embed[si](cur_tokens).unsqueeze(1)
            
            # ä½ç½®åµŒå…¥: [1, 1, embed_dim] -> [B, 1, embed_dim]
            cur_pos_emb = self.pos_embed[si].expand(B, -1, -1)
            
            # ç»„åˆåµŒå…¥
            cur_repr = cur_token_emb + cur_pos_emb  # [B, 1, embed_dim]
            
            # æ·»åŠ åˆ°åºåˆ—
            x = torch.cat([x, cur_repr], dim=1).contiguous()  # [B, seq_len, embed_dim]
            
            # Transformerå¤„ç†
            for block in self.blocks:
                x = block(x, conditioning, attn_bias=None)
            
            # é¢„æµ‹ä¸‹ä¸€ä¸ªå°ºåº¦
            next_scale_idx = si + 1
            next_gene_scale = self.gene_scales[next_scale_idx]
            
            # ğŸ”§ ä¿®æ”¹: åŸºå› ç»´åº¦çš„é¢„æµ‹
            # ä½¿ç”¨æœ€åçš„è¡¨ç¤ºæ¥é¢„æµ‹ä¸‹ä¸€å°ºåº¦çš„å•ä¸ªtoken
            pred_repr = x[:, -1:].contiguous()  # [B, 1, embed_dim] - æœ€åä¸€ä¸ªä½ç½®
            pred_repr = self.head_nm(pred_repr, conditioning)
            logits = self.head[next_scale_idx](pred_repr)  # [B, 1, vocab_size]
            logits = logits.contiguous()
            
            # è®¡ç®—æŸå¤±
            gt_next = gt_tokens[next_scale_idx].squeeze(-1) if gt_tokens[next_scale_idx].dim() == 2 else gt_tokens[next_scale_idx]
            gt_next = gt_next.contiguous()  # [B]
            
            scale_loss = F.cross_entropy(logits.squeeze(1), gt_next)  # [B, vocab_size] vs [B]
            losses_per_scale.append(scale_loss)
            total_loss += scale_loss
        
        # å¹³å‡æŸå¤±
        avg_loss = total_loss / len(losses_per_scale) if losses_per_scale else total_loss
        
        return avg_loss
    
    def autoregressive_infer_cfg(
        self,
        B: int,
        class_labels: Optional[torch.Tensor] = None,
        cfg: float = 1.5,
        top_k: int = 50,
        top_p: float = 0.9,
        temperature: float = 1.0,
        more_smooth: bool = False,
        rng: Optional[torch.Generator] = None,
    ) -> List[torch.Tensor]:
        """
        åŸºå› ç»´åº¦çš„è‡ªå›å½’æ¨ç†ç”Ÿæˆ
        
        Args:
            B: batch size
            class_labels: [B] - æ¡ä»¶ç±»åˆ«æ ‡ç­¾
            cfg: Classifier-free guidanceç¼©æ”¾å› å­
            å…¶ä»–å‚æ•°: é‡‡æ ·æ§åˆ¶å‚æ•°
        
        Returns:
            List[torch.Tensor]: æ¯ä¸ªåŸºå› å°ºåº¦çš„ç”Ÿæˆtokens
                               æ¯ä¸ªå…ƒç´ : [B] - æ¯ä¸ªæ ·æœ¬æ¯ä¸ªå°ºåº¦1ä¸ªtoken
        """
        # CFGè®¾ç½®
        if cfg != 1.0:
            if class_labels is not None:
                class_labels_cfg = torch.cat([class_labels, torch.zeros_like(class_labels)], dim=0)
            else:
                device = next(self.parameters()).device
                class_labels_cfg = torch.cat([
                    torch.zeros(B, dtype=torch.long, device=device),
                    torch.zeros(B, dtype=torch.long, device=device)
                ], dim=0)
            B_cfg = B * 2
        else:
            class_labels_cfg = class_labels
            B_cfg = B
        
        # è·å–æ¡ä»¶
        conditioning = self.get_conditioning(B_cfg, class_labels_cfg)
        if hasattr(self.shared_ada_lin, 'weight'):
            conditioning = self.shared_ada_lin(conditioning)
        
        # åˆå§‹åŒ–åºåˆ—
        sos = self.pos_start.unsqueeze(0).expand(B_cfg, 1, -1)
        x = sos
        
        generated_tokens = []
        
        # ğŸ”§ ä¿®æ”¹: åŸºå› ç»´åº¦çš„è‡ªå›å½’ç”Ÿæˆ
        for si, gene_scale in enumerate(self.gene_scales):
            if si == 0:
                # ç¬¬ä¸€ä¸ªå°ºåº¦: ä»èµ·å§‹tokené¢„æµ‹
                pred_repr = x  # [B_cfg, 1, embed_dim]
            else:
                # åç»­å°ºåº¦: ä½¿ç”¨ç´¯ç§¯åºåˆ—çš„æœ€åä½ç½®
                pred_repr = x[:, -1:].contiguous()  # [B_cfg, 1, embed_dim]
            
            # Transformerå¤„ç†
            for block in self.blocks:
                if si == 0:
                    pred_repr = block(pred_repr, conditioning, attn_bias=None)
                else:
                    # å¯¹æ•´ä¸ªåºåˆ—å¤„ç†ï¼Œä½†åªå–æœ€åçš„è¡¨ç¤º
                    full_repr = block(x, conditioning, attn_bias=None)
                    pred_repr = full_repr[:, -1:].contiguous()
            
            # ç”Ÿæˆå½“å‰å°ºåº¦çš„token
            pred_repr = self.head_nm(pred_repr, conditioning)
            logits = self.head[si](pred_repr)  # [B_cfg, 1, vocab_size]
            
            # CFGåº”ç”¨
            if cfg != 1.0:
                logits_cond, logits_uncond = logits.chunk(2, dim=0)
                logits = logits_uncond + cfg * (logits_cond - logits_uncond)
                logits = logits[:B]  # åªä¿ç•™æ¡ä»¶éƒ¨åˆ†
            
            # æ¸©åº¦ç¼©æ”¾
            if more_smooth:
                logits = logits / 1.5
            if temperature != 1.0:
                logits = logits / temperature
            
            # é‡‡æ ·
            if top_k > 0 or top_p > 0:
                tokens = sample_with_top_k_top_p_(logits, top_k=top_k, top_p=top_p, rng=rng)
                tokens = tokens.squeeze(-1).contiguous()  # [B]
            else:
                probs = F.softmax(logits.squeeze(1), dim=-1)  # [B, vocab_size]
                tokens = torch.multinomial(probs, num_samples=1, generator=rng).squeeze(-1)  # [B]
            
            generated_tokens.append(tokens)
            
            # æ·»åŠ ç”Ÿæˆçš„tokenåˆ°åºåˆ— (é™¤äº†æœ€åä¸€ä¸ªå°ºåº¦)
            if si < len(self.gene_scales) - 1:
                # TokenåµŒå…¥
                gen_token_emb = self.token_embed[si](tokens).unsqueeze(1)  # [B, 1, embed_dim]
                
                # ä½ç½®åµŒå…¥  
                gen_pos_emb = self.pos_embed[si].expand(B, -1, -1)  # [B, 1, embed_dim]
                
                # ç»„åˆå¹¶æ·»åŠ åˆ°åºåˆ—
                gen_repr = (gen_token_emb + gen_pos_emb).contiguous()  # [B, 1, embed_dim]
                
                # ğŸ” è°ƒè¯•ä¿¡æ¯
                print(f"  Debug scale {si}: x.shape={x.shape}, gen_repr.shape={gen_repr.shape}, B={B}, cfg={cfg}")
                
                # ç¡®ä¿xçš„ç»´åº¦æ­£ç¡®ï¼šå¦‚æœä½¿ç”¨CFGï¼Œéœ€è¦ç‰¹åˆ«å¤„ç†
                if cfg != 1.0:
                    # CFGæ¨¡å¼ï¼šxæ˜¯[B_cfg, seq_len, embed_dim]ï¼Œéœ€è¦åªå–å‰Bä¸ª
                    x_cond = x[:B].contiguous()  # [B, seq_len, embed_dim]
                    print(f"  CFG mode: x_cond.shape={x_cond.shape}")
                    x_new = torch.cat([x_cond, gen_repr], dim=1).contiguous()  # [B, seq_len+1, embed_dim]
                    # å¤åˆ¶ç»™unconditionaléƒ¨åˆ†
                    x = torch.cat([x_new, x_new], dim=0).contiguous()  # [B_cfg, seq_len+1, embed_dim]
                else:
                    x = torch.cat([x, gen_repr], dim=1).contiguous()  # [B, seq_len+1, embed_dim]
        
        return generated_tokens 