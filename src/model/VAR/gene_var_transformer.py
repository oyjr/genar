"""
åŸºå› VAR Transformeræ¨¡å— - Stage 2è®­ç»ƒ

å®ç°åŸºäºVARæ¶æ„çš„æ¡ä»¶åŸºå› è¡¨è¾¾ç”Ÿæˆæ¨¡å‹ã€‚

æ ¸å¿ƒç‰¹æ€§ï¼š
1. æ¡ä»¶å¤„ç†å™¨ï¼šå¤„ç†ç»„ç»‡å­¦ç‰¹å¾å’Œç©ºé—´åæ ‡
2. VAR Transformerï¼šè‡ªå›å½’ç”ŸæˆåŸºå› tokens
3. ä¸¤é˜¶æ®µè®­ç»ƒï¼šStage 2å†»ç»“VQVAEï¼Œåªè®­ç»ƒTransformer
4. Next Token Predictionï¼šæ ‡å‡†çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹è®­ç»ƒ

æ¶æ„æµç¨‹ï¼š
1. æ¡ä»¶ä¿¡æ¯ï¼šç»„ç»‡å­¦ç‰¹å¾[1024] + ç©ºé—´åæ ‡[2] â†’ æ¡ä»¶åµŒå…¥[640]
2. Tokenåºåˆ—ï¼šåŸºå› tokens[B, 1446] (æ¥è‡ªå†»ç»“çš„VQVAEç¼–ç )
3. VARç”Ÿæˆï¼šæ¡ä»¶åµŒå…¥ + å†å²tokens â†’ ä¸‹ä¸€ä¸ªtokené¢„æµ‹
4. æŸå¤±è®¡ç®—ï¼šäº¤å‰ç†µæŸå¤± (next token prediction)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Tuple, Optional, Any, List
import math
import os
from tqdm import tqdm

from .shared_components import SharedVectorQuantizer
from .multi_scale_gene_vqvae import MultiScaleGeneVQVAE


class ConditionProcessor(nn.Module):
    """
    æ¡ä»¶å¤„ç†å™¨ - å¤„ç†ç»„ç»‡å­¦ç‰¹å¾å’Œç©ºé—´åæ ‡
    
    åŠŸèƒ½ï¼š
    1. ç»„ç»‡å­¦ç‰¹å¾å¤„ç†ï¼š1024ç»´ â†’ 512ç»´
    2. ç©ºé—´åæ ‡å¤„ç†ï¼š2ç»´ â†’ 128ç»´ (ä½ç½®ç¼–ç )
    3. æ¡ä»¶èåˆï¼š512 + 128 = 640ç»´æ¡ä»¶åµŒå…¥
    """
    
    def __init__(
        self,
        histology_dim: int = 1024,
        spatial_dim: int = 2,
        histology_hidden_dim: int = 512,
        spatial_hidden_dim: int = 128,
        condition_embed_dim: int = 640
    ):
        super().__init__()
        
        self.histology_dim = histology_dim
        self.spatial_dim = spatial_dim
        self.condition_embed_dim = condition_embed_dim
        
        # ç»„ç»‡å­¦ç‰¹å¾å¤„ç†å™¨
        self.histology_processor = nn.Sequential(
            nn.LayerNorm(histology_dim),
            nn.Linear(histology_dim, histology_hidden_dim),
            nn.ReLU(),
            nn.Linear(histology_hidden_dim, histology_hidden_dim),
            nn.LayerNorm(histology_hidden_dim)
        )
        
        # ç©ºé—´åæ ‡å¤„ç†å™¨ (ä½ç½®ç¼–ç )
        self.spatial_processor = nn.Sequential(
            nn.Linear(spatial_dim, spatial_hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(spatial_hidden_dim // 2, spatial_hidden_dim),
            nn.LayerNorm(spatial_hidden_dim)
        )
        
        # æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç  (å¯é€‰)
        self.use_sincos_pos = True
        if self.use_sincos_pos:
            # ä¸º2Dåæ ‡åˆ›å»ºæ­£å¼¦ä½™å¼¦ç¼–ç 
            self.pos_encoding_dim = spatial_hidden_dim // 2
            div_term = torch.exp(torch.arange(0, self.pos_encoding_dim, 2).float() * 
                               (-math.log(10000.0) / self.pos_encoding_dim))
            self.register_buffer('div_term', div_term)
        
        # æœ€ç»ˆæŠ•å½±å±‚ (ç¡®ä¿æ€»ç»´åº¦ä¸ºcondition_embed_dim)
        total_dim = histology_hidden_dim + spatial_hidden_dim
        if total_dim != condition_embed_dim:
            self.final_projection = nn.Linear(total_dim, condition_embed_dim)
        else:
            self.final_projection = nn.Identity()
    
    def forward(
        self, 
        histology_features: torch.Tensor,  # [B, 1024]
        spatial_coords: torch.Tensor       # [B, 2]
    ) -> torch.Tensor:                     # [B, 640]
        """
        å‰å‘ä¼ æ’­
        
        Args:
            histology_features: ç»„ç»‡å­¦ç‰¹å¾ [B, 1024]
            spatial_coords: ç©ºé—´åæ ‡ [B, 2]
            
        Returns:
            æ¡ä»¶åµŒå…¥ [B, 640]
        """
        # å¤„ç†ç»„ç»‡å­¦ç‰¹å¾
        histology_embed = self.histology_processor(histology_features)  # [B, 512]
        
        # å¤„ç†ç©ºé—´åæ ‡
        if self.use_sincos_pos:
            # åº”ç”¨æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç 
            B = spatial_coords.shape[0]
            x_coords = spatial_coords[:, 0:1]  # [B, 1]
            y_coords = spatial_coords[:, 1:2]  # [B, 1]
            
            # ä¸ºxå’Œyåæ ‡åˆ†åˆ«è®¡ç®—æ­£å¼¦ä½™å¼¦ç¼–ç 
            x_pe = torch.zeros(B, self.pos_encoding_dim, device=spatial_coords.device)
            y_pe = torch.zeros(B, self.pos_encoding_dim, device=spatial_coords.device)
            
            x_pe[:, 0::2] = torch.sin(x_coords * self.div_term[None, :])  # å¶æ•°ç»´åº¦sin
            x_pe[:, 1::2] = torch.cos(x_coords * self.div_term[None, :])  # å¥‡æ•°ç»´åº¦cos
            y_pe[:, 0::2] = torch.sin(y_coords * self.div_term[None, :])
            y_pe[:, 1::2] = torch.cos(y_coords * self.div_term[None, :])
            
            pos_encoding = torch.cat([x_pe, y_pe], dim=1)  # [B, spatial_hidden_dim]
            spatial_embed = self.spatial_processor(spatial_coords) + pos_encoding
        else:
            spatial_embed = self.spatial_processor(spatial_coords)  # [B, 128]
        
        # èåˆç‰¹å¾
        condition_features = torch.cat([histology_embed, spatial_embed], dim=1)  # [B, 640]
        condition_embed = self.final_projection(condition_features)  # [B, 640]
        
        return condition_embed


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç  - ä¸ºtokenåºåˆ—æ·»åŠ ä½ç½®ä¿¡æ¯"""
    
    def __init__(self, d_model: int, max_len: int = 2000):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)  # [max_len, 1, d_model]
        
        self.register_buffer('pe', pe)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [seq_len, batch, d_model]
        Returns:
            ä½ç½®ç¼–ç åçš„å¼ é‡ [seq_len, batch, d_model]
        """
        return x + self.pe[:x.size(0), :]


class GeneVARTransformer(nn.Module):
    """
    åŸºå› VAR Transformer - Stage 2çš„æ ¸å¿ƒæ¨¡å‹
    
    æ¶æ„ï¼š
    1. TokenåµŒå…¥ï¼šå°†åŸºå› tokensè½¬æ¢ä¸ºåµŒå…¥å‘é‡
    2. ä½ç½®ç¼–ç ï¼šä¸ºtokenåºåˆ—æ·»åŠ ä½ç½®ä¿¡æ¯
    3. æ¡ä»¶èåˆï¼šå°†æ¡ä»¶ä¿¡æ¯èå…¥æ¯ä¸ªTransformerå±‚
    4. Transformerï¼šå¤šå±‚è‡ªæ³¨æ„åŠ›æœºåˆ¶
    5. è¾“å‡ºæŠ•å½±ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒ
    """
    
    def __init__(
        self,
        vocab_size: int = 4096,
        embed_dim: int = 640,
        num_heads: int = 8,
        num_layers: int = 12,
        feedforward_dim: int = 2560,
        dropout: float = 0.1,
        max_sequence_length: int = 1500,
        condition_embed_dim: int = 640
    ):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.max_sequence_length = max_sequence_length
        
        # TokenåµŒå…¥å±‚
        self.token_embedding = nn.Embedding(vocab_size, embed_dim)
        
        # ä½ç½®ç¼–ç 
        self.positional_encoding = PositionalEncoding(embed_dim, max_sequence_length)
        
        # æ¡ä»¶æŠ•å½± (å¦‚æœæ¡ä»¶ç»´åº¦ä¸ç­‰äºembed_dim)
        if condition_embed_dim != embed_dim:
            self.condition_projection = nn.Linear(condition_embed_dim, embed_dim)
        else:
            self.condition_projection = nn.Identity()
        
        # Transformerè§£ç å™¨å±‚
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=feedforward_dim,
            dropout=dropout,
            activation='relu',
            batch_first=False  # ä½¿ç”¨(seq_len, batch, embed_dim)æ ¼å¼
        )
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)
        
        # è¾“å‡ºæŠ•å½±å±‚
        self.output_projection = nn.Linear(embed_dim, vocab_size)
        
        # åˆå§‹åŒ–å‚æ•°
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
                if module.bias is not None:
                    torch.nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def create_causal_mask(self, seq_len: int, device: torch.device) -> torch.Tensor:
        """åˆ›å»ºå› æœæ³¨æ„åŠ›æ©ç """
        mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf'))
        return mask
    
    def forward(
        self,
        input_tokens: torch.Tensor,      # [B, seq_len]
        condition_embed: torch.Tensor,   # [B, condition_embed_dim]
        target_tokens: Optional[torch.Tensor] = None  # [B, seq_len] for training
    ) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_tokens: è¾“å…¥tokenåºåˆ— [B, seq_len]
            condition_embed: æ¡ä»¶åµŒå…¥ [B, condition_embed_dim] 
            target_tokens: ç›®æ ‡tokenåºåˆ— [B, seq_len] (è®­ç»ƒæ—¶ä½¿ç”¨)
            
        Returns:
            åŒ…å«logitså’Œlossçš„å­—å…¸
        """
        B, seq_len = input_tokens.shape
        device = input_tokens.device
        
        # TokenåµŒå…¥
        token_embeds = self.token_embedding(input_tokens)  # [B, seq_len, embed_dim]
        token_embeds = token_embeds.transpose(0, 1)        # [seq_len, B, embed_dim]
        
        # ä½ç½®ç¼–ç 
        token_embeds = self.positional_encoding(token_embeds)  # [seq_len, B, embed_dim]
        
        # å¤„ç†æ¡ä»¶ä¿¡æ¯
        condition_proj = self.condition_projection(condition_embed)  # [B, embed_dim]
        # æ‰©å±•æ¡ä»¶ä¸ºè®°å¿†åºåˆ—ï¼Œç”¨ä½œTransformerçš„memory
        memory = condition_proj.unsqueeze(0)  # [1, B, embed_dim]
        
        # åˆ›å»ºå› æœæ©ç 
        tgt_mask = self.create_causal_mask(seq_len, device)  # [seq_len, seq_len]
        
        # Transformerè§£ç 
        transformer_output = self.transformer_decoder(
            tgt=token_embeds,           # [seq_len, B, embed_dim]
            memory=memory,              # [1, B, embed_dim]
            tgt_mask=tgt_mask          # [seq_len, seq_len]
        )  # [seq_len, B, embed_dim]
        
        # è¾“å‡ºæŠ•å½±
        logits = self.output_projection(transformer_output)  # [seq_len, B, vocab_size]
        logits = logits.transpose(0, 1)  # [B, seq_len, vocab_size]
        
        result = {'logits': logits}
        
        # å¦‚æœæä¾›äº†ç›®æ ‡tokensï¼Œè®¡ç®—æŸå¤±å’ŒæŒ‡æ ‡
        if target_tokens is not None:
            # è®¡ç®—äº¤å‰ç†µæŸå¤± (next token prediction)
            # è¾“å…¥: input_tokens[:-1], ç›®æ ‡: target_tokens[1:]
            shift_logits = logits[:, :-1, :].contiguous()  # [B, seq_len-1, vocab_size]
            shift_labels = target_tokens[:, 1:].contiguous()  # [B, seq_len-1]
            
            # ğŸ”§ Stage 2åªä½¿ç”¨äº¤å‰ç†µæŸå¤±ï¼Œç§»é™¤å…¶ä»–æŸå¤±ç»„ä»¶
            loss = F.cross_entropy(
                shift_logits.view(-1, self.vocab_size),
                shift_labels.view(-1),
                ignore_index=-1,  # å¿½ç•¥padding tokens
                reduction='mean'
            )
            
            result['loss'] = loss
            
            # è®¡ç®—å‡†ç¡®ç‡å’Œå›°æƒ‘åº¦
            with torch.no_grad():
                # Tokené¢„æµ‹å‡†ç¡®ç‡
                predictions = torch.argmax(shift_logits, dim=-1)
                valid_mask = (shift_labels != -1)  # å¿½ç•¥padding
                accuracy = (predictions == shift_labels)[valid_mask].float().mean()
                result['accuracy'] = accuracy
                
                # ğŸ”§ å›°æƒ‘åº¦è®¡ç®—ï¼šperplexity = exp(loss)
                # å›°æƒ‘åº¦è¡¡é‡æ¨¡å‹é¢„æµ‹çš„ä¸ç¡®å®šæ€§ï¼Œè¶Šä½è¶Šå¥½
                perplexity = torch.exp(loss)
                result['perplexity'] = perplexity
                
                # ğŸ”§ é¢å¤–æŒ‡æ ‡ï¼štop-5å‡†ç¡®ç‡
                top5_predictions = torch.topk(shift_logits, k=5, dim=-1)[1]  # [B, seq_len-1, 5]
                shift_labels_expanded = shift_labels.unsqueeze(-1).expand_as(top5_predictions)
                top5_accuracy = (top5_predictions == shift_labels_expanded).any(dim=-1)[valid_mask].float().mean()
                result['top5_accuracy'] = top5_accuracy
        
        return result
    
    @torch.no_grad()
    def generate(
        self,
        condition_embed: torch.Tensor,   # [B, condition_embed_dim]
        max_length: int = 1446,
        temperature: float = 1.0,
        top_k: Optional[int] = None,
        top_p: Optional[float] = None
    ) -> torch.Tensor:                   # [B, max_length]
        """
        è‡ªå›å½’ç”ŸæˆåŸºå› tokens
        
        Args:
            condition_embed: æ¡ä»¶åµŒå…¥ [B, condition_embed_dim]
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦
            top_k: top-ké‡‡æ ·
            top_p: nucleusé‡‡æ ·
            
        Returns:
            ç”Ÿæˆçš„tokenåºåˆ— [B, max_length]
        """
        B = condition_embed.shape[0]
        device = condition_embed.device
        
        # åˆå§‹åŒ–åºåˆ— (ä½¿ç”¨ç‰¹æ®Šçš„å¼€å§‹tokenï¼Œè¿™é‡Œç”¨0)
        generated = torch.zeros(B, 1, dtype=torch.long, device=device)
        
        for step in range(max_length - 1):
            # å‰å‘ä¼ æ’­
            outputs = self.forward(generated, condition_embed)
            logits = outputs['logits']  # [B, current_length, vocab_size]
            
            # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits
            next_token_logits = logits[:, -1, :] / temperature  # [B, vocab_size]
            
            # åº”ç”¨top-ké‡‡æ ·
            if top_k is not None:
                values, indices = torch.topk(next_token_logits, top_k)
                next_token_logits = torch.full_like(next_token_logits, float('-inf'))
                next_token_logits.scatter_(1, indices, values)
            
            # åº”ç”¨nucleus (top-p)é‡‡æ ·
            if top_p is not None:
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                
                # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„tokens
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                
                for i in range(B):
                    indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]
                    next_token_logits[i][indices_to_remove] = float('-inf')
            
            # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
            probs = F.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, 1)  # [B, 1]
            
            # æ·»åŠ åˆ°åºåˆ—ä¸­
            generated = torch.cat([generated, next_token], dim=1)
        
        return generated


class Stage2Trainer:
    """
    Stage 2è®­ç»ƒå™¨ - è®­ç»ƒåŸºå› VAR Transformer
    
    åŠŸèƒ½ï¼š
    1. å†»ç»“Stage 1çš„VQVAEæ¨¡å‹
    2. è®­ç»ƒVAR Transformerè¿›è¡Œæ¡ä»¶ç”Ÿæˆ
    3. ç®¡ç†è®­ç»ƒå¾ªç¯å’ŒéªŒè¯
    4. ä¿å­˜å’ŒåŠ è½½checkpoint
    """
    
    def __init__(
        self,
        vqvae_model: MultiScaleGeneVQVAE,
        var_transformer: GeneVARTransformer,
        condition_processor: ConditionProcessor,
        device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'),
        learning_rate: float = 1e-4,
        weight_decay: float = 1e-4,
        print_freq: int = 100
    ):
        self.device = device
        self.print_freq = print_freq
        
        # æ¨¡å‹ç»„ä»¶
        self.vqvae_model = vqvae_model.to(device)
        self.var_transformer = var_transformer.to(device)
        self.condition_processor = condition_processor.to(device)
        
        # å†»ç»“VQVAEå‚æ•°
        for param in self.vqvae_model.parameters():
            param.requires_grad = False
        self.vqvae_model.eval()
        
        # ä¼˜åŒ–å™¨ (åªä¼˜åŒ–VAR Transformerå’Œæ¡ä»¶å¤„ç†å™¨)
        trainable_params = list(self.var_transformer.parameters()) + list(self.condition_processor.parameters())
        self.optimizer = torch.optim.AdamW(trainable_params, lr=learning_rate, weight_decay=weight_decay)
        
        # è®­ç»ƒç»Ÿè®¡
        self.epoch_losses = []
        self.epoch_accuracies = []
    
    def train_epoch(self, dataloader, epoch: int) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€ä¸ªepoch
        
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨ (åŒ…å«åŸºå› è¡¨è¾¾ã€ç»„ç»‡å­¦ç‰¹å¾ã€ç©ºé—´åæ ‡)
            epoch: å½“å‰epoch
            
        Returns:
            å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        """
        self.var_transformer.train()
        self.condition_processor.train()
        
        epoch_loss = 0.0
        epoch_accuracy = 0.0
        num_batches = len(dataloader)
        
        for batch_idx, batch in enumerate(dataloader):
            # ä¸¥æ ¼éªŒè¯æ•°æ®æ ¼å¼
            if isinstance(batch, (list, tuple)):
                if len(batch) < 3:
                    raise ValueError(f"Batch must contain [gene_expressions, histology_features, spatial_coords], "
                                   f"but got {len(batch)} elements")
                gene_expressions = batch[0]
                histology_features = batch[1]
                spatial_coords = batch[2]
            else:
                raise ValueError("Batch must be a tuple/list containing [gene_expressions, histology_features, spatial_coords]. "
                               "Single tensor batches are not supported for Stage 2 training.")
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            gene_expressions = gene_expressions.to(self.device)
            histology_features = histology_features.to(self.device)
            spatial_coords = spatial_coords.to(self.device)
            
            # ä½¿ç”¨å†»ç»“çš„VQVAEç¼–ç åŸºå› è¡¨è¾¾ä¸ºtokens
            with torch.no_grad():
                vqvae_result = self.vqvae_model(gene_expressions)
                tokens = vqvae_result['tokens']  # Dict of tokens for each scale
                
                # å°†å¤šå°ºåº¦tokenså±•å¹³ä¸ºåºåˆ—
                token_sequence = []
                for scale in ['global', 'pathway', 'module', 'individual']:
                    scale_tokens = tokens[scale].view(tokens[scale].shape[0], -1)  # [B, num_tokens]
                    token_sequence.append(scale_tokens)
                
                full_token_sequence = torch.cat(token_sequence, dim=1)  # [B, total_seq_len]
            
            # å¤„ç†æ¡ä»¶ä¿¡æ¯
            condition_embed = self.condition_processor(histology_features, spatial_coords)
            
            # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡ (teacher forcing)
            input_tokens = full_token_sequence  # [B, seq_len]
            target_tokens = full_token_sequence  # [B, seq_len] (same for autoregressive training)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.var_transformer(input_tokens, condition_embed, target_tokens)
            
            loss = outputs['loss']
            accuracy = outputs['accuracy']
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.var_transformer.parameters(), max_norm=1.0)
            torch.nn.utils.clip_grad_norm_(self.condition_processor.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            # ç´¯ç§¯ç»Ÿè®¡
            epoch_loss += loss.item()
            epoch_accuracy += accuracy.item()
            
            # æ‰“å°è¿›åº¦
            if batch_idx % self.print_freq == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}/{num_batches}: "
                      f"Loss={loss.item():.4f}, Accuracy={accuracy.item():.4f}")
        
        # è®¡ç®—å¹³å‡å€¼
        avg_loss = epoch_loss / num_batches
        avg_accuracy = epoch_accuracy / num_batches
        
        # ä¿å­˜ç»Ÿè®¡
        self.epoch_losses.append(avg_loss)
        self.epoch_accuracies.append(avg_accuracy)
        
        return {'loss': avg_loss, 'accuracy': avg_accuracy}
    
    @torch.no_grad()
    def validate_epoch(self, dataloader, epoch: int) -> Dict[str, float]:
        """
        éªŒè¯ä¸€ä¸ªepoch
        
        Args:
            dataloader: éªŒè¯æ•°æ®åŠ è½½å™¨
            epoch: å½“å‰epoch
            
        Returns:
            éªŒè¯æŸå¤±å’Œå‡†ç¡®ç‡
        """
        self.var_transformer.eval()
        self.condition_processor.eval()
        
        val_loss = 0.0
        val_accuracy = 0.0
        num_batches = len(dataloader)
        
        for batch in dataloader:
            # ä¸¥æ ¼éªŒè¯æ•°æ®æ ¼å¼ (ä¸è®­ç»ƒç›¸åŒ)
            if isinstance(batch, (list, tuple)):
                if len(batch) < 3:
                    raise ValueError(f"Validation batch must contain [gene_expressions, histology_features, spatial_coords], "
                                   f"but got {len(batch)} elements")
                gene_expressions = batch[0]
                histology_features = batch[1]
                spatial_coords = batch[2]
            else:
                raise ValueError("Validation batch must be a tuple/list containing [gene_expressions, histology_features, spatial_coords]. "
                               "Single tensor batches are not supported for Stage 2 validation.")
            
            gene_expressions = gene_expressions.to(self.device)
            histology_features = histology_features.to(self.device)
            spatial_coords = spatial_coords.to(self.device)
            
            # ç¼–ç åŸºå› tokens
            vqvae_result = self.vqvae_model(gene_expressions)
            tokens = vqvae_result['tokens']
            
            token_sequence = []
            for scale in ['global', 'pathway', 'module', 'individual']:
                scale_tokens = tokens[scale].view(tokens[scale].shape[0], -1)
                token_sequence.append(scale_tokens)
            
            full_token_sequence = torch.cat(token_sequence, dim=1)
            
            # å¤„ç†æ¡ä»¶
            condition_embed = self.condition_processor(histology_features, spatial_coords)
            
            # å‰å‘ä¼ æ’­
            outputs = self.var_transformer(full_token_sequence, condition_embed, full_token_sequence)
            
            val_loss += outputs['loss'].item()
            val_accuracy += outputs['accuracy'].item()
        
        avg_val_loss = val_loss / num_batches
        avg_val_accuracy = val_accuracy / num_batches
        
        return {'loss': avg_val_loss, 'accuracy': avg_val_accuracy}
    
    def save_checkpoint(self, filepath: str, epoch: int, metadata: Optional[Dict] = None):
        """ä¿å­˜Stage 2 checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'var_transformer_state_dict': self.var_transformer.state_dict(),
            'condition_processor_state_dict': self.condition_processor.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epoch_losses': self.epoch_losses,
            'epoch_accuracies': self.epoch_accuracies,
            'metadata': metadata or {}
        }
        torch.save(checkpoint, filepath)
        print(f"Stage 2 checkpointä¿å­˜è‡³: {filepath}")
    
    def load_checkpoint(self, filepath: str) -> Dict:
        """åŠ è½½Stage 2 checkpoint - ä¸¥æ ¼éªŒè¯"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        # ä¸¥æ ¼éªŒè¯checkpointå®Œæ•´æ€§
        required_keys = ['var_transformer_state_dict', 'condition_processor_state_dict', 
                        'optimizer_state_dict', 'epoch', 'epoch_losses', 'epoch_accuracies']
        
        for key in required_keys:
            if key not in checkpoint:
                raise KeyError(f"Missing required key in Stage 2 checkpoint: {key}")
        
        self.var_transformer.load_state_dict(checkpoint['var_transformer_state_dict'])
        self.condition_processor.load_state_dict(checkpoint['condition_processor_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epoch_losses = checkpoint['epoch_losses']
        self.epoch_accuracies = checkpoint['epoch_accuracies']
        
        print(f"Stage 2 checkpointåŠ è½½: {filepath}, epoch: {checkpoint['epoch']}")
        return checkpoint