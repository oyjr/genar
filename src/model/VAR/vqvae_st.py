#!/usr/bin/env python3
"""
Multi-scale VQVAE for Spatial Transcriptomics Gene Expression
å®Œå…¨é€‚é…VARæ¶æ„çš„å¤šå°ºåº¦VQVAEå®ç°

å…³é”®è®¾è®¡:
- 5ä¸ªç‹¬ç«‹çš„VQVAEç¼–ç å™¨å¤„ç†ä¸åŒå°ºåº¦: 1x1, 2x2, 3x3, 4x4, 5x5
- è¾“å…¥: åŸºå› ä¼ªå›¾åƒ [B, 1, 15, 15] (225ä¸ªåŸºå› )
- è¾“å‡º: å¤šå°ºåº¦ç¦»æ•£tokens [1+4+9+16+25 = 55ä¸ªtokens]
- å®Œå…¨ä¿æŒVARåŸå§‹VQVAEæ¶æ„ä¸å˜
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Tuple, Optional, Dict, Any
import math

# å¯¼å…¥VARåŸºç¡€ç»„ä»¶
try:
    from .var_basic_components import (
        Encoder, Decoder, VectorQuantizer2, 
        ResnetBlock, AttnBlock, Downsample, Upsample,
        nonlinearity, normalize
    )
except ImportError:
    from var_basic_components import (
        Encoder, Decoder, VectorQuantizer2,
        ResnetBlock, AttnBlock, Downsample, Upsample, 
        nonlinearity, normalize
    )


class MultiScaleVQVAE_ST(nn.Module):
    """
    å¤šå°ºåº¦VQVAEç”¨äºç©ºé—´è½¬å½•ç»„å­¦åŸºå› è¡¨è¾¾æ•°æ®
    
    æ¶æ„è®¾è®¡:
    - 5ä¸ªç‹¬ç«‹çš„VQVAEç¼–ç å™¨/è§£ç å™¨å¯¹
    - æ¯ä¸ªå°ºåº¦å¤„ç†ä¸åŒåˆ†è¾¨ç‡çš„åŸºå› è¡¨è¾¾æ¨¡å¼
    - ä»å…¨å±€åŸºå› æ¨¡å¼(1x1)åˆ°å•åŸºå› çº§åˆ«(5x5)
    - æ€»å…±ç”Ÿæˆ55ä¸ªç¦»æ•£tokensä¾›VARä½¿ç”¨
    """
    
    def __init__(
        self,
        gene_count: int = 225,                    # åŸºå› æ•°é‡ (15x15 = 225)
        patch_nums: Tuple[int, ...] = (1, 2, 3, 4, 5),  # å¤šå°ºåº¦patchæ•°é‡
        vocab_size: int = 8192,                   # æ¯ä¸ªå°ºåº¦çš„è¯æ±‡è¡¨å¤§å°
        embed_dim: int = 256,                     # VQVAEåµŒå…¥ç»´åº¦
        hidden_dim: int = 128,                    # ç¼–ç å™¨éšè—ç»´åº¦
        num_res_blocks: int = 2,                  # ResNetå—æ•°é‡
        dropout: float = 0.0,                     # Dropoutç‡
        beta: float = 0.25,                       # Commitment lossæƒé‡
        using_znorm: bool = False,                # æ˜¯å¦ä½¿ç”¨Z-normalization
        share_decoder: bool = True,               # æ˜¯å¦å…±äº«è§£ç å™¨
        test_mode: bool = False,                  # æµ‹è¯•æ¨¡å¼(å†»ç»“å‚æ•°)
    ):
        super().__init__()
        
        self.gene_count = gene_count
        self.patch_nums = patch_nums
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.num_scales = len(patch_nums)
        self.share_decoder = share_decoder
        self.test_mode = test_mode
        
        # è®¡ç®—æ€»tokenæ•°é‡
        self.total_tokens = sum(pn * pn for pn in patch_nums)  # 1+4+9+16+25 = 55
        
        # åŸºå› ç©ºé—´ç»´åº¦
        self.gene_spatial_dim = int(math.sqrt(gene_count))  # 15 for 225 genes
        if self.gene_spatial_dim * self.gene_spatial_dim != gene_count:
            raise ValueError(f"åŸºå› æ•°é‡{gene_count}å¿…é¡»æ˜¯å®Œå…¨å¹³æ–¹æ•°")
        
        print(f"ğŸ§¬ åˆå§‹åŒ–å¤šå°ºåº¦VQVAE_ST:")
        print(f"  - åŸºå› æ•°é‡: {gene_count} ({self.gene_spatial_dim}x{self.gene_spatial_dim})")
        print(f"  - å¤šå°ºåº¦patch: {patch_nums}")
        print(f"  - æ€»tokenæ•°: {self.total_tokens}")
        print(f"  - è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        print(f"  - åµŒå…¥ç»´åº¦: {embed_dim}")
        print(f"  - å…±äº«è§£ç å™¨: {share_decoder}")
        
        # ä¸ºæ¯ä¸ªå°ºåº¦åˆ›å»ºç‹¬ç«‹çš„ç¼–ç å™¨
        self.encoders = nn.ModuleList()
        for i, pn in enumerate(patch_nums):
            encoder = self._create_encoder(
                input_size=pn,  # è¾“å…¥å°ºå¯¸
                hidden_dim=hidden_dim,
                embed_dim=embed_dim,
                num_res_blocks=num_res_blocks,
                dropout=dropout,
                scale_idx=i
            )
            self.encoders.append(encoder)
        
        # ä¸ºæ¯ä¸ªå°ºåº¦åˆ›å»ºé‡åŒ–å™¨
        self.quantizers = nn.ModuleList()
        for i, pn in enumerate(patch_nums):
            quantizer = VectorQuantizer2(
                vocab_size=vocab_size,
                Cvae=embed_dim,
                using_znorm=using_znorm,
                beta=beta,
                v_patch_nums=(pn,),  # æ¯ä¸ªé‡åŒ–å™¨åªå¤„ç†å•ä¸€å°ºåº¦
                default_qresi_counts=1,
                quant_resi=0.5,
                share_quant_resi=1,  # å•å°ºåº¦å…±äº«
            )
            self.quantizers.append(quantizer)
        
        # è§£ç å™¨ - å¯é€‰æ‹©å…±äº«æˆ–ç‹¬ç«‹
        if share_decoder:
            # å…±äº«è§£ç å™¨ - ä»æ‰€æœ‰å°ºåº¦çš„ç‰¹å¾é‡å»ºåˆ°å®Œæ•´åŸºå› ç©ºé—´
            self.decoder = self._create_shared_decoder(
                embed_dim=embed_dim,
                hidden_dim=hidden_dim,
                output_size=self.gene_spatial_dim,
                num_res_blocks=num_res_blocks,
                dropout=dropout
            )
            self.decoders = None
        else:
            # ç‹¬ç«‹è§£ç å™¨ - æ¯ä¸ªå°ºåº¦ç‹¬ç«‹è§£ç 
            self.decoders = nn.ModuleList()
            for i, pn in enumerate(patch_nums):
                decoder = self._create_decoder(
                    embed_dim=embed_dim,
                    hidden_dim=hidden_dim,
                    output_size=pn,
                    target_size=self.gene_spatial_dim,
                    num_res_blocks=num_res_blocks,
                    dropout=dropout,
                    scale_idx=i
                )
                self.decoders.append(decoder)
            self.decoder = None
        
        # é‡åŒ–å‰åçš„å·ç§¯å±‚
        self.quant_convs = nn.ModuleList([
            nn.Conv2d(embed_dim, embed_dim, 1) for _ in patch_nums
        ])
        self.post_quant_convs = nn.ModuleList([
            nn.Conv2d(embed_dim, embed_dim, 1) for _ in patch_nums
        ])
        
        # åˆå§‹åŒ–æƒé‡
        self.init_weights()
        
        # æµ‹è¯•æ¨¡å¼è®¾ç½®
        if test_mode:
            self.eval()
            for p in self.parameters():
                p.requires_grad_(False)
        
    def _create_encoder(
        self, 
        input_size: int, 
        hidden_dim: int, 
        embed_dim: int,
        num_res_blocks: int,
        dropout: float,
        scale_idx: int
    ) -> nn.Module:
        """ä¸ºæŒ‡å®šå°ºåº¦åˆ›å»ºç¼–ç å™¨"""
        
        layers = []
        
        # è¾“å…¥æŠ•å½±å±‚ - ä»åŸºå› ä¼ªå›¾åƒåˆ°éšè—ç»´åº¦
        layers.append(nn.Conv2d(1, hidden_dim, 3, stride=1, padding=1))
        layers.append(nn.GroupNorm(32, hidden_dim))
        layers.append(nn.SiLU())
        
        # è‡ªé€‚åº”åˆ°ç›®æ ‡å°ºå¯¸
        if input_size != self.gene_spatial_dim:
            # ä¸‹é‡‡æ ·åˆ°ç›®æ ‡å°ºå¯¸
            while layers[-3].out_channels < embed_dim // 2:
                in_ch = layers[-3].out_channels if hasattr(layers[-3], 'out_channels') else hidden_dim
                out_ch = min(in_ch * 2, embed_dim // 2)
                
                layers.append(nn.Conv2d(in_ch, out_ch, 3, stride=2, padding=1))
                layers.append(nn.GroupNorm(32, out_ch))
                layers.append(nn.SiLU())
        
        # ResNetå—
        current_dim = layers[-3].out_channels if hasattr(layers[-3], 'out_channels') else hidden_dim
        for _ in range(num_res_blocks):
            layers.append(ResnetBlock(
                in_channels=current_dim,
                out_channels=current_dim,
                dropout=dropout
            ))
        
        # æœ€ç»ˆæŠ•å½±åˆ°åµŒå…¥ç»´åº¦
        layers.append(nn.Conv2d(current_dim, embed_dim, 3, stride=1, padding=1))
        layers.append(nn.GroupNorm(32, embed_dim))
        layers.append(nn.SiLU())
        
        # è‡ªé€‚åº”æ± åŒ–åˆ°ç›®æ ‡å°ºå¯¸
        layers.append(nn.AdaptiveAvgPool2d((input_size, input_size)))
        
        return nn.Sequential(*layers)
    
    def _create_decoder(
        self,
        embed_dim: int,
        hidden_dim: int, 
        output_size: int,
        target_size: int,
        num_res_blocks: int,
        dropout: float,
        scale_idx: int
    ) -> nn.Module:
        """ä¸ºæŒ‡å®šå°ºåº¦åˆ›å»ºè§£ç å™¨"""
        
        layers = []
        
        # è¾“å…¥æŠ•å½±
        layers.append(nn.Conv2d(embed_dim, hidden_dim, 3, stride=1, padding=1))
        layers.append(nn.GroupNorm(32, hidden_dim))
        layers.append(nn.SiLU())
        
        # ResNetå—
        for _ in range(num_res_blocks):
            layers.append(ResnetBlock(
                in_channels=hidden_dim,
                out_channels=hidden_dim,
                dropout=dropout
            ))
        
        # ä¸Šé‡‡æ ·åˆ°ç›®æ ‡å°ºå¯¸
        layers.append(nn.Upsample(size=(target_size, target_size), mode='bilinear', align_corners=False))
        
        # æœ€ç»ˆè¾“å‡ºå±‚
        layers.append(nn.Conv2d(hidden_dim, 1, 3, stride=1, padding=1))
        
        return nn.Sequential(*layers)
    
    def _create_shared_decoder(
        self,
        embed_dim: int,
        hidden_dim: int,
        output_size: int,
        num_res_blocks: int,
        dropout: float
    ) -> nn.Module:
        """åˆ›å»ºå…±äº«è§£ç å™¨ï¼Œä»èåˆçš„å¤šå°ºåº¦ç‰¹å¾é‡å»ºå®Œæ•´åŸºå› è¡¨è¾¾"""
        
        # ç‰¹å¾èåˆå±‚
        fusion_layers = []
        fusion_layers.append(nn.Conv2d(embed_dim * self.num_scales, hidden_dim * 2, 1))
        fusion_layers.append(nn.GroupNorm(32, hidden_dim * 2))
        fusion_layers.append(nn.SiLU())
        
        # ResNetå¤„ç†
        layers = []
        layers.extend(fusion_layers)
        
        current_dim = hidden_dim * 2
        for _ in range(num_res_blocks + 1):  # å¤šä¸€å±‚ResNetå¤„ç†èåˆç‰¹å¾
            layers.append(ResnetBlock(
                in_channels=current_dim,
                out_channels=hidden_dim,
                dropout=dropout
            ))
            current_dim = hidden_dim
        
        # ä¸Šé‡‡æ ·åˆ°ç›®æ ‡åˆ†è¾¨ç‡
        layers.append(nn.Upsample(size=(output_size, output_size), mode='bilinear', align_corners=False))
        
        # æœ€ç»ˆè¾“å‡º
        layers.append(nn.Conv2d(hidden_dim, 1, 3, stride=1, padding=1))
        
        return nn.Sequential(*layers)
    
    def init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.GroupNorm):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, std=0.02)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def encode(self, gene_pseudo_img: torch.Tensor) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:
        """
        ç¼–ç åŸºå› ä¼ªå›¾åƒåˆ°å¤šå°ºåº¦ç¦»æ•£tokens
        
        Args:
            gene_pseudo_img: [B, 1, H, W] åŸºå› ä¼ªå›¾åƒ (H=W=15 for 225 genes)
            
        Returns:
            ms_tokens: List[torch.Tensor] - æ¯ä¸ªå°ºåº¦çš„ç¦»æ•£token indices
            ms_embeddings: List[torch.Tensor] - æ¯ä¸ªå°ºåº¦çš„è¿ç»­åµŒå…¥
        """
        B = gene_pseudo_img.shape[0]
        
        # è°ƒæ•´è¾“å…¥åˆ°æ­£ç¡®çš„åŸºå› ç©ºé—´ç»´åº¦
        if gene_pseudo_img.shape[-1] != self.gene_spatial_dim:
            gene_pseudo_img = F.interpolate(
                gene_pseudo_img, 
                size=(self.gene_spatial_dim, self.gene_spatial_dim),
                mode='bilinear', 
                align_corners=False
            )
        
        ms_tokens = []
        ms_embeddings = []
        
        for i, pn in enumerate(self.patch_nums):
            # ä¸ºå½“å‰å°ºåº¦è°ƒæ•´è¾“å…¥å°ºå¯¸
            scale_input = F.adaptive_avg_pool2d(gene_pseudo_img, (pn, pn))
            
            # ç¼–ç 
            encoded = self.encoders[i](scale_input)  # [B, embed_dim, pn, pn]
            
            # é‡åŒ–å‰å·ç§¯
            quant_input = self.quant_convs[i](encoded)
            
            # é‡åŒ–
            quantized, _, vq_loss = self.quantizers[i](quant_input)
            
            # æå–token indices
            tokens = self.quantizers[i].f_to_idxBl_or_fhat(quant_input, to_fhat=False, v_patch_nums=(pn,))
            if isinstance(tokens, list):
                tokens = tokens[0]  # [B, pn*pn]
            
            ms_tokens.append(tokens)
            ms_embeddings.append(quantized)
        
        return ms_tokens, ms_embeddings
    
    def decode(self, ms_tokens: List[torch.Tensor]) -> torch.Tensor:
        """
        ä»å¤šå°ºåº¦tokensè§£ç å›åŸºå› ä¼ªå›¾åƒ
        
        Args:
            ms_tokens: List[torch.Tensor] - æ¯ä¸ªå°ºåº¦çš„token indices
            
        Returns:
            reconstructed: [B, 1, H, W] é‡å»ºçš„åŸºå› ä¼ªå›¾åƒ
        """
        B = ms_tokens[0].shape[0]
        
        # å°†tokensè½¬æ¢ä¸ºembeddings
        ms_embeddings = []
        for i, tokens in enumerate(ms_tokens):
            pn = self.patch_nums[i]
            
            # é‡å¡‘tokens
            if tokens.dim() == 2:
                # [B, pn*pn] -> [B, pn, pn]
                tokens_2d = tokens.view(B, pn, pn)
            else:
                tokens_2d = tokens
            
            # æŸ¥æ‰¾embeddings
            embeddings = self.quantizers[i].embedding(tokens_2d)  # [B, pn, pn, embed_dim]
            embeddings = embeddings.permute(0, 3, 1, 2).contiguous()  # [B, embed_dim, pn, pn]
            
            # åé‡åŒ–å·ç§¯
            embeddings = self.post_quant_convs[i](embeddings)
            
            ms_embeddings.append(embeddings)
        
        # è§£ç 
        if self.share_decoder:
            # å…±äº«è§£ç å™¨ï¼šèåˆæ‰€æœ‰å°ºåº¦ç‰¹å¾
            # å°†æ‰€æœ‰å°ºåº¦ä¸Šé‡‡æ ·åˆ°ç›¸åŒå¤§å°å¹¶è¿æ¥
            target_size = self.gene_spatial_dim
            fused_features = []
            
            for emb in ms_embeddings:
                upsampled = F.interpolate(emb, size=(target_size, target_size), mode='bilinear', align_corners=False)
                fused_features.append(upsampled)
            
            # è¿æ¥æ‰€æœ‰å°ºåº¦ç‰¹å¾
            fused = torch.cat(fused_features, dim=1)  # [B, embed_dim*num_scales, H, W]
            
            # å…±äº«è§£ç å™¨é‡å»º
            reconstructed = self.decoder(fused)
        else:
            # ç‹¬ç«‹è§£ç å™¨ï¼šæ¯ä¸ªå°ºåº¦ç‹¬ç«‹è§£ç åèåˆ
            scale_outputs = []
            
            for i, emb in enumerate(ms_embeddings):
                decoded = self.decoders[i](emb)  # [B, 1, H, W]
                scale_outputs.append(decoded)
            
            # ç®€å•å¹³å‡èåˆ
            reconstructed = torch.stack(scale_outputs, dim=0).mean(dim=0)
        
        return reconstructed
    
    def forward(self, gene_pseudo_img: torch.Tensor, return_tokens: bool = False, return_loss: bool = True):
        """
        å‰å‘ä¼ æ’­ - å®Œæ•´çš„ç¼–ç -é‡åŒ–-è§£ç è¿‡ç¨‹
        
        Args:
            gene_pseudo_img: [B, 1, H, W] è¾“å…¥åŸºå› ä¼ªå›¾åƒ
            return_tokens: æ˜¯å¦è¿”å›ç¦»æ•£tokens
            return_loss: æ˜¯å¦è®¡ç®—å¹¶è¿”å›æŸå¤±
            
        Returns:
            å¦‚æœreturn_tokens=True:
                (reconstructed, ms_tokens, total_loss)
            å¦åˆ™:
                (reconstructed, total_loss)
        """
        # ç¼–ç 
        ms_tokens, ms_embeddings = self.encode(gene_pseudo_img)
        
        # è§£ç 
        reconstructed = self.decode(ms_tokens)
        
        # è®¡ç®—æŸå¤±
        if return_loss:
            # é‡å»ºæŸå¤±
            recon_loss = F.mse_loss(reconstructed, gene_pseudo_img)
            
            # VQæŸå¤±(commitment loss)
            vq_loss = 0.0
            for i, emb in enumerate(ms_embeddings):
                # æ¯ä¸ªé‡åŒ–å™¨çš„commitment losså·²ç»åœ¨é‡åŒ–è¿‡ç¨‹ä¸­è®¡ç®—
                # è¿™é‡Œæ·»åŠ é¢å¤–çš„æ­£åˆ™åŒ–é¡¹
                vq_loss += F.mse_loss(emb.detach(), emb) * 0.25
            
            total_loss = recon_loss + vq_loss / len(ms_embeddings)
        else:
            total_loss = None
        
        if return_tokens:
            return reconstructed, ms_tokens, total_loss
        else:
            return reconstructed, total_loss
    
    def get_tokens(self, gene_pseudo_img: torch.Tensor) -> List[torch.Tensor]:
        """ä»…è·å–ç¦»æ•£tokensï¼Œç”¨äºVARè®­ç»ƒ"""
        ms_tokens, _ = self.encode(gene_pseudo_img)
        return ms_tokens
    
    def reconstruct_from_tokens(self, ms_tokens: List[torch.Tensor]) -> torch.Tensor:
        """ä»tokensé‡å»ºï¼Œç”¨äºVARæ¨ç†"""
        return self.decode(ms_tokens)


def test_multiscale_vqvae_st():
    """æµ‹è¯•å¤šå°ºåº¦VQVAE_ST"""
    print("ğŸ§ª æµ‹è¯•å¤šå°ºåº¦VQVAE_ST...")
    
    # æ¨¡å‹å‚æ•°
    gene_count = 225  # 15x15
    patch_nums = (1, 2, 3, 4, 5)
    batch_size = 4
    
    # åˆ›å»ºæ¨¡å‹
    model = MultiScaleVQVAE_ST(
        gene_count=gene_count,
        patch_nums=patch_nums,
        vocab_size=4096,
        embed_dim=256,
        hidden_dim=128,
        share_decoder=True
    )
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    gene_img = torch.randn(batch_size, 1, 15, 15)
    
    print(f"è¾“å…¥å½¢çŠ¶: {gene_img.shape}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    reconstructed, ms_tokens, loss = model(gene_img, return_tokens=True, return_loss=True)
    
    print(f"é‡å»ºå½¢çŠ¶: {reconstructed.shape}")
    print(f"é‡å»ºæŸå¤±: {loss:.4f}")
    print(f"å¤šå°ºåº¦tokensæ•°é‡: {len(ms_tokens)}")
    
    total_tokens = 0
    for i, tokens in enumerate(ms_tokens):
        pn = patch_nums[i]
        expected_tokens = pn * pn
        actual_tokens = tokens.shape[1]
        total_tokens += actual_tokens
        print(f"  å°ºåº¦{i+1} ({pn}x{pn}): {tokens.shape} - {actual_tokens} tokens (æœŸæœ›: {expected_tokens})")
    
    print(f"æ€»tokenæ•°: {total_tokens} (æœŸæœ›: {sum(pn*pn for pn in patch_nums)})")
    
    # æµ‹è¯•tokensé‡å»º
    reconstructed_from_tokens = model.reconstruct_from_tokens(ms_tokens)
    recon_diff = F.mse_loss(reconstructed, reconstructed_from_tokens)
    print(f"Tokené‡å»ºå·®å¼‚: {recon_diff:.6f}")
    
    print("âœ… å¤šå°ºåº¦VQVAE_STæµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    test_multiscale_vqvae_st() 