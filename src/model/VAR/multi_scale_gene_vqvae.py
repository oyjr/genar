"""
å¤šå°ºåº¦åŸºå› VQVAEæ¨¡å— - Stage 1è®­ç»ƒ

å®ç°åŸºäºç”Ÿç‰©å­¦å¤šå°ºåº¦çš„åŸºå› è¡¨è¾¾Vector-Quantized Variational AutoEncoderã€‚

æ ¸å¿ƒç‰¹æ€§ï¼š
1. ç”Ÿç‰©å­¦å¤šå°ºåº¦åˆ†è§£ï¼šGlobal(1) â†’ Pathway(8) â†’ Module(32) â†’ Individual(200)
2. å…±äº«é‡åŒ–å™¨ï¼šç¬¦åˆVARåŸå§‹è®¾è®¡ï¼Œå•ä¸€codebookï¼Œè¯æ±‡è¡¨å¤§å°4096
3. æ®‹å·®é‡å»ºç­–ç•¥ï¼šé€å±‚ç´¯ç§¯é‡å»ºï¼Œç¡®ä¿ä¿¡æ¯å®Œæ•´æ€§
4. ç‹¬ç«‹è®­ç»ƒï¼šStage 1åªéœ€è¦åŸºå› è¡¨è¾¾æ•°æ®ï¼Œæ— éœ€ç»„ç»‡å­¦ç‰¹å¾

è®­ç»ƒç›®æ ‡ï¼š
- å­¦ä¹ åŸºå› è¡¨è¾¾çš„å¤šå°ºåº¦ç¦»æ•£è¡¨ç¤º
- ä¸ºStage 2æä¾›ç¨³å®šçš„é‡åŒ–ç‰¹å¾
- ç”Ÿæˆç”¨äºVAR Transformerçš„tokens
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Tuple, Optional, Any

from .shared_components import (
    SharedVectorQuantizer,
    GlobalEncoder, PathwayEncoder, ModuleEncoder, IndividualEncoder,
    GlobalDecoder, PathwayDecoder, ModuleDecoder, IndividualDecoder,
    ResidualReconstructor,
    MultiScaleDecomposer
)


class MultiScaleGeneVQVAE(nn.Module):
    """
    å¤šå°ºåº¦åŸºå› VQVAE - Stage 1è®­ç»ƒçš„æ ¸å¿ƒæ¨¡å‹
    
    æ¶æ„æµç¨‹ï¼š
    1. è¾“å…¥: åŸºå› è¡¨è¾¾ [B, 200]
    2. å¤šå°ºåº¦åˆ†è§£: â†’ Global[1], Pathway[8], Module[32], Individual[200]
    3. åˆ†å±‚ç¼–ç : â†’ ç»Ÿä¸€128ç»´ç‰¹å¾è¡¨ç¤º
    4. å…±äº«é‡åŒ–: â†’ ç¦»æ•£tokens (ä»åŒä¸€codebook)
    5. åˆ†å±‚è§£ç : â†’ é‡å»ºå„å°ºåº¦ç‰¹å¾
    6. æ®‹å·®é‡å»º: â†’ æœ€ç»ˆåŸºå› è¡¨è¾¾ [B, 200]
    
    æŸå¤±å‡½æ•°ï¼š
    - æ€»é‡å»ºæŸå¤±ï¼šMSE(final_reconstruction, original_gene_expression)
    - åˆ†å±‚é‡å»ºæŸå¤±ï¼šå„å°ºåº¦é‡å»ºçš„MSEæŸå¤±
    - VQæŸå¤±ï¼šæ‰€æœ‰å°ºåº¦çš„Vector QuantizationæŸå¤±
    """
    
    def __init__(
        self,
        vocab_size: int = 4096,
        embed_dim: int = 128,
        beta: float = 0.25,
        hierarchical_loss_weight: float = 0.1,
        vq_loss_weight: float = 0.25
    ):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.beta = beta
        self.hierarchical_loss_weight = hierarchical_loss_weight
        self.vq_loss_weight = vq_loss_weight
        
        # 1. å¤šå°ºåº¦åˆ†è§£å™¨
        self.decomposer = MultiScaleDecomposer()
        
        # 2. å¤šå°ºåº¦ç¼–ç å™¨
        self.encoders = nn.ModuleDict({
            'global': GlobalEncoder(embed_dim=embed_dim),
            'pathway': PathwayEncoder(embed_dim=embed_dim),
            'module': ModuleEncoder(embed_dim=embed_dim),
            'individual': IndividualEncoder(embed_dim=embed_dim)
        })
        
        # 3. å…±äº«é‡åŒ–å™¨ (VARæ ¸å¿ƒè®¾è®¡)
        self.shared_quantizer = SharedVectorQuantizer(
            vocab_size=vocab_size,
            embed_dim=embed_dim,
            beta=beta
        )
        
        # 4. å¤šå°ºåº¦è§£ç å™¨
        self.decoders = nn.ModuleDict({
            'global': GlobalDecoder(embed_dim=embed_dim),
            'pathway': PathwayDecoder(embed_dim=embed_dim),
            'module': ModuleDecoder(embed_dim=embed_dim),
            'individual': IndividualDecoder(embed_dim=embed_dim)
        })
        
        # 5. æ®‹å·®é‡å»ºå™¨
        self.reconstructor = ResidualReconstructor()
        
        print(f"ğŸ§¬ MultiScaleGeneVQVAEåˆå§‹åŒ–:")
        print(f"   è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        print(f"   åµŒå…¥ç»´åº¦: {embed_dim}")
        print(f"   Î²å‚æ•°: {beta}")
        print(f"   åˆ†å±‚æŸå¤±æƒé‡: {hierarchical_loss_weight}")
        print(f"   VQæŸå¤±æƒé‡: {vq_loss_weight}")
    
    def encode(self, gene_expression: torch.Tensor) -> Dict[str, Any]:
        """
        ç¼–ç é˜¶æ®µï¼šåŸºå› è¡¨è¾¾ â†’ å¤šå°ºåº¦tokens
        
        Args:
            gene_expression: [B, 200] - è¾“å…¥åŸºå› è¡¨è¾¾
            
        Returns:
            åŒ…å«tokensã€é‡åŒ–ç‰¹å¾ã€VQæŸå¤±çš„å­—å…¸
        """
        # 1. å¤šå°ºåº¦åˆ†è§£
        decomposed = self.decomposer(gene_expression)
        
        # 2. å¤šå°ºåº¦ç¼–ç 
        encoded = {}
        for scale in ['global', 'pathway', 'module', 'individual']:
            encoded[scale] = self.encoders[scale](decomposed[scale])
        
        # 3. å…±äº«é‡åŒ–
        tokens = {}
        quantized = {}
        vq_losses = []
        
        for scale in ['global', 'pathway', 'module', 'individual']:
            scale_tokens, scale_quantized, scale_vq_loss = self.shared_quantizer(encoded[scale])
            tokens[scale] = scale_tokens
            quantized[scale] = scale_quantized
            vq_losses.append(scale_vq_loss)
        
        total_vq_loss = sum(vq_losses)
        
        return {
            'decomposed': decomposed,
            'encoded': encoded,
            'tokens': tokens,
            'quantized': quantized,
            'vq_loss': total_vq_loss,
            'scale_vq_losses': vq_losses
        }
    
    def decode(self, quantized: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """
        è§£ç é˜¶æ®µï¼šé‡åŒ–ç‰¹å¾ â†’ é‡å»ºåŸºå› è¡¨è¾¾
        
        Args:
            quantized: å„å°ºåº¦çš„é‡åŒ–ç‰¹å¾å­—å…¸
            
        Returns:
            åŒ…å«é‡å»ºç»“æœçš„å­—å…¸
        """
        # 1. å¤šå°ºåº¦è§£ç 
        decoded = {}
        for scale in ['global', 'pathway', 'module', 'individual']:
            decoded[scale] = self.decoders[scale](quantized[scale])
        
        # 2. æ®‹å·®é‡å»º
        reconstruction_result = self.reconstructor(
            decoded['global'], decoded['pathway'],
            decoded['module'], decoded['individual']
        )
        
        return {
            'decoded': decoded,
            'reconstruction_result': reconstruction_result,
            'final_reconstruction': reconstruction_result['final_reconstruction']
        }
    
    def decode_from_tokens(self, tokens: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """
        ä»tokensè§£ç ï¼štokens â†’ é‡å»ºåŸºå› è¡¨è¾¾
        
        Args:
            tokens: å„å°ºåº¦çš„tokenå­—å…¸
            
        Returns:
            åŒ…å«é‡å»ºç»“æœçš„å­—å…¸
        """
        # 1. ä»tokensè·å–é‡åŒ–ç‰¹å¾
        quantized = {}
        for scale in ['global', 'pathway', 'module', 'individual']:
            quantized[scale] = self.shared_quantizer.decode(tokens[scale])
        
        # 2. è§£ç 
        return self.decode(quantized)
    
    def forward(self, gene_expression: torch.Tensor) -> Dict[str, Any]:
        """
        å‰å‘ä¼ æ’­ï¼šåŸºå› è¡¨è¾¾ â†’ é‡å»ºåŸºå› è¡¨è¾¾
        
        Args:
            gene_expression: [B, 200] - è¾“å…¥åŸºå› è¡¨è¾¾
            
        Returns:
            åŒ…å«æ‰€æœ‰ä¸­é—´ç»“æœå’ŒæŸå¤±çš„å­—å…¸
        """
        # 1. ç¼–ç 
        encode_result = self.encode(gene_expression)
        
        # 2. è§£ç 
        decode_result = self.decode(encode_result['quantized'])
        
        # 3. è®¡ç®—æŸå¤±
        loss_result = self.compute_losses(
            original=gene_expression,
            encode_result=encode_result,
            decode_result=decode_result
        )
        
        # åˆå¹¶ç»“æœ
        result = {
            **encode_result,
            **decode_result,
            **loss_result
        }
        
        return result
    
    def compute_losses(
        self,
        original: torch.Tensor,
        encode_result: Dict[str, Any],
        decode_result: Dict[str, Any]
    ) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—å„ç§æŸå¤±å‡½æ•°
        
        Args:
            original: [B, 200] - åŸå§‹åŸºå› è¡¨è¾¾
            encode_result: ç¼–ç ç»“æœ
            decode_result: è§£ç ç»“æœ
            
        Returns:
            åŒ…å«å„ç§æŸå¤±çš„å­—å…¸
        """
        # 1. æ€»é‡å»ºæŸå¤± (æœ€é‡è¦)
        final_reconstruction = decode_result['final_reconstruction']
        total_reconstruction_loss = F.mse_loss(final_reconstruction, original)
        
        # 2. åˆ†å±‚é‡å»ºæŸå¤±
        decomposed = encode_result['decomposed']
        decoded = decode_result['decoded']
        
        hierarchical_losses = {}
        for scale in ['global', 'pathway', 'module', 'individual']:
            hierarchical_losses[f'{scale}_recon_loss'] = F.mse_loss(
                decoded[scale], decomposed[scale]
            )
        
        total_hierarchical_loss = sum(hierarchical_losses.values())
        
        # 3. VQæŸå¤±
        total_vq_loss = encode_result['vq_loss']
        
        # 4. æ€»æŸå¤±
        total_loss = (total_reconstruction_loss + 
                     self.hierarchical_loss_weight * total_hierarchical_loss +
                     self.vq_loss_weight * total_vq_loss)
        
        return {
            'total_loss': total_loss,
            'total_reconstruction_loss': total_reconstruction_loss,
            'total_hierarchical_loss': total_hierarchical_loss,
            'total_vq_loss': total_vq_loss,
            **hierarchical_losses
        }
    
    def get_quantization_info(self) -> Dict[str, Any]:
        """è·å–é‡åŒ–ä¿¡æ¯ï¼Œç”¨äºç›‘æ§è®­ç»ƒè¿‡ç¨‹"""
        codebook_usage = torch.zeros(self.vocab_size)
        return {
            'vocab_size': self.vocab_size,
            'embed_dim': self.embed_dim,
            'codebook_usage': codebook_usage,
            'utilization_rate': 0.0  # éœ€è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°
        }
    
    def update_codebook_usage(self, tokens: Dict[str, torch.Tensor]) -> float:
        """
        æ›´æ–°codebookä½¿ç”¨æƒ…å†µ
        
        Args:
            tokens: å„å°ºåº¦çš„tokenå­—å…¸
            
        Returns:
            codebookåˆ©ç”¨ç‡
        """
        # æ”¶é›†æ‰€æœ‰tokens
        all_tokens = torch.cat([
            tokens[scale].flatten() 
            for scale in ['global', 'pathway', 'module', 'individual']
        ])
        
        # ç»Ÿè®¡ä½¿ç”¨çš„tokenæ•°é‡
        unique_tokens = torch.unique(all_tokens)
        utilization_rate = len(unique_tokens) / self.vocab_size
        
        return utilization_rate
    
    @torch.no_grad()
    def generate_random_tokens(self, batch_size: int, device: torch.device) -> Dict[str, torch.Tensor]:
        """
        ç”Ÿæˆéšæœºtokensï¼Œç”¨äºæµ‹è¯•å’ŒéªŒè¯
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            device: è®¾å¤‡
            
        Returns:
            éšæœºtokenå­—å…¸
        """
        tokens = {}
        
        # ç”Ÿæˆå„å°ºåº¦çš„éšæœºtokens
        scale_shapes = {
            'global': (batch_size, 1),
            'pathway': (batch_size, 8),
            'module': (batch_size, 32),
            'individual': (batch_size, 200)
        }
        
        for scale, shape in scale_shapes.items():
            tokens[scale] = torch.randint(
                low=0, high=self.vocab_size,
                size=shape, device=device, dtype=torch.long
            )
        
        return tokens
    
    def save_stage1_checkpoint(self, path: str, epoch: int, optimizer_state: Optional[dict] = None) -> None:
        """
        ä¿å­˜Stage 1è®­ç»ƒcheckpoint
        
        Args:
            path: ä¿å­˜è·¯å¾„
            epoch: å½“å‰epoch
            optimizer_state: ä¼˜åŒ–å™¨çŠ¶æ€
        """
        checkpoint = {
            'model_state_dict': self.state_dict(),
            'model_config': {
                'vocab_size': self.vocab_size,
                'embed_dim': self.embed_dim,
                'beta': self.beta,
                'hierarchical_loss_weight': self.hierarchical_loss_weight,
                'vq_loss_weight': self.vq_loss_weight
            },
            'epoch': epoch,
            'stage': 'stage1_vqvae'
        }
        
        if optimizer_state is not None:
            checkpoint['optimizer_state_dict'] = optimizer_state
        
        torch.save(checkpoint, path)
        print(f"ğŸ’¾ Stage 1 checkpointä¿å­˜åˆ°: {path}")
    
    @classmethod
    def load_stage1_checkpoint(cls, path: str, device: torch.device) -> Tuple['MultiScaleGeneVQVAE', dict]:
        """
        åŠ è½½Stage 1è®­ç»ƒcheckpoint
        
        Args:
            path: checkpointè·¯å¾„
            device: ç›®æ ‡è®¾å¤‡
            
        Returns:
            (model, checkpoint_info)
        """
        checkpoint = torch.load(path, map_location=device)
        
        # é‡å»ºæ¨¡å‹
        model_config = checkpoint['model_config']
        model = cls(**model_config)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(device)
        
        checkpoint_info = {
            'epoch': checkpoint['epoch'],
            'stage': checkpoint['stage'],
            'optimizer_state_dict': checkpoint.get('optimizer_state_dict', None)
        }
        
        print(f"ğŸ“‚ Stage 1 checkpointä» {path} åŠ è½½å®Œæˆ")
        return model, checkpoint_info


class Stage1Trainer:
    """
    Stage 1è®­ç»ƒå™¨ - ä¸“é—¨ç”¨äºè®­ç»ƒå¤šå°ºåº¦åŸºå› VQVAE
    
    è®­ç»ƒç‰¹ç‚¹ï¼š
    1. åªéœ€è¦åŸºå› è¡¨è¾¾æ•°æ®
    2. æ‰¹æ¬¡å¤„ç†spotçº§åˆ«æ ·æœ¬
    3. ç›‘æ§VQæŸå¤±å’Œé‡å»ºç²¾åº¦
    4. æ£€æŸ¥codebookåˆ©ç”¨ç‡
    """
    
    def __init__(
        self,
        model: MultiScaleGeneVQVAE,
        optimizer: torch.optim.Optimizer,
        device: torch.device,
        print_freq: int = 100
    ):
        self.model = model
        self.optimizer = optimizer
        self.device = device
        self.print_freq = print_freq
        
        # è®­ç»ƒç»Ÿè®¡
        self.epoch_losses = []
        self.codebook_utilizations = []
    
    def train_epoch(self, dataloader, epoch: int) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€ä¸ªepoch
        
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨ (åªåŒ…å«åŸºå› è¡¨è¾¾æ•°æ®)
            epoch: å½“å‰epoch
            
        Returns:
            å¹³å‡æŸå¤±å­—å…¸
        """
        self.model.train()
        
        epoch_losses = {
            'total_loss': 0.0,
            'reconstruction_loss': 0.0,
            'hierarchical_loss': 0.0,
            'vq_loss': 0.0
        }
        
        epoch_utilizations = []
        num_batches = len(dataloader)
        
        for batch_idx, batch in enumerate(dataloader):
            # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
            if isinstance(batch, (list, tuple)):
                gene_expressions = batch[0]  # å–ç¬¬ä¸€ä¸ªå…ƒç´ ä½œä¸ºåŸºå› è¡¨è¾¾æ•°æ®
            else:
                gene_expressions = batch
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            gene_expressions = gene_expressions.to(self.device)  # [B, 200]
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            result = self.model(gene_expressions)
            
            # åå‘ä¼ æ’­
            total_loss = result['total_loss']
            total_loss.backward()
            self.optimizer.step()
            
            # ç´¯ç§¯æŸå¤±
            epoch_losses['total_loss'] += total_loss.item()
            epoch_losses['reconstruction_loss'] += result['total_reconstruction_loss'].item()
            epoch_losses['hierarchical_loss'] += result['total_hierarchical_loss'].item()
            epoch_losses['vq_loss'] += result['total_vq_loss'].item()
            
            # è®¡ç®—codebookåˆ©ç”¨ç‡
            utilization = self.model.update_codebook_usage(result['tokens'])
            epoch_utilizations.append(utilization)
            
            # æ‰“å°è¿›åº¦
            if batch_idx % self.print_freq == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}/{num_batches}: "
                      f"Loss={total_loss.item():.4f}, "
                      f"Recon={result['total_reconstruction_loss'].item():.4f}, "
                      f"VQ={result['total_vq_loss'].item():.4f}, "
                      f"Util={utilization:.3f}")
        
        # è®¡ç®—å¹³å‡å€¼
        for key in epoch_losses:
            epoch_losses[key] /= num_batches
        
        avg_utilization = sum(epoch_utilizations) / len(epoch_utilizations)
        epoch_losses['codebook_utilization'] = avg_utilization
        
        # ä¿å­˜ç»Ÿè®¡
        self.epoch_losses.append(epoch_losses)
        self.codebook_utilizations.append(avg_utilization)
        
        return epoch_losses
    
    @torch.no_grad()
    def validate_epoch(self, dataloader, epoch: int) -> Dict[str, float]:
        """
        éªŒè¯ä¸€ä¸ªepoch
        
        Args:
            dataloader: éªŒè¯æ•°æ®åŠ è½½å™¨
            epoch: å½“å‰epoch
            
        Returns:
            éªŒè¯æŸå¤±å­—å…¸
        """
        self.model.eval()
        
        val_losses = {
            'total_loss': 0.0,
            'reconstruction_loss': 0.0,
            'hierarchical_loss': 0.0,
            'vq_loss': 0.0
        }
        
        val_utilizations = []
        num_batches = len(dataloader)
        
        for batch in dataloader:
            # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
            if isinstance(batch, (list, tuple)):
                gene_expressions = batch[0]  # å–ç¬¬ä¸€ä¸ªå…ƒç´ ä½œä¸ºåŸºå› è¡¨è¾¾æ•°æ®
            else:
                gene_expressions = batch
            
            gene_expressions = gene_expressions.to(self.device)
            
            # å‰å‘ä¼ æ’­
            result = self.model(gene_expressions)
            
            # ç´¯ç§¯æŸå¤±
            val_losses['total_loss'] += result['total_loss'].item()
            val_losses['reconstruction_loss'] += result['total_reconstruction_loss'].item()
            val_losses['hierarchical_loss'] += result['total_hierarchical_loss'].item()
            val_losses['vq_loss'] += result['total_vq_loss'].item()
            
            # è®¡ç®—åˆ©ç”¨ç‡
            utilization = self.model.update_codebook_usage(result['tokens'])
            val_utilizations.append(utilization)
        
        # è®¡ç®—å¹³å‡å€¼
        for key in val_losses:
            val_losses[key] /= num_batches
        
        avg_utilization = sum(val_utilizations) / len(val_utilizations)
        val_losses['codebook_utilization'] = avg_utilization
        
        return val_losses
    
    def get_training_stats(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        return {
            'epoch_losses': self.epoch_losses,
            'codebook_utilizations': self.codebook_utilizations,
            'num_epochs_trained': len(self.epoch_losses)
        } 