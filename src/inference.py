#!/usr/bin/env python3
"""
VAR_STæ¨¡å‹æ¨ç†è„šæœ¬
ç”¨äºåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹checkpointå¹¶å¯¹æŒ‡å®šæ ·æœ¬è¿›è¡Œæ¨ç†æµ‹è¯•
"""

import os
import sys
import argparse
import logging
from datetime import datetime
from pathlib import Path

# ç¡®ä¿å¯¼å…¥é¡¹ç›®ç›®å½•ä¸‹çš„æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import torch
import numpy as np
import pandas as pd
import pytorch_lightning as pl
from torch.utils.data import DataLoader

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from dataset.hest_dataset import STDataset
from model import ModelInterface
from model.model_metrics import ModelMetrics
from utils import fix_seed

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æ•°æ®é›†é…ç½®
DATASETS = {
    'PRAD': {
        'path': '/data/ouyangjiarui/stem/hest1k_datasets/PRAD/',
        'val_slides': 'MEND144',
        'test_slides': 'MEND144',
        'recommended_encoder': 'uni'
    },
    'her2st': {
        'path': '/data/ouyangjiarui/stem/hest1k_datasets/her2st/',
        'val_slides': 'SPA148',
        'test_slides': 'SPA148', 
        'recommended_encoder': 'conch'
    }
}

# ç¼–ç å™¨ç‰¹å¾ç»´åº¦æ˜ å°„
ENCODER_FEATURE_DIMS = {
    'uni': 1024,
    'conch': 512
}


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='VAR_STæ¨¡å‹æ¨ç†è„šæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¯¹PRADæ•°æ®é›†çš„MEND144æ ·æœ¬è¿›è¡Œæ¨ç†
  python src/inference.py \\
      --ckpt_path logs/PRAD/VAR_ST/best-epoch=epoch=01-val_loss_final=val_loss_final=101.7450.ckpt \\
      --dataset PRAD \\
      --slide_id MEND144 \\
      --output_dir ./inference_results
      
  # ä½¿ç”¨GPUè¿›è¡Œæ¨ç†
  python src/inference.py \\
      --ckpt_path your_checkpoint.ckpt \\
      --dataset PRAD \\
      --slide_id MEND144 \\
      --gpu_id 0
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--ckpt_path', type=str, required=True,
                        help='æ¨¡å‹checkpointæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--dataset', type=str, required=True, choices=list(DATASETS.keys()),
                        help='æ•°æ®é›†åç§° (PRAD, her2st)')
    parser.add_argument('--slide_id', type=str, required=True,
                        help='è¦æ¨ç†çš„slide ID (å¦‚: MEND144)')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--encoder', type=str, choices=list(ENCODER_FEATURE_DIMS.keys()),
                        help='ç¼–ç å™¨ç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨æ•°æ®é›†æ¨èç¼–ç å™¨')
    parser.add_argument('--output_dir', type=str, default='./inference_results',
                        help='ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: ./inference_results)')
    parser.add_argument('--gpu_id', type=int, default=0,
                        help='ä½¿ç”¨çš„GPU ID (é»˜è®¤: 0, -1è¡¨ç¤ºä½¿ç”¨CPU)')
    parser.add_argument('--batch_size', type=int, default=64,
                        help='æ¨ç†æ‰¹æ¬¡å¤§å° (é»˜è®¤: 64)')
    parser.add_argument('--max_gene_count', type=int, default=500,
                        help='æœ€å¤§åŸºå› è®¡æ•°å€¼ (é»˜è®¤: 500)')
    parser.add_argument('--seed', type=int, default=2021,
                        help='éšæœºç§å­ (é»˜è®¤: 2021)')
    parser.add_argument('--save_predictions', action='store_true',
                        help='æ˜¯å¦ä¿å­˜è¯¦ç»†çš„é¢„æµ‹ç»“æœåˆ°æ–‡ä»¶')
    
    return parser.parse_args()


def setup_device(gpu_id: int):
    """è®¾ç½®è®¡ç®—è®¾å¤‡"""
    if gpu_id == -1:
        device = torch.device('cpu')
        logger.info("ä½¿ç”¨CPUè¿›è¡Œæ¨ç†")
    else:
        if torch.cuda.is_available():
            device = torch.device(f'cuda:{gpu_id}')
            logger.info(f"ä½¿ç”¨GPU {gpu_id}è¿›è¡Œæ¨ç†")
        else:
            device = torch.device('cpu')
            logger.warning("CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè¿›è¡Œæ¨ç†")
    
    return device


def load_model_from_checkpoint(ckpt_path: str, device: torch.device):
    """ä»checkpointåŠ è½½æ¨¡å‹"""
    logger.info(f"ä»checkpointåŠ è½½æ¨¡å‹: {ckpt_path}")
    
    if not os.path.exists(ckpt_path):
        raise FileNotFoundError(f"Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {ckpt_path}")
    
    # åŠ è½½checkpoint
    checkpoint = torch.load(ckpt_path, map_location=device)
    
    # ä»checkpointä¸­æå–é…ç½®ä¿¡æ¯
    if 'hyper_parameters' not in checkpoint:
        raise ValueError("Checkpointä¸­ç¼ºå°‘hyper_parametersä¿¡æ¯")
    
    config = checkpoint['hyper_parameters']['config']
    logger.info(f"åŠ è½½çš„æ¨¡å‹é…ç½®: {config.MODEL.model_name}")
    
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = ModelInterface(config)
    
    # åŠ è½½æƒé‡
    model.load_state_dict(checkpoint['state_dict'])
    model.to(device)
    model.eval()
    
    logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")
    return model, config


def create_test_dataloader(config, slide_id: str, batch_size: int = 64):
    """åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    logger.info(f"åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼Œslide_id: {slide_id}")
    
    # åŸºç¡€å‚æ•°
    base_params = {
        'data_path': config.data_path,
        'expr_name': config.expr_name,
        'slide_val': slide_id,  # å°†æŒ‡å®šslideä½œä¸ºéªŒè¯é›†
        'slide_test': slide_id,  # å°†æŒ‡å®šslideä½œä¸ºæµ‹è¯•é›†
        'encoder_name': config.encoder_name,
        'use_augmented': False,  # æ¨ç†æ—¶ä¸ä½¿ç”¨æ•°æ®å¢å¼º
        'max_gene_count': getattr(config, 'max_gene_count', 500),
    }
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    test_dataset = STDataset(mode='test', expand_augmented=False, **base_params)
    
    # åˆ›å»ºDataLoader
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    logger.info(f"æµ‹è¯•æ•°æ®é›†å¤§å°: {len(test_dataset)} ä¸ªæ ·æœ¬")
    return test_loader, test_dataset


def run_inference(model, test_loader, device: torch.device):
    """è¿è¡Œæ¨ç†"""
    logger.info("å¼€å§‹æ¨ç†...")
    
    model.eval()
    all_predictions = []
    all_targets = []
    all_losses = []
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_loader):
            # å°†æ•°æ®ç§»åˆ°è®¾å¤‡ä¸Š
            for key, value in batch.items():
                if torch.is_tensor(value):
                    batch[key] = value.to(device)
            
            # é¢„å¤„ç†è¾“å…¥
            processed_batch = model.model_utils.preprocess_inputs(batch)
            
            # æ¨ç†æ¨¡å¼ï¼šä¸ä½¿ç”¨teacher forcing
            inference_batch = processed_batch.copy()
            if 'target_genes' in inference_batch:
                targets = inference_batch.pop('target_genes')
            else:
                targets = batch['target_genes']
            
            # æ‰§è¡Œæ¨ç†
            results = model.model(**inference_batch, top_k=1)  # ä½¿ç”¨top-k=1è¿›è¡Œç¡®å®šæ€§æ¨ç†
            
            # è·å–é¢„æµ‹ç»“æœ
            if 'predictions' in results:
                predictions = results['predictions']
            elif 'generated_sequence' in results:
                predictions = results['generated_sequence']
            else:
                raise ValueError("æ¨¡å‹è¾“å‡ºä¸­æ‰¾ä¸åˆ°é¢„æµ‹ç»“æœ")
            
            # è®¡ç®—æŸå¤±ï¼ˆç”¨äºç›‘æ§ï¼‰
            loss_batch = processed_batch.copy()
            loss_results = model.model(**loss_batch)
            loss = model._compute_loss(loss_results, batch)
            
            # æ”¶é›†ç»“æœ
            all_predictions.append(predictions.cpu())
            all_targets.append(targets.cpu())
            all_losses.append(loss.item())
            
            if (batch_idx + 1) % 10 == 0:
                logger.info(f"å·²å¤„ç† {batch_idx + 1}/{len(test_loader)} ä¸ªæ‰¹æ¬¡")
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    predictions = torch.cat(all_predictions, dim=0)  # [N, 200]
    targets = torch.cat(all_targets, dim=0)  # [N, 200]
    avg_loss = np.mean(all_losses)
    
    logger.info(f"æ¨ç†å®Œæˆï¼Œæ€»æ ·æœ¬æ•°: {predictions.shape[0]}, å¹³å‡æŸå¤±: {avg_loss:.6f}")
    
    return predictions, targets, avg_loss


def calculate_detailed_metrics(predictions: torch.Tensor, targets: torch.Tensor):
    """è®¡ç®—è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡"""
    logger.info("è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    if torch.is_tensor(predictions):
        predictions = predictions.numpy()
    if torch.is_tensor(targets):
        targets = targets.numpy()
    
    # åˆ›å»ºModelMetricså®ä¾‹è¿›è¡Œè®¡ç®—
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„é…ç½®å¯¹è±¡
    class SimpleConfig:
        def __init__(self):
            self.MODEL = type('obj', (object,), {'num_genes': 200})()
    
    config = SimpleConfig()
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„lightning_moduleæ¨¡æ‹Ÿå¯¹è±¡
    class SimpleLightningModule:
        def log(self, *args, **kwargs):
            pass
    
    lightning_module = SimpleLightningModule()
    
    # åˆ›å»ºModelMetricså®ä¾‹
    model_metrics = ModelMetrics(config, lightning_module)
    
    # è®¡ç®—PCCæŒ‡æ ‡ - åº”ç”¨log2å˜æ¢
    pcc_metrics = model_metrics.calculate_comprehensive_pcc_metrics(
        predictions, targets, apply_log2=True
    )
    
    # è®¡ç®—é¢å¤–çš„ç»Ÿè®¡æŒ‡æ ‡
    # åŸå§‹æ•°æ®ç»Ÿè®¡
    pred_stats = {
        'pred_mean': float(np.mean(predictions)),
        'pred_std': float(np.std(predictions)),
        'pred_min': float(np.min(predictions)),
        'pred_max': float(np.max(predictions)),
    }
    
    target_stats = {
        'target_mean': float(np.mean(targets)),
        'target_std': float(np.std(targets)),
        'target_min': float(np.min(targets)),
        'target_max': float(np.max(targets)),
    }
    
    # åŸºå› çº§åˆ«çš„ç›¸å…³æ€§åˆ†æ
    gene_correlations = model_metrics.calculate_gene_correlations(targets, predictions)
    
    # è®¡ç®—æ¯ä¸ªåŸºå› çš„ç»Ÿè®¡ä¿¡æ¯
    gene_stats = []
    for i in range(predictions.shape[1]):
        gene_pred = predictions[:, i]
        gene_target = targets[:, i]
        
        gene_stat = {
            'gene_idx': i,
            'correlation': float(gene_correlations[i]),
            'pred_mean': float(np.mean(gene_pred)),
            'target_mean': float(np.mean(gene_target)),
            'pred_std': float(np.std(gene_pred)),
            'target_std': float(np.std(gene_target)),
        }
        gene_stats.append(gene_stat)
    
    # æ’åºåŸºå› ç›¸å…³æ€§
    sorted_gene_stats = sorted(gene_stats, key=lambda x: x['correlation'], reverse=True)
    
    return {
        'pcc_metrics': pcc_metrics,
        'pred_stats': pred_stats,
        'target_stats': target_stats,
        'gene_correlations': gene_correlations,
        'gene_stats': gene_stats,
        'sorted_gene_stats': sorted_gene_stats
    }


def print_results(metrics: dict, avg_loss: float):
    """æ‰“å°æ¨ç†ç»“æœ"""
    pcc_metrics = metrics['pcc_metrics']
    pred_stats = metrics['pred_stats']
    target_stats = metrics['target_stats']
    sorted_gene_stats = metrics['sorted_gene_stats']
    
    print("\n" + "="*60)
    print("ğŸ¯ VAR_STæ¨¡å‹æ¨ç†ç»“æœ")
    print("="*60)
    
    # ä¸»è¦æŒ‡æ ‡
    print(f"\nğŸ“Š ä¸»è¦è¯„ä¼°æŒ‡æ ‡:")
    print(f"   æŸå¤± (Loss):      {avg_loss:.6f}")
    print(f"   PCC-10:          {pcc_metrics['pcc_10']:.4f}")
    print(f"   PCC-50:          {pcc_metrics['pcc_50']:.4f}")
    print(f"   PCC-200:         {pcc_metrics['pcc_200']:.4f}")
    print(f"   MSE:             {pcc_metrics['mse']:.6f}")
    print(f"   MAE:             {pcc_metrics['mae']:.6f}")
    print(f"   RVD:             {pcc_metrics['rvd']:.6f}")
    
    # æ•°æ®ç»Ÿè®¡
    print(f"\nğŸ“ˆ é¢„æµ‹å€¼ç»Ÿè®¡:")
    print(f"   å‡å€¼:            {pred_stats['pred_mean']:.2f}")
    print(f"   æ ‡å‡†å·®:          {pred_stats['pred_std']:.2f}")
    print(f"   èŒƒå›´:            [{pred_stats['pred_min']:.2f}, {pred_stats['pred_max']:.2f}]")
    
    print(f"\nğŸ“ˆ çœŸå®å€¼ç»Ÿè®¡:")
    print(f"   å‡å€¼:            {target_stats['target_mean']:.2f}")
    print(f"   æ ‡å‡†å·®:          {target_stats['target_std']:.2f}")
    print(f"   èŒƒå›´:            [{target_stats['target_min']:.2f}, {target_stats['target_max']:.2f}]")
    
    # Topè¡¨ç°åŸºå› 
    print(f"\nğŸ† Top-10è¡¨ç°æœ€ä½³åŸºå› :")
    for i, gene_stat in enumerate(sorted_gene_stats[:10]):
        print(f"   {i+1:2d}. åŸºå› {gene_stat['gene_idx']:3d}: PCC={gene_stat['correlation']:.4f}")
    
    # Bottomè¡¨ç°åŸºå› 
    print(f"\nâš ï¸  Bottom-5è¡¨ç°æœ€å·®åŸºå› :")
    for i, gene_stat in enumerate(sorted_gene_stats[-5:]):
        rank = len(sorted_gene_stats) - 4 + i
        print(f"   {rank:2d}. åŸºå› {gene_stat['gene_idx']:3d}: PCC={gene_stat['correlation']:.4f}")
    
    print("\n" + "="*60)


def save_results(metrics: dict, predictions: torch.Tensor, targets: torch.Tensor, 
                avg_loss: float, output_dir: str, slide_id: str, save_predictions: bool = False):
    """ä¿å­˜æ¨ç†ç»“æœ"""
    logger.info(f"ä¿å­˜ç»“æœåˆ°: {output_dir}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜ä¸»è¦æŒ‡æ ‡
    results_summary = {
        'slide_id': slide_id,
        'timestamp': datetime.now().isoformat(),
        'num_samples': predictions.shape[0],
        'num_genes': predictions.shape[1],
        'avg_loss': avg_loss,
        **metrics['pcc_metrics'],
        **metrics['pred_stats'],
        **metrics['target_stats']
    }
    
    summary_file = os.path.join(output_dir, f'{slide_id}_inference_summary.json')
    import json
    with open(summary_file, 'w') as f:
        json.dump(results_summary, f, indent=2)
    
    # ä¿å­˜åŸºå› çº§åˆ«ç»Ÿè®¡
    gene_stats_df = pd.DataFrame(metrics['gene_stats'])
    gene_stats_file = os.path.join(output_dir, f'{slide_id}_gene_statistics.csv')
    gene_stats_df.to_csv(gene_stats_file, index=False)
    
    # å¯é€‰ï¼šä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
    if save_predictions:
        predictions_file = os.path.join(output_dir, f'{slide_id}_predictions.npz')
        np.savez_compressed(
            predictions_file,
            predictions=predictions.numpy() if torch.is_tensor(predictions) else predictions,
            targets=targets.numpy() if torch.is_tensor(targets) else targets
        )
        logger.info(f"è¯¦ç»†é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {predictions_file}")
    
    logger.info(f"ç»“æœæ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
    logger.info(f"åŸºå› ç»Ÿè®¡å·²ä¿å­˜åˆ°: {gene_stats_file}")


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # è®¾ç½®éšæœºç§å­
    fix_seed(args.seed)
    
    # è®¾ç½®è®¾å¤‡
    device = setup_device(args.gpu_id)
    
    # æ£€æŸ¥checkpointæ–‡ä»¶
    if not os.path.exists(args.ckpt_path):
        logger.error(f"Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {args.ckpt_path}")
        return
    
    # æ£€æŸ¥æ•°æ®é›†é…ç½®
    if args.dataset not in DATASETS:
        logger.error(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {args.dataset}")
        return
    
    dataset_info = DATASETS[args.dataset]
    
    # ç¡®å®šç¼–ç å™¨
    encoder_name = args.encoder or dataset_info['recommended_encoder']
    
    logger.info(f"æ¨ç†é…ç½®:")
    logger.info(f"  æ•°æ®é›†: {args.dataset}")
    logger.info(f"  Slide ID: {args.slide_id}")
    logger.info(f"  ç¼–ç å™¨: {encoder_name}")
    logger.info(f"  Checkpoint: {args.ckpt_path}")
    logger.info(f"  è¾“å‡ºç›®å½•: {args.output_dir}")
    
    try:
        # åŠ è½½æ¨¡å‹
        model, config = load_model_from_checkpoint(args.ckpt_path, device)
        
        # æ›´æ–°é…ç½®ä¸­çš„æ•°æ®é›†ä¿¡æ¯
        config.data_path = dataset_info['path']
        config.expr_name = args.dataset
        config.encoder_name = encoder_name
        config.max_gene_count = args.max_gene_count
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
        test_loader, test_dataset = create_test_dataloader(config, args.slide_id, args.batch_size)
        
        # è¿è¡Œæ¨ç†
        predictions, targets, avg_loss = run_inference(model, test_loader, device)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics = calculate_detailed_metrics(predictions, targets)
        
        # æ‰“å°ç»“æœ
        print_results(metrics, avg_loss)
        
        # ä¿å­˜ç»“æœ
        save_results(metrics, predictions, targets, avg_loss, 
                    args.output_dir, args.slide_id, args.save_predictions)
        
        logger.info("æ¨ç†å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code) 