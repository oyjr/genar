import os
import pandas as pd
import numpy as np
from scipy import sparse
import torch
import scanpy as sc
import anndata as ad
from scipy.ndimage import gaussian_filter
from torch.utils.data import Dataset
from torchvision import transforms
from typing import List, Dict, Tuple, Optional, Union


class STDataset(Dataset):
    def __init__(self,
                 mode: str,                    # 'train', 'val', 'test'
                 data_path: str,               # æ•°æ®é›†æ ¹è·¯å¾„
                 expr_name: str,               # æ•°æ®é›†åç§°
                 slide_val: str = '',          # éªŒè¯é›†slides
                 slide_test: str = '',         # æµ‹è¯•é›†slides
                 encoder_name: str = 'uni',    # ç¼–ç å™¨ç±»å‹
                 use_augmented: bool = False,  # æ˜¯å¦ä½¿ç”¨å¢å¼º
                 expand_augmented: bool = False,  # æ˜¯å¦å±•å¼€å¢å¼ºä¸ºå¤šä¸ªæ ·æœ¬
                 normalize: bool = True,       # æ•°æ®å½’ä¸€åŒ–
                 cpm: bool = True,            # CPMå½’ä¸€åŒ–
                 smooth: bool = True):        # é«˜æ–¯å¹³æ»‘
        """
        ç©ºé—´è½¬å½•ç»„å­¦æ•°æ®é›†
        
        Args:
            mode: æ•°æ®æ¨¡å¼ ('train', 'val', 'test')
            data_path: æ•°æ®é›†æ ¹è·¯å¾„
            expr_name: æ•°æ®é›†åç§° (å¦‚ 'PRAD')
            slide_val: éªŒè¯é›†slide IDsï¼Œé€—å·åˆ†éš”
            slide_test: æµ‹è¯•é›†slide IDsï¼Œé€—å·åˆ†éš”
            encoder_name: ç¼–ç å™¨ç±»å‹ ('uni' æˆ– 'conch')
            use_augmented: æ˜¯å¦ä½¿ç”¨å¢å¼ºåµŒå…¥æ–‡ä»¶
            expand_augmented: æ˜¯å¦å°†3Då¢å¼ºåµŒå…¥å±•å¼€ä¸º7å€è®­ç»ƒæ ·æœ¬
                - True: æ¯ä¸ªspotå˜æˆ7ä¸ªè®­ç»ƒæ ·æœ¬ (çœŸæ­£çš„æ•°æ®å¢å¼º)
                - False: åªä½¿ç”¨ç¬¬ä¸€ä¸ªå¢å¼ºç‰ˆæœ¬ (åŸå›¾)
            normalize: æ˜¯å¦è¿›è¡Œæ•°æ®å½’ä¸€åŒ–
            cpm: æ˜¯å¦è¿›è¡ŒCPMå½’ä¸€åŒ–
            smooth: æ˜¯å¦è¿›è¡Œé«˜æ–¯å¹³æ»‘
        """
        super(STDataset, self).__init__()
        
        # éªŒè¯è¾“å…¥å‚æ•°
        if mode not in ['train', 'val', 'test']:
            raise ValueError(f"mode must be one of ['train', 'val', 'test'], but got {mode}")
        
        if encoder_name not in ['uni', 'conch']:
            raise ValueError(f"encoder_name must be one of ['uni', 'conch'], but got {encoder_name}")
        
        # expand_augmentedåªåœ¨use_augmented=Trueä¸”mode='train'æ—¶æœ‰æ•ˆ
        if expand_augmented and not use_augmented:
            print("âš ï¸  è­¦å‘Š: expand_augmented=Trueä½†use_augmented=Falseï¼Œå°†è¢«å¿½ç•¥")
            expand_augmented = False
        
        if expand_augmented and mode != 'train':
            print("âš ï¸  è­¦å‘Š: expand_augmentedåªåœ¨è®­ç»ƒæ¨¡å¼æœ‰æ•ˆï¼Œå…¶ä»–æ¨¡å¼å°†ä½¿ç”¨ç¬¬ä¸€ä¸ªå¢å¼ºç‰ˆæœ¬")
            expand_augmented = False
        
        self.mode = mode
        self.data_path = data_path
        self.expr_name = expr_name
        self.encoder_name = encoder_name
        self.use_augmented = use_augmented
        self.expand_augmented = expand_augmented
        self.norm_param = {'normalize': normalize, 'cpm': cpm, 'smooth': smooth}
        
        # æ„å»ºè·¯å¾„
        self.st_dir = f"{data_path}st"
        self.processed_dir = f"{data_path}processed_data"
        
        # æ„å»ºåµŒå…¥è·¯å¾„
        emb_suffix = "_aug" if use_augmented else ""
        self.emb_dir = f"{self.processed_dir}/1spot_{encoder_name}_ebd{emb_suffix}"
        
        # æ‰“å°åˆå§‹åŒ–ä¿¡æ¯
        print(f"ğŸ”§ åˆå§‹åŒ–STDataset:")
        print(f"  - æ¨¡å¼: {mode}")
        print(f"  - æ•°æ®è·¯å¾„: {data_path}")
        print(f"  - æ•°æ®é›†åç§°: {expr_name}")
        print(f"  - ç¼–ç å™¨: {encoder_name}")
        print(f"  - ä½¿ç”¨å¢å¼º: {use_augmented}")
        
        if self.expand_augmented:
            print(f"  - ğŸš€ å¢å¼ºæ¨¡å¼: 7å€æ ·æœ¬å±•å¼€ (æ¯ä¸ªspotå˜æˆ7ä¸ªè®­ç»ƒæ ·æœ¬)")
        elif self.use_augmented:
            print(f"  - ğŸ“Š å¢å¼ºæ¨¡å¼: åªä½¿ç”¨ç¬¬ä¸€ä¸ªå¢å¼ºç‰ˆæœ¬ (åŸå›¾)")
        else:
            print(f"  - ğŸ”§ æ ‡å‡†æ¨¡å¼: ä½¿ç”¨åŸå§‹2DåµŒå…¥")
        
        print(f"  - STç›®å½•: {self.st_dir}")
        print(f"  - åµŒå…¥ç›®å½•: {self.emb_dir}")
        
        # åŠ è½½åŸºå› åˆ—è¡¨
        self.genes = self.load_gene_list()
        print(f"  - åŠ è½½åŸºå› æ•°é‡: {len(self.genes)}")
        
        # åŠ è½½å’Œåˆ’åˆ†slides
        self.slide_splits = self.load_slide_splits(slide_val, slide_test)
        self.ids = self.slide_splits[mode]
        
        print(f"  - {mode}é›†slideæ•°é‡: {len(self.ids)}")
        print(f"  - {mode}é›†slides: {self.ids}")
        
        self.int2id = dict(enumerate(self.ids))
        
        # æ ¹æ®æ¨¡å¼åˆå§‹åŒ–
        if self.mode == 'train':
            self._init_train_mode()
        
        print(f"âœ… STDatasetåˆå§‹åŒ–å®Œæˆ")

    def load_gene_list(self) -> List[str]:
        """ä»selected_gene_list.txtè¯»å–åŸºå› åˆ—è¡¨"""
        gene_file = f"{self.processed_dir}/selected_gene_list.txt"
        
        if not os.path.exists(gene_file):
            raise FileNotFoundError(f"åŸºå› åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {gene_file}")
        
        try:
            with open(gene_file, 'r', encoding='utf-8') as f:
                genes = [line.strip() for line in f.readlines() if line.strip()]
            
            if len(genes) == 0:
                raise ValueError(f"åŸºå› åˆ—è¡¨ä¸ºç©º: {gene_file}")
            
            print(f"ä»{gene_file}åŠ è½½{len(genes)}ä¸ªåŸºå› ")
            return genes
            
        except UnicodeDecodeError as e:
            raise ValueError(f"åŸºå› åˆ—è¡¨æ–‡ä»¶ç¼–ç é”™è¯¯: {gene_file}, é”™è¯¯: {e}")
        except PermissionError as e:
            raise PermissionError(f"æ²¡æœ‰æƒé™è¯»å–åŸºå› åˆ—è¡¨æ–‡ä»¶: {gene_file}, é”™è¯¯: {e}")
        except IOError as e:
            raise IOError(f"è¯»å–åŸºå› åˆ—è¡¨æ–‡ä»¶æ—¶å‘ç”ŸIOé”™è¯¯: {gene_file}, é”™è¯¯: {e}")

    def load_slide_splits(self, slide_val: str, slide_test: str) -> Dict[str, List[str]]:
        """åŠ è½½å’Œåˆ’åˆ†slides"""
        # è¯»å–æ‰€æœ‰slideåˆ—è¡¨
        slide_file = f"{self.processed_dir}/all_slide_lst.txt"

        if not os.path.exists(slide_file):
            raise FileNotFoundError(f"Slideåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {slide_file}")
        
        try: 
            with open(slide_file, 'r', encoding='utf-8') as f:
                all_slides = [line.strip() for line in f.readlines() if line.strip()]
            
            if len(all_slides) == 0:
                raise ValueError(f"Slideåˆ—è¡¨ä¸ºç©º: {slide_file}")
            
            print(f"ä»{slide_file}åŠ è½½{len(all_slides)}ä¸ªslides")
        
        except UnicodeDecodeError as e:
            raise ValueError(f"Slideåˆ—è¡¨æ–‡ä»¶ç¼–ç é”™è¯¯: {slide_file}, é”™è¯¯: {e}")
        except PermissionError as e:
            raise PermissionError(f"æ²¡æœ‰æƒé™è¯»å–Slideåˆ—è¡¨æ–‡ä»¶: {slide_file}, é”™è¯¯: {e}")
        except IOError as e:
            raise IOError(f"è¯»å–Slideåˆ—è¡¨æ–‡ä»¶æ—¶å‘ç”ŸIOé”™è¯¯: {slide_file}, é”™è¯¯: {e}")
        
        # è§£æéªŒè¯é›†å’Œæµ‹è¯•é›†slides
        val_slides = [s.strip() for s in slide_val.split(',') if s.strip()] if slide_val else []
        test_slides = [s.strip() for s in slide_test.split(',') if s.strip()] if slide_test else []
        
        # éªŒè¯slide IDæœ‰æ•ˆæ€§
        all_slides_set = set(all_slides)
        for slide in val_slides + test_slides:
            if slide not in all_slides_set:
                raise ValueError(f"æŒ‡å®šçš„slide IDä¸å­˜åœ¨: {slide}, å¯ç”¨çš„slides: {sorted(all_slides)}")
        
        # æ£€æŸ¥é‡å¤
        overlap = set(val_slides) & set(test_slides)
        if overlap:
            raise ValueError(f"éªŒè¯é›†å’Œæµ‹è¯•é›†å­˜åœ¨é‡å¤slides: {overlap}")
        
        # å‰©ä½™slidesåˆ†é…ç»™è®­ç»ƒé›†
        used_slides = set(val_slides + test_slides)
        train_slides = [s for s in all_slides if s not in used_slides]
        
        splits = {
            'train': train_slides,
            'val': val_slides,
            'test': test_slides
        }
        
        print(f"Slideåˆ’åˆ†:")
        print(f"  - è®­ç»ƒé›†: {len(train_slides)} slides")
        print(f"  - éªŒè¯é›†: {len(val_slides)} slides")
        print(f"  - æµ‹è¯•é›†: {len(test_slides)} slides")
        
        return splits

    def _init_train_mode(self):
        """åˆå§‹åŒ–è®­ç»ƒæ¨¡å¼"""
        print("åˆå§‹åŒ–è®­ç»ƒæ¨¡å¼æ•°æ®åŠ è½½...")
        
        # é¢„åŠ è½½STæ•°æ®
        self.adata_dict = {}
        for slide_id in self.ids:
            print(f"åŠ è½½{slide_id}çš„STæ•°æ®...")
            self.adata_dict[slide_id] = self.load_st(slide_id, self.genes, **self.norm_param)
        
        if self.expand_augmented:
            print("ğŸš€ å¯ç”¨å¢å¼ºæ ·æœ¬å±•å¼€æ¨¡å¼ï¼šæ¯ä¸ªspotæ‰©å±•ä¸º7ä¸ªè®­ç»ƒæ ·æœ¬")
            
            # åœ¨å±•å¼€æ¨¡å¼ä¸‹ï¼Œé¢„åŠ è½½å¹¶å±•å¼€åµŒå…¥æ•°æ®
            self.expanded_emb_dict = {}
            self.expanded_adata_dict = {}
            
            for slide_id in self.ids:
                print(f"å±•å¼€{slide_id}çš„å¢å¼ºæ•°æ®...")
                
                # åŠ è½½3DåµŒå…¥æ•°æ®
                emb = self.load_emb(slide_id, None, 'all')  # [num_spots, 7, feature_dim]
                original_adata = self.adata_dict[slide_id]
                
                if len(emb.shape) == 3:
                    # å±•å¼€åµŒå…¥ï¼š[num_spots, 7, feature_dim] -> [num_spots*7, feature_dim]
                    num_spots, num_augs, feature_dim = emb.shape
                    expanded_emb = emb.reshape(-1, feature_dim)  # [num_spots*7, feature_dim]
                    
                    # å±•å¼€åŸºå› è¡¨è¾¾æ•°æ®ï¼š[num_spots, num_genes] -> [num_spots*7, num_genes]
                    if sparse.issparse(original_adata.X):
                        original_X = original_adata.X.toarray()
                    else:
                        original_X = original_adata.X
                    
                    # æ¯ä¸ªspotçš„è¡¨è¾¾æ•°æ®é‡å¤7æ¬¡
                    expanded_X = np.repeat(original_X, num_augs, axis=0)  # [num_spots*7, num_genes]
                    
                    # å±•å¼€ä½ç½®ä¿¡æ¯
                    expanded_positions = np.repeat(original_adata.obsm['positions'], num_augs, axis=0)
                    
                    # åˆ›å»ºå±•å¼€åçš„AnnDataå¯¹è±¡
                    expanded_adata = ad.AnnData(X=expanded_X, var=original_adata.var.copy())
                    expanded_adata.var_names = original_adata.var_names
                    expanded_adata.obsm['positions'] = expanded_positions
                    
                    # æ·»åŠ å¢å¼ºä¿¡æ¯åˆ°obs
                    aug_ids = np.tile(np.arange(num_augs), num_spots)  # [0,1,2,3,4,5,6, 0,1,2,3,4,5,6, ...]
                    spot_ids = np.repeat(np.arange(num_spots), num_augs)  # [0,0,0,0,0,0,0, 1,1,1,1,1,1,1, ...]
                    
                    expanded_adata.obs['original_spot_id'] = spot_ids
                    expanded_adata.obs['aug_id'] = aug_ids
                    expanded_adata.obs['array_row'] = expanded_positions[:, 0]
                    expanded_adata.obs['array_col'] = expanded_positions[:, 1]
                    
                    self.expanded_emb_dict[slide_id] = expanded_emb
                    self.expanded_adata_dict[slide_id] = expanded_adata
                    
                    print(f"  {slide_id}: {num_spots} spots -> {num_spots*num_augs} å¢å¼ºæ ·æœ¬")
                else:
                    # å¦‚æœä¸æ˜¯3Dæ ¼å¼ï¼Œä¿æŒåŸæ ·
                    print(f"  {slide_id}: é3Dæ ¼å¼ï¼Œä¿æŒåŸå§‹{emb.shape[0]}ä¸ªæ ·æœ¬")
                    self.expanded_emb_dict[slide_id] = emb
                    self.expanded_adata_dict[slide_id] = original_adata
            
            # ä½¿ç”¨å±•å¼€åçš„æ•°æ®è®¡ç®—é•¿åº¦
            self.lengths = [len(adata) for adata in self.expanded_adata_dict.values()]
            
        else:
            # åŸæœ‰æ¨¡å¼ï¼šè®¡ç®—ç´¯ç§¯é•¿åº¦ç”¨äºç´¢å¼•æ˜ å°„
            self.lengths = [len(adata) for adata in self.adata_dict.values()]
        
        self.cumlen = np.cumsum(self.lengths)
        
        print(f"è®­ç»ƒæ•°æ®ç»Ÿè®¡:")
        print(f"  - å„slideæ ·æœ¬æ•°é‡: {self.lengths}")
        print(f"  - ç´¯ç§¯é•¿åº¦: {self.cumlen}")
        print(f"  - æ€»æ ·æœ¬æ•°é‡: {self.cumlen[-1]}")
        if self.expand_augmented:
            original_total = sum(len(self.adata_dict[slide_id]) for slide_id in self.ids)
            print(f"  - åŸå§‹spotæ•°é‡: {original_total}")
            print(f"  - æ‰©å±•å€æ•°: {self.cumlen[-1] / original_total:.1f}x")

    def load_emb(self, slide_id: str, idx: Optional[int] = None, mode: str = 'first') -> torch.Tensor:
        """åŠ è½½åµŒå…¥ç‰¹å¾
        
        Args:
            slide_id: slideæ ‡è¯†ç¬¦
            idx: spotç´¢å¼•ï¼Œå¦‚æœNoneåˆ™è¿”å›æ‰€æœ‰spots
            mode: 3Då¢å¼ºåµŒå…¥çš„å¤„ç†æ¨¡å¼
                - 'first': ä½¿ç”¨ç¬¬ä¸€ä¸ªå¢å¼ºç‰ˆæœ¬ (åŸå›¾)
                - 'all': è¿”å›æ‰€æœ‰7ä¸ªç‰ˆæœ¬ (ç”¨äºexpand_augmented)
        """
        # æ„å»ºæ–‡ä»¶åï¼Œå¢å¼ºåµŒå…¥éœ€è¦æ·»åŠ _augåç¼€
        if self.use_augmented:
            emb_file = f"{self.emb_dir}/{slide_id}_{self.encoder_name}_aug.pt"
        else:
            emb_file = f"{self.emb_dir}/{slide_id}_{self.encoder_name}.pt"
        
        if not os.path.exists(emb_file):
            raise FileNotFoundError(f"åµŒå…¥æ–‡ä»¶ä¸å­˜åœ¨: {emb_file}")
        
        try:
            # ä½¿ç”¨weights_only=Trueç¡®ä¿å®‰å…¨
            emb = torch.load(emb_file, weights_only=True)
            
            if not isinstance(emb, torch.Tensor):
                raise TypeError(f"åµŒå…¥æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›torch.Tensorï¼Œå¾—åˆ°{type(emb)}")
            
            # å¤„ç†ä¸åŒçš„tensorç»´åº¦
            if len(emb.shape) == 3:
                # 3D tensor: [num_spots, num_augmentations, feature_dim]
                if mode == 'first':
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¢å¼ºç‰ˆæœ¬ï¼ˆåŸå›¾ï¼‰
                    print(f"æ£€æµ‹åˆ°3Då¢å¼ºåµŒå…¥æ ¼å¼: {emb.shape} -> ä½¿ç”¨ç¬¬ä¸€ä¸ªå¢å¼ºç‰ˆæœ¬")
                    emb = emb[:, 0, :]  # [num_spots, feature_dim]
                elif mode == 'all':
                    # è¿”å›æ‰€æœ‰å¢å¼ºç‰ˆæœ¬ (ç”¨äºexpand_augmented)
                    print(f"æ£€æµ‹åˆ°3Då¢å¼ºåµŒå…¥æ ¼å¼: {emb.shape} -> ä¿ç•™æ‰€æœ‰å¢å¼ºç‰ˆæœ¬")
                    pass  # ä¿æŒåŸå§‹3Dæ ¼å¼
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å¼: {mode}ï¼Œåªæ”¯æŒ 'first' æˆ– 'all'")
                    
            elif len(emb.shape) == 2:
                # 2D tensor: [num_spots, feature_dim] (æ ‡å‡†æ ¼å¼)
                pass
            else:
                raise ValueError(f"åµŒå…¥ç»´åº¦é”™è¯¯ï¼ŒæœŸæœ›2Dæˆ–3D tensorï¼Œå¾—åˆ°{emb.shape}")
            
            # æ ¹æ®ç¼–ç å™¨ç±»å‹éªŒè¯ç‰¹å¾ç»´åº¦
            expected_dim = 1024 if self.encoder_name == 'uni' else 512
            final_dim = emb.shape[-1]  # è·å–æœ€åä¸€ç»´
            if final_dim != expected_dim:
                raise ValueError(f"åµŒå…¥ç‰¹å¾ç»´åº¦é”™è¯¯ï¼Œ{self.encoder_name}ç¼–ç å™¨æœŸæœ›{expected_dim}ç»´ï¼Œå¾—åˆ°{final_dim}ç»´")
            
            # è¿”å›æŒ‡å®šç´¢å¼•æˆ–å…¨éƒ¨
            if idx is not None:
                if idx >= emb.shape[0]:
                    raise IndexError(f"ç´¢å¼•è¶Šç•Œ: {idx} >= {emb.shape[0]}")
                if mode == 'all' and len(emb.shape) == 3:
                    return emb[idx]  # [num_augmentations, feature_dim]
                else:
                    return emb[idx]  # [feature_dim]
            else:
                return emb  # [num_spots, feature_dim] æˆ– [num_spots, num_augmentations, feature_dim]
                
        except FileNotFoundError:
            raise FileNotFoundError(f"åµŒå…¥æ–‡ä»¶ä¸å­˜åœ¨: {emb_file}")
        except PermissionError as e:
            raise PermissionError(f"æ²¡æœ‰æƒé™è¯»å–åµŒå…¥æ–‡ä»¶: {emb_file}, é”™è¯¯: {e}")
        except torch.serialization.pickle.UnpicklingError as e:
            raise ValueError(f"åµŒå…¥æ–‡ä»¶æŸåæˆ–æ ¼å¼ä¸æ­£ç¡®: {emb_file}, é”™è¯¯: {e}")
        except RuntimeError as e:
            raise RuntimeError(f"åŠ è½½åµŒå…¥æ–‡ä»¶æ—¶å‘ç”Ÿè¿è¡Œæ—¶é”™è¯¯: {emb_file}, é”™è¯¯: {e}")
        except Exception as e:
            raise ValueError(f"åŠ è½½åµŒå…¥æ–‡ä»¶å¤±è´¥ {emb_file}: {e}")

    def load_st(self, slide_id: str, genes: Optional[List[str]] = None, **kwargs) -> ad.AnnData:
        """åŠ è½½STæ•°æ®"""
        st_file = f"{self.st_dir}/{slide_id}.h5ad"
        
        if not os.path.exists(st_file):
            raise FileNotFoundError(f"STæ–‡ä»¶ä¸å­˜åœ¨: {st_file}")
        
        print(f"åŠ è½½STæ•°æ®: {st_file}")
        
        try:
            adata = sc.read_h5ad(st_file)
            
            # æ£€æŸ¥å¿…è¦çš„é”®
            if 'spatial' not in adata.obsm:
                raise ValueError(f"STæ•°æ®ç¼ºå°‘spatialåæ ‡: {st_file}")
            
            # æ ‡å‡†åŒ–åæ ‡åˆ°0-1èŒƒå›´
            coords = adata.obsm['spatial'].copy()
            coords = (coords - coords.min(axis=0)) / (coords.max(axis=0) - coords.min(axis=0))
            
            # æ·»åŠ array_rowå’Œarray_colåˆ°obsï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if 'array_row' not in adata.obs.columns:
                adata.obs['array_row'] = adata.obsm['spatial'][:, 0]
                adata.obs['array_col'] = adata.obsm['spatial'][:, 1]
            
            # åŸºå› è¿‡æ»¤
            if genes is not None:
                print(f"è¿‡æ»¤åŸºå› ï¼Œä»{adata.n_vars}ä¸ªåŸºå› ä¸­é€‰æ‹©{len(genes)}ä¸ªç›®æ ‡åŸºå› ")
                
                common_genes = list(set(genes).intersection(set(adata.var_names)))
                if len(common_genes) < len(genes):
                    missing_genes = list(set(genes) - set(common_genes))
                    print(f"è­¦å‘Š: {len(missing_genes)}ä¸ªåŸºå› åœ¨{slide_id}ä¸­ä¸å­˜åœ¨: {missing_genes[:5]}...")
                
                adata = adata[:, common_genes].copy()
                print(f"è¿‡æ»¤åä¿ç•™{adata.n_vars}ä¸ªåŸºå› ")
            
            # æ•°æ®å½’ä¸€åŒ–
            if kwargs.get('normalize', True):
                print("æ‰§è¡Œæ•°æ®å½’ä¸€åŒ–...")
                
                # 1. CPMå½’ä¸€åŒ–
                if kwargs.get('cpm', True):
                    print("  - CPMå½’ä¸€åŒ–")
                    sc.pp.normalize_total(adata, target_sum=1e6, inplace=True)
                
                # 2. å¯¹æ•°å˜æ¢
                print("  - å¯¹æ•°å˜æ¢")
                sc.pp.log1p(adata)
                
                # 3. Z-scoreæ ‡å‡†åŒ–
                print("  - Z-scoreæ ‡å‡†åŒ–")
                if sparse.issparse(adata.X):
                    X = adata.X.toarray()
                else:
                    X = adata.X
                
                gene_mean = X.mean(axis=0)
                gene_std = X.std(axis=0)
                gene_std[gene_std == 0] = 1.0
                
                X = (X - gene_mean) / gene_std
                adata.X = sparse.csr_matrix(X) if sparse.issparse(adata.X) else X
                
                # 4. é«˜æ–¯å¹³æ»‘ï¼ˆå¯é€‰ï¼‰
                if kwargs.get('smooth', False):
                    print("  - é«˜æ–¯å¹³æ»‘")
                    if sparse.issparse(adata.X):
                        adata.X = sparse.csr_matrix(gaussian_filter(adata.X.toarray(), sigma=1))
                    else:
                        adata.X = gaussian_filter(adata.X, sigma=1)
            
            # ä¿å­˜æ ‡å‡†åŒ–åçš„åæ ‡
            adata.obsm['positions'] = coords
            
            print(f"STæ•°æ®åŠ è½½å®Œæˆ: {adata.n_obs} spots, {adata.n_vars} genes")
            return adata
            
        except Exception as e:
            raise ValueError(f"åŠ è½½STæ•°æ®å¤±è´¥ {st_file}: {e}")

    def __len__(self) -> int:
        if self.mode == 'train':
            return self.cumlen[-1] if len(self.cumlen) > 0 else 0
        else:
            return len(self.ids)

    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:
        if self.mode == 'train':
            return self._get_train_item(index)
        else:
            return self._get_eval_item(index)

    def _get_train_item(self, index: int) -> Dict[str, torch.Tensor]:
        """è®­ç»ƒæ¨¡å¼è·å–å•ä¸ªspotæ•°æ®"""
        # æ‰¾åˆ°å¯¹åº”çš„slideå’Œæ ·æœ¬ç´¢å¼•
        i = 0
        while index >= self.cumlen[i]:
            i += 1
        
        sample_idx = index
        if i > 0:
            sample_idx = index - self.cumlen[i-1]
        
        slide_id = self.int2id[i]
        
        if self.expand_augmented and hasattr(self, 'expanded_emb_dict'):
            # ä½¿ç”¨é¢„å±•å¼€çš„æ•°æ®
            features = self.expanded_emb_dict[slide_id][sample_idx]  # [feature_dim]
            
            # ä»å±•å¼€çš„AnnDataä¸­è·å–åŸºå› è¡¨è¾¾
            expanded_adata = self.expanded_adata_dict[slide_id]
            expression = expanded_adata[sample_idx].X
            
            if sparse.issparse(expression):
                expression = expression.toarray().squeeze(0)
            else:
                expression = expression.squeeze(0)
            
            # è·å–ä½ç½®ä¿¡æ¯
            positions = expanded_adata.obsm['positions'][sample_idx]  # [2]
            
            # è·å–å¢å¼ºä¿¡æ¯ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
            original_spot_id = int(expanded_adata.obs['original_spot_id'].iloc[sample_idx])
            aug_id = int(expanded_adata.obs['aug_id'].iloc[sample_idx])
            
            return {
                'img': torch.FloatTensor(features),  # [feature_dim]
                'target_genes': torch.FloatTensor(expression),  # [num_genes]
                'positions': torch.FloatTensor(positions),  # [2]
                'slide_id': slide_id,
                'spot_idx': sample_idx,
                'original_spot_id': original_spot_id,  # åŸå§‹spot ID
                'aug_id': aug_id  # å¢å¼ºç‰ˆæœ¬ID (0-6)
            }
        else:
            # åŸæœ‰æ¨¡å¼ï¼šåŠ¨æ€åŠ è½½
            features = self.load_emb(slide_id, sample_idx, 'first')  # [feature_dim]
            
            # åŠ è½½åŸºå› è¡¨è¾¾
            adata = self.adata_dict[slide_id]
            expression = adata[sample_idx].X
            
            if sparse.issparse(expression):
                expression = expression.toarray().squeeze(0)
            else:
                expression = expression.squeeze(0)
            
            # åŠ è½½ä½ç½®ä¿¡æ¯
            positions = adata.obsm['positions'][sample_idx]  # [2]
            
            return {
                'img': features,  # [feature_dim]
                'target_genes': torch.FloatTensor(expression),  # [num_genes]
                'positions': torch.FloatTensor(positions),  # [2]
                'slide_id': slide_id,
                'spot_idx': sample_idx
            }

    def _get_eval_item(self, index: int) -> Dict[str, torch.Tensor]:
        """éªŒè¯/æµ‹è¯•æ¨¡å¼è·å–æ•´ä¸ªslideæ•°æ®"""
        slide_id = self.int2id[index]
        
        # åŠ è½½åµŒå…¥ç‰¹å¾
        features = self.load_emb(slide_id, None, 'first')  # [num_spots, feature_dim]
        
        # åŠ è½½STæ•°æ®
        adata = self.load_st(slide_id, self.genes, **self.norm_param)
        
        # åŠ è½½åŸºå› è¡¨è¾¾
        expression = adata.X
        if sparse.issparse(expression):
            expression = expression.toarray()
        
        # åŠ è½½ä½ç½®ä¿¡æ¯
        positions = adata.obsm['positions']  # [num_spots, 2]
        
        return {
            'img': features,  # [num_spots, feature_dim]
            'target_genes': torch.FloatTensor(expression),  # [num_spots, num_genes]
            'positions': torch.FloatTensor(positions),  # [num_spots, 2]
            'slide_id': slide_id,
            'num_spots': adata.n_obs
        }

