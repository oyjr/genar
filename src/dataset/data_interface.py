import inspect
import importlib
import pytorch_lightning as pl
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

import dataset


class DataInterface(pl.LightningDataModule):
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # ğŸ†• æ£€æµ‹VAR-STæ¨¡å¼ - ä¿®å¤é…ç½®è®¿é—®æ–¹å¼
        model_name = ''
        if hasattr(config, 'MODEL') and hasattr(config.MODEL, 'model_name'):
            model_name = config.MODEL.model_name
        elif hasattr(config, 'model_name'):
            model_name = config.model_name
        
        is_var_st = model_name.upper() == 'VAR_ST'
        
        # å¼ºåˆ¶VAR-STæ¨¡å‹ä½¿ç”¨196åŸºå› æ¨¡å¼
        if is_var_st:
            use_var_st_genes = True
            var_st_gene_count = 196
            print(f"ğŸ§¬ æ£€æµ‹åˆ°VAR_STæ¨¡å‹ï¼Œå¼ºåˆ¶å¯ç”¨196åŸºå› æ¨¡å¼")
        else:
            use_var_st_genes = getattr(config, 'use_var_st_genes', False)
            var_st_gene_count = getattr(config, 'var_st_gene_count', 196)
        
        print(f"åˆå§‹åŒ–DataInterface:")
        print(f"  - æ•°æ®é›†åç§°: STDataset")
        print(f"  - è¡¨è¾¾è°±åç§°: {config.expr_name}")
        print(f"  - æ•°æ®è·¯å¾„: {config.data_path}")
        print(f"  - ç¼–ç å™¨: {config.encoder_name}")
        print(f"  - ä½¿ç”¨å¢å¼º: {config.use_augmented}")
        print(f"  - ğŸ§¬ æ£€æµ‹åˆ°æ¨¡å‹åç§°: {model_name}")
        print(f"  - ğŸ†• VAR-STåŸºå› æ¨¡å¼: {use_var_st_genes}")
        
        if use_var_st_genes:
            print(f"  - ğŸ§¬ VAR-STåŸºå› æ•°é‡: {var_st_gene_count}")
        
        # å­˜å‚¨é…ç½®ä»¥ä¾¿åç»­ä½¿ç”¨
        self.use_var_st_genes = use_var_st_genes
        self.var_st_gene_count = var_st_gene_count

    def setup(self, stage=None):
        """
        è®¾ç½®æ•°æ®é›†
        Args:
            stage: 'fit', 'val', 'test' æˆ– None
        """
        print(f"è®¾ç½®æ•°æ®é›†é˜¶æ®µ: {stage}")
        
        # ç»Ÿä¸€ä½¿ç”¨STDataset
        dataset_class = getattr(dataset, 'STDataset')
        
        # åŸºç¡€å‚æ•°é…ç½® - ğŸ†• æ·»åŠ VAR-STåŸºå› æ”¯æŒ
        base_params = {
            'data_path': self.config.data_path,
            'expr_name': self.config.expr_name,
            'slide_val': self.config.slide_val,
            'slide_test': self.config.slide_test,
            'encoder_name': self.config.encoder_name,
            'use_augmented': self.config.use_augmented,
            'normalize': self.config.DATA.normalize,
            'use_var_st_genes': self.use_var_st_genes,  # ğŸ†• VAR-STåŸºå› æ¨¡å¼
            'var_st_gene_count': self.var_st_gene_count,  # ğŸ†• VAR-STåŸºå› æ•°é‡
        }
        
        print(f"åŸºç¡€å‚æ•°é…ç½®: {base_params}")
        
        if stage == 'fit' or stage is None:
            train_params = base_params.copy()
            train_params['mode'] = 'train'
            # åªæœ‰è®­ç»ƒæ¨¡å¼æ‰ä¼ é€’expand_augmentedå‚æ•°
            train_params['expand_augmented'] = self.config.expand_augmented
            print(f"åˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
            self.train_dataset = dataset_class(**train_params)
            print(f"è®­ç»ƒæ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œå¤§å°: {len(self.train_dataset)}")
        
        if stage == 'val' or stage == 'fit' or stage is None:
            val_params = base_params.copy()
            val_params['mode'] = 'val'
            # éªŒè¯æ¨¡å¼ä¸ä½¿ç”¨expand_augmented
            val_params['expand_augmented'] = False
            print(f"åˆ›å»ºéªŒè¯æ•°æ®é›†...")
            self.val_dataset = dataset_class(**val_params)
            print(f"éªŒè¯æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œå¤§å°: {len(self.val_dataset)}")
        
        if stage == 'test' or stage is None:
            test_params = base_params.copy()
            test_params['mode'] = 'test'
            # æµ‹è¯•æ¨¡å¼ä¸ä½¿ç”¨expand_augmented
            test_params['expand_augmented'] = False
            print(f"åˆ›å»ºæµ‹è¯•æ•°æ®é›†...")
            self.test_dataset = dataset_class(**test_params)
            print(f"æµ‹è¯•æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œå¤§å°: {len(self.test_dataset)}")

        print("\n=== æ•°æ®é›†ä¿¡æ¯æ€»ç»“ ===")
        if hasattr(self, 'train_dataset'):
            print(f"è®­ç»ƒæ•°æ®é›†: {len(self.train_dataset)} ä¸ªæ ·æœ¬")
            if self.use_var_st_genes:
                print(f"ğŸ§¬ ä½¿ç”¨VAR-STæ¨¡å¼: å‰{self.var_st_gene_count}ä¸ªåŸºå› ")
        if hasattr(self, 'val_dataset'):
            print(f"éªŒè¯æ•°æ®é›†: {len(self.val_dataset)} ä¸ªæ ·æœ¬")
        if hasattr(self, 'test_dataset'):
            print(f"æµ‹è¯•æ•°æ®é›†: {len(self.test_dataset)} ä¸ªæ ·æœ¬")
        print("====================\n")

    def train_dataloader(self):
        train_config = self.config.DATA.train_dataloader
        
        # Handle both dict and Namespace types
        if isinstance(train_config, dict):
            batch_size = train_config.get('batch_size', 256)
            shuffle = train_config.get('shuffle', True)
            pin_memory = train_config.get('pin_memory', True)
            num_workers = train_config.get('num_workers', 4)
        else:
            batch_size = getattr(train_config, 'batch_size', 256)
            shuffle = getattr(train_config, 'shuffle', True)
            pin_memory = getattr(train_config, 'pin_memory', True)
            num_workers = getattr(train_config, 'num_workers', 4)
            
        print(f"åˆ›å»ºè®­ç»ƒDataLoader: batch_size={batch_size}")
        return DataLoader(
            self.train_dataset, 
            batch_size=batch_size, 
            shuffle=shuffle, 
            pin_memory=pin_memory,
            num_workers=num_workers
        )
    
    def val_dataloader(self):
        val_config = self.config.DATA.val_dataloader
        
        # Handle both dict and Namespace types
        if isinstance(val_config, dict):
            batch_size = val_config.get('batch_size', 1)
            shuffle = val_config.get('shuffle', False)
            pin_memory = val_config.get('pin_memory', True)
            num_workers = val_config.get('num_workers', 4)
        else:
            batch_size = getattr(val_config, 'batch_size', 1)
            shuffle = getattr(val_config, 'shuffle', False)
            pin_memory = getattr(val_config, 'pin_memory', True)
            num_workers = getattr(val_config, 'num_workers', 4)
            
        print(f"åˆ›å»ºéªŒè¯DataLoader: batch_size={batch_size}")
        return DataLoader(
            self.val_dataset, 
            batch_size=batch_size, 
            shuffle=shuffle, 
            pin_memory=pin_memory, 
            num_workers=num_workers
        )

    def test_dataloader(self):
        # Check if test_dataloader config exists, otherwise use val_dataloader config
        if hasattr(self.config.DATA, 'test_dataloader'):
            test_config = self.config.DATA.test_dataloader
        else:
            test_config = self.config.DATA.val_dataloader
        
        # Handle both dict and Namespace types
        if isinstance(test_config, dict):
            batch_size = test_config.get('batch_size', 1)
            shuffle = test_config.get('shuffle', False)
            pin_memory = test_config.get('pin_memory', True)
            num_workers = test_config.get('num_workers', 4)
        else:
            batch_size = getattr(test_config, 'batch_size', 1)
            shuffle = getattr(test_config, 'shuffle', False)
            pin_memory = getattr(test_config, 'pin_memory', True)
            num_workers = getattr(test_config, 'num_workers', 4)
            
        print(f"åˆ›å»ºæµ‹è¯•DataLoader: batch_size={batch_size}")
        return DataLoader(
            self.test_dataset, 
            batch_size=batch_size, 
            shuffle=shuffle, 
            pin_memory=pin_memory, 
            num_workers=num_workers
        )

    def predict_dataloader(self):
        return self.test_dataloader()
    
    def load_data_module(self):
        pass
                    