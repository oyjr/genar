"""
Gene clustering and reordering based on expression similarity

This module implements gene clustering to reorder genes based on their 
spatial expression patterns for better biological coherence in VAR models.

Author: Assistant
Date: 2024
"""

import os
import json
import numpy as np
import shutil
from datetime import datetime
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from typing import List, Dict, Tuple
import logging

from .utils import (
    load_gene_list, save_gene_list, get_train_slides, 
    load_slide_gene_expression
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GeneClusteringProcessor:
    """åŸºå› èšç±»å¤„ç†å™¨"""
    
    def __init__(self, scale_dims: Tuple[int, ...] = (1, 4, 8, 40, 100, 200)):
        """
        Args:
            scale_dims: VARæ¨¡å‹çš„å¤šå°ºåº¦é…ç½®
        """
        self.scale_dims = scale_dims
        
        # æ•°æ®é›†é…ç½®
        self.datasets = {
            'PRAD': {
                'path': '/data/ouyangjiarui/stem/hest1k_datasets/PRAD/',
                'val_slides': 'MEND145'
            },
            'her2st': {
                'path': '/data/ouyangjiarui/stem/hest1k_datasets/her2st/',
                'val_slides': 'SPA148'
            },
            'kidney': {
                'path': '/data/ouyangjiarui/stem/hest1k_datasets/kidney/',
                'val_slides': 'NCBI697'
            },
            'mouse_brain': {
                'path': '/data/ouyangjiarui/stem/hest1k_datasets/mouse_brain/',
                'val_slides': 'NCBI667'
            },
            'ccRCC': {
                'path': '/data/ouyangjiarui/stem/hest1k_datasets/ccRCC/',
                'val_slides': 'INT2'
            }
        }
    
    def process_dataset(self, dataset_name: str) -> None:
        """å¤„ç†å•ä¸ªæ•°æ®é›†çš„åŸºå› èšç±»"""
        
        if dataset_name not in self.datasets:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
        
        dataset_config = self.datasets[dataset_name]
        data_path = os.path.join(dataset_config['path'], 'processed_data')
        val_slide = dataset_config['val_slides']
        
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ•°æ®é›†: {dataset_name}")
        logger.info(f"   æ•°æ®è·¯å¾„: {data_path}")
        logger.info(f"   éªŒè¯é›†slide: {val_slide}")
        
        # 1. å¤‡ä»½åŸå§‹åŸºå› åˆ—è¡¨
        self._backup_original_gene_list(data_path)
        
        # 2. è·å–è®­ç»ƒé›†slides
        train_slides = get_train_slides(data_path, val_slide)
        
        # 3. åŠ è½½è®­ç»ƒé›†åŸºå› è¡¨è¾¾æ•°æ®
        combined_expr = self._load_training_data(data_path, train_slides)
        logger.info(f"ğŸ“Š è®­ç»ƒé›†æ€»spots: {combined_expr.shape[0]}")
        
        # 4. æ‰§è¡ŒåŸºå› èšç±»
        clustered_order = self._perform_clustering(combined_expr)
        
        # 5. é‡æ–°æ’åˆ—å¹¶ä¿å­˜åŸºå› åˆ—è¡¨
        self._save_clustered_gene_list(data_path, clustered_order)
        
        # 6. ä¿å­˜èšç±»ä¿¡æ¯
        self._save_clustering_info(data_path, dataset_name, train_slides, 
                                  combined_expr.shape[0], clustered_order)
        
        logger.info(f"âœ… {dataset_name} åŸºå› èšç±»å®Œæˆ")
    
    def _backup_original_gene_list(self, data_path: str) -> None:
        """å¤‡ä»½åŸå§‹åŸºå› åˆ—è¡¨"""
        original_file = os.path.join(data_path, 'selected_gene_list.txt')
        backup_file = os.path.join(data_path, 'unclustered_selected_gene_list.txt')
        
        if not os.path.exists(backup_file):
            shutil.copy(original_file, backup_file)
            logger.info(f"ğŸ’¾ å¤‡ä»½åŸå§‹åŸºå› åˆ—è¡¨: {backup_file}")
        else:
            logger.info(f"ğŸ“ å¤‡ä»½æ–‡ä»¶å·²å­˜åœ¨: {backup_file}")
    
    def _load_training_data(self, data_path: str, train_slides: List[str]) -> np.ndarray:
        """åŠ è½½è®­ç»ƒé›†çš„åŸºå› è¡¨è¾¾æ•°æ®"""
        logger.info(f"ğŸ“¥ åŠ è½½{len(train_slides)}ä¸ªè®­ç»ƒslidesçš„æ•°æ®...")
        
        all_expr_data = []
        
        for slide_id in train_slides:
            try:
                slide_expr = load_slide_gene_expression(data_path, slide_id)
                logger.info(f"   {slide_id}: {slide_expr.shape[0]} spots, {slide_expr.shape[1]} genes")
                
                # åªå–å‰200ä¸ªåŸºå› 
                if slide_expr.shape[1] >= 200:
                    slide_expr = slide_expr[:, :200]
                else:
                    logger.warning(f"âš ï¸  {slide_id}åªæœ‰{slide_expr.shape[1]}ä¸ªåŸºå› ï¼Œå°‘äº200ä¸ª")
                    continue
                
                all_expr_data.append(slide_expr)
                
            except Exception as e:
                logger.error(f"âŒ åŠ è½½{slide_id}å¤±è´¥: {e}")
                continue
        
        if not all_expr_data:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•è®­ç»ƒæ•°æ®")
        
        # åˆå¹¶æ‰€æœ‰è®­ç»ƒæ•°æ®
        combined_expr = np.concatenate(all_expr_data, axis=0)
        logger.info(f"ğŸ“Š åˆå¹¶è®­ç»ƒæ•°æ®: {combined_expr.shape}")
        
        return combined_expr
    
    def _perform_clustering(self, gene_expr_matrix: np.ndarray) -> np.ndarray:
        """æ‰§è¡ŒåŸºå› èšç±»"""
        logger.info("ğŸ§¬ å¼€å§‹åŸºå› èšç±»...")
        
        # æ„å»ºåŸºå› ç‰¹å¾çŸ©é˜µ: [n_genes, n_spots]
        gene_features = gene_expr_matrix.T
        logger.info(f"   åŸºå› ç‰¹å¾çŸ©é˜µ: {gene_features.shape}")
        
        # æ ‡å‡†åŒ–å¤„ç†ï¼ˆæ¯ä¸ªåŸºå› æ ‡å‡†åŒ–ï¼‰
        scaler = StandardScaler()
        gene_features_norm = scaler.fit_transform(gene_features)
        logger.info(f"   æ ‡å‡†åŒ–å®Œæˆ")
        
        # ç¬¬ä¸€é˜¶æ®µï¼šèšæˆ4ä¸ªå¤§ç¾¤ç»„
        logger.info("   é˜¶æ®µ1: èšç±»ä¸º4ä¸ªå¤§ç¾¤ç»„...")
        kmeans_4 = KMeans(n_clusters=4, random_state=42, n_init=10)
        major_clusters = kmeans_4.fit_predict(gene_features_norm)
        
        cluster_sizes = np.bincount(major_clusters)
        logger.info(f"   å¤§ç¾¤ç»„å¤§å°: {cluster_sizes}")
        
        # ç¬¬äºŒé˜¶æ®µï¼šæ¯ä¸ªå¤§ç¾¤ç»„å†…éƒ¨ç»†åˆ†
        logger.info("   é˜¶æ®µ2: å¤§ç¾¤ç»„å†…éƒ¨ç»†åˆ†...")
        clustered_order = []
        
        for major_group in range(4):
            genes_in_major = np.where(major_clusters == major_group)[0]
            group_features = gene_features_norm[genes_in_major]
            
            if len(genes_in_major) <= 10:
                # ç¾¤ç»„å¤ªå°ï¼Œç›´æ¥æ·»åŠ 
                clustered_order.extend(genes_in_major.tolist())
                logger.info(f"     ç¾¤ç»„{major_group}: {len(genes_in_major)}ä¸ªåŸºå› ï¼ˆæ— ç»†åˆ†ï¼‰")
            else:
                # ç¾¤ç»„å†…éƒ¨å†ç»†åˆ†
                n_sub_clusters = max(2, len(genes_in_major) // 12)
                kmeans_sub = KMeans(n_clusters=n_sub_clusters, random_state=42)
                sub_clusters = kmeans_sub.fit_predict(group_features)
                
                # æŒ‰å­ç¾¤ç»„æ’åºæ·»åŠ 
                for sub_group in range(n_sub_clusters):
                    genes_in_sub = genes_in_major[sub_clusters == sub_group]
                    clustered_order.extend(genes_in_sub.tolist())
                
                logger.info(f"     ç¾¤ç»„{major_group}: {len(genes_in_major)}ä¸ªåŸºå›  â†’ {n_sub_clusters}ä¸ªå­ç¾¤")
        
        clustered_order = np.array(clustered_order)
        logger.info(f"âœ… èšç±»å®Œæˆï¼Œé‡æ’åº: {len(clustered_order)}ä¸ªåŸºå› ")
        
        return clustered_order
    
    def _save_clustered_gene_list(self, data_path: str, clustered_order: np.ndarray) -> None:
        """ä¿å­˜èšç±»åçš„åŸºå› åˆ—è¡¨"""
        # åŠ è½½åŸå§‹åŸºå› åˆ—è¡¨
        backup_file = os.path.join(data_path, 'unclustered_selected_gene_list.txt')
        original_genes = load_gene_list(backup_file)
        
        # é‡æ–°æ’åˆ—
        clustered_genes = [original_genes[i] for i in clustered_order]
        
        # ä¿å­˜åˆ°åŸæ–‡ä»¶ä½ç½®
        output_file = os.path.join(data_path, 'selected_gene_list.txt')
        save_gene_list(output_file, clustered_genes)
        
        logger.info(f"ğŸ’¾ ä¿å­˜èšç±»ååŸºå› åˆ—è¡¨: {output_file}")
    
    def _save_clustering_info(self, data_path: str, dataset_name: str, 
                            train_slides: List[str], total_spots: int, 
                            clustered_order: np.ndarray) -> None:
        """ä¿å­˜èšç±»è¯¦ç»†ä¿¡æ¯"""
        clustering_info = {
            'dataset': dataset_name,
            'train_slides': train_slides,
            'total_spots': total_spots,
            'clustered_order': clustered_order.tolist(),
            'scale_dims': self.scale_dims,
            'timestamp': datetime.now().isoformat(),
            'algorithm': 'kmeans_hierarchical',
            'parameters': {
                'stage1_clusters': 4,
                'normalization': 'zscore',
                'random_state': 42
            }
        }
        
        info_file = os.path.join(data_path, 'clustering_info.json')
        with open(info_file, 'w') as f:
            json.dump(clustering_info, f, indent=2)
        
        logger.info(f"ğŸ“ ä¿å­˜èšç±»ä¿¡æ¯: {info_file}")
    
    def process_all_datasets(self) -> None:
        """å¤„ç†æ‰€æœ‰æ•°æ®é›†"""
        for dataset_name in self.datasets.keys():
            try:
                self.process_dataset(dataset_name)
            except Exception as e:
                logger.error(f"âŒ å¤„ç†{dataset_name}å¤±è´¥: {e}")
                continue