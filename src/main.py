import os
import sys
import argparse
import logging
from datetime import datetime
from glob import glob
from pathlib import Path

from typing import Dict, Any

# Á°Æ‰øùÂØºÂÖ•È°πÁõÆÁõÆÂΩï‰∏ãÁöÑÊ®°Âùó
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import torch
import pytorch_lightning as pl
from pytorch_lightning.strategies.ddp import DDPStrategy
from pytorch_lightning import loggers as pl_loggers

# ÂØºÂÖ•È°πÁõÆÊ®°Âùó
from dataset.data_interface import DataInterface
from model import ModelInterface
from utils import (
    load_callbacks,
    load_loggers,
    fix_seed
)

# ËÆæÁΩÆÊó•ÂøóËÆ∞ÂΩïÂô®
logger = logging.getLogger(__name__)

torch.set_float32_matmul_precision('high')


# ÁºñÁ†ÅÂô®ÁâπÂæÅÁª¥Â∫¶Êò†Â∞Ñ
ENCODER_FEATURE_DIMS = {
    'uni': 1024,
    'conch': 512
}

# Êï∞ÊçÆÈõÜÈÖçÁΩÆ - ÂåÖÂê´Ë∑ØÂæÑÂíåslideÂàíÂàÜ‰ø°ÊÅØ
DATASETS = {
    'PRAD': {
        'path': '/data/ouyangjiarui/stem/hest1k_datasets/PRAD/',
        'val_slides': 'MEND139',
        'test_slides': 'MEND140',
        'recommended_encoder': 'uni'
    },
    'her2st': {
        'path': '/data/ouyangjiarui/stem/hest1k_datasets/her2st/',
        'val_slides': 'A1,B1',
        'test_slides': 'C1,D1', 
        'recommended_encoder': 'conch'
    }
}

# Ê®°ÂûãÈÖçÁΩÆ
MODELS = {
    'MFBP': {
        'model_name': 'MFBP',
        'num_genes': 200,
        'dropout_rate': 0.1
    }
}

# ÈªòËÆ§ËÆ≠ÁªÉÈÖçÁΩÆ - ‰ªébase_config.yamlÊèêÂèñÁöÑÊ†∏ÂøÉÈÖçÁΩÆ
DEFAULT_CONFIG = {
    'GENERAL': {
        'seed': 2021,
        'log_path': './logs',  # Will be updated to dataset-specific path
        'debug': False
    },
    'DATA': {
        'normalize': True,  # STEmÊñπÂºè: log2(+1)ÂèòÊç¢
        'train_dataloader': {
            'batch_size': 256,
            'num_workers': 4,
            'pin_memory': True,
            'shuffle': True,
            'persistent_workers': True
        },
        'val_dataloader': {
            'batch_size': 1,
            'num_workers': 4,
            'pin_memory': True,
            'shuffle': False,
            'persistent_workers': True
        },
        'test_dataloader': {
            'batch_size': 1,
            'num_workers': 4,
            'pin_memory': True,
            'shuffle': False,
            'persistent_workers': True
        }
    },
    'TRAINING': {
        'num_epochs': 200,
        'learning_rate': 1.0e-4,
        'weight_decay': 1.0e-4,
        'mode': 'min',
        'monitor': 'val_loss',
        'lr_scheduler': {
            'monitor': 'val_loss',
            'patience': 5,
            'factor': 0.5,
            'mode': 'min'
        },
        'gradient_clip_val': 1.0
    },
    'CALLBACKS': {
        'early_stopping': {
            'monitor': 'val_loss',
            'patience': 10,
            'mode': 'min',
            'min_delta': 0.0
        },
        'model_checkpoint': {
            'monitor': 'val_loss',
            'save_top_k': 1,
            'mode': 'min',
            'filename': 'epoch={epoch}-val_loss={val_loss:.4f}'
        },
        'learning_rate_monitor': {
            'logging_interval': 'epoch'
        }
    },
    'MULTI_GPU': {
        'strategy': 'ddp',
        'sync_batchnorm': True,
        'find_unused_parameters': False,
        'lr_scaling': 'linear',
        'base_lr': 1.0e-4,
        'accumulate_grad_batches': 1
    }
}





def get_parse():
    """
    Parse command line arguments for simplified MFBP training.
    
    Returns:
        argparse.ArgumentParser: Configured argument parser with simplified parameters
    """
    parser = argparse.ArgumentParser(
        description='Simplified Training for Spatial Transcriptomics Models',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage
  python src/main.py --dataset PRAD --gpus 4
  
  # With custom parameters
  python src/main.py --dataset PRAD --model MFBP --encoder uni \\
      --gpus 4 --epochs 200 --batch_size 256 --lr 1e-4
  
  # Single GPU training
  python src/main.py --dataset her2st --gpus 1
  
  # Test mode
  python src/main.py --dataset PRAD --gpus 1 --mode test
        """
    )
    
    # === Ê†∏ÂøÉÂèÇÊï∞ ===
    parser.add_argument('--dataset', type=str, choices=list(DATASETS.keys()),
                        help='Êï∞ÊçÆÈõÜÂêçÁß∞ (PRAD, her2st)')
    parser.add_argument('--model', type=str, default='MFBP', choices=list(MODELS.keys()),
                        help='Ê®°ÂûãÂêçÁß∞ (ÈªòËÆ§: MFBP)')
    parser.add_argument('--encoder', type=str, choices=list(ENCODER_FEATURE_DIMS.keys()),
                        help='ÁºñÁ†ÅÂô®Á±ªÂûã (uni, conch)ÔºåÈªòËÆ§‰ΩøÁî®Êï∞ÊçÆÈõÜÊé®ËçêÁºñÁ†ÅÂô®')
    
    # === ËÆ≠ÁªÉÂèÇÊï∞ ===
    parser.add_argument('--gpus', type=int, default=1,
                        help='GPUÊï∞Èáè (ÈªòËÆ§: 1)')
    parser.add_argument('--epochs', type=int,
                        help='ËÆ≠ÁªÉËΩÆÊï∞ (ÈªòËÆ§: 200)')
    parser.add_argument('--batch_size', type=int,
                        help='ÊâπÊ¨°Â§ßÂ∞è (ÈªòËÆ§: 256)')
    parser.add_argument('--lr', type=float,
                        help='Â≠¶‰π†Áéá (ÈªòËÆ§: 1e-4)')
    parser.add_argument('--weight-decay', type=float,
                        help='ÊùÉÈáçË°∞Âáè (ÈªòËÆ§: 1e-4)')
    
    # === Â§öGPUÂèÇÊï∞ ===
    parser.add_argument('--strategy', type=str, default='auto',
                        choices=['auto', 'ddp', 'ddp_spawn', 'dp'],
                        help='Â§öGPUÁ≠ñÁï• (ÈªòËÆ§: autoÔºåÂ§öGPUÊó∂‰ΩøÁî®ddp)')
    parser.add_argument('--sync-batchnorm', action='store_true',
                        help='ÂêØÁî®ÂêåÊ≠•BatchNorm (Â§öGPUËÆ≠ÁªÉÊé®Ëçê)')
    
    # === Êï∞ÊçÆÂ¢ûÂº∫ÂèÇÊï∞ ===
    parser.add_argument('--use-augmented', action='store_true', default=True,
                        help='‰ΩøÁî®Êï∞ÊçÆÂ¢ûÂº∫ (ÈªòËÆ§: True)')
    parser.add_argument('--expand-augmented', action='store_true', default=True,
                        help='Â±ïÂºÄÂ¢ûÂº∫Êï∞ÊçÆ‰∏∫7ÂÄçÊ†∑Êú¨ (ÈªòËÆ§: True)')
    
    # === ÂÖ∂‰ªñÂèÇÊï∞ ===
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'],
                        help='ËøêË°åÊ®°Âºè (ÈªòËÆ§: train)')
    parser.add_argument('--seed', type=int,
                        help='ÈöèÊú∫ÁßçÂ≠ê (ÈªòËÆ§: 2021)')
    
    # === ÂêëÂêéÂÖºÂÆπÂèÇÊï∞ (‰øùÁïôÊúÄÂ∞ëÂøÖË¶ÅÁöÑ) ===
    parser.add_argument('--config', type=str,
                        help='[Â∑≤ÂºÉÁî®] ËØ∑‰ΩøÁî® --dataset ÂèÇÊï∞Êõø‰ª£')
    
    return parser


def build_config_from_args(args):
    """
    ‰ªéÁÆÄÂåñÁöÑÂëΩ‰ª§Ë°åÂèÇÊï∞ÊûÑÂª∫ÂÆåÊï¥ÈÖçÁΩÆ
    
    Args:
        args: Ëß£ÊûêÂêéÁöÑÂëΩ‰ª§Ë°åÂèÇÊï∞
        
    Returns:
        ÂÆåÊï¥ÁöÑÈÖçÁΩÆÂØπË±°
    """
    from addict import Dict
    
    # Â¶ÇÊûú‰ΩøÁî®‰∫ÜÂéüÊúâÁöÑconfigÂèÇÊï∞ÔºåÂàô‰ΩøÁî®ÂéüÊúâÈÄªËæë
    if args.config:
        print("üîÑ ‰ΩøÁî®ÂéüÊúâÈÖçÁΩÆÊñá‰ª∂Ê®°Âºè")
        return None  # ËøîÂõûNoneË°®Á§∫‰ΩøÁî®ÂéüÊúâÈÄªËæë
    
    # Ê£ÄÊü•ÂøÖÈúÄÂèÇÊï∞
    if not args.dataset:
        raise ValueError("ÂøÖÈ°ªÊåáÂÆö --dataset ÂèÇÊï∞")
    
    if args.dataset not in DATASETS:
        raise ValueError(f"‰∏çÊîØÊåÅÁöÑÊï∞ÊçÆÈõÜ: {args.dataset}ÔºåÊîØÊåÅÁöÑÊï∞ÊçÆÈõÜ: {list(DATASETS.keys())}")
    
    print(f"üöÄ ‰ΩøÁî®ÁÆÄÂåñÈÖçÁΩÆÊ®°Âºè: Êï∞ÊçÆÈõÜ={args.dataset}, Ê®°Âûã={args.model}")
    
    # Ëé∑ÂèñÊï∞ÊçÆÈõÜ‰ø°ÊÅØ
    dataset_info = DATASETS[args.dataset]
    
    # Ëé∑ÂèñÊ®°Âûã‰ø°ÊÅØ
    model_info = MODELS[args.model]
    
    # Á°ÆÂÆöÁºñÁ†ÅÂô®
    encoder_name = args.encoder or dataset_info['recommended_encoder']
    
    # Á°ÆÂÆöGPUÁõ∏ÂÖ≥ÂèÇÊï∞
    devices = args.gpus
    strategy = 'ddp' if devices > 1 and args.strategy == 'auto' else args.strategy
    sync_batchnorm = getattr(args, 'sync_batchnorm', False) or (devices > 1)
    
    # ÊûÑÂª∫ÂÆåÊï¥ÈÖçÁΩÆ
    config = Dict(DEFAULT_CONFIG)
    
    # Êõ¥Êñ∞Êó•ÂøóË∑ØÂæÑ‰∏∫Êï∞ÊçÆÈõÜÂêçÁß∞ÂíåÊ®°ÂûãÂêçÁß∞
    config.GENERAL.log_path = f'./logs/{args.dataset}/{args.model}'
    
    # Êõ¥Êñ∞Ê®°ÂûãÈÖçÁΩÆ
    config.MODEL = Dict(model_info)
    config.MODEL.feature_dim = ENCODER_FEATURE_DIMS[encoder_name]
    
    # Êõ¥Êñ∞ËÆ≠ÁªÉÂèÇÊï∞
    if args.epochs:
        config.TRAINING.num_epochs = args.epochs
    if args.lr:
        config.TRAINING.learning_rate = args.lr
    if args.weight_decay:
        config.TRAINING.weight_decay = args.weight_decay
    if getattr(args, 'batch_size', None):
        config.DATA.train_dataloader.batch_size = args.batch_size
    
    # Êõ¥Êñ∞ÁßçÂ≠ê
    if args.seed:
        config.GENERAL.seed = args.seed
    
    # ËÆæÁΩÆÊï∞ÊçÆÈõÜÁõ∏ÂÖ≥ÂèÇÊï∞
    config.mode = args.mode
    config.expr_name = args.dataset
    config.data_path = dataset_info['path']
    config.slide_val = dataset_info['val_slides']
    config.slide_test = dataset_info['test_slides']
    config.encoder_name = encoder_name
    config.use_augmented = getattr(args, 'use_augmented', True)
    config.expand_augmented = getattr(args, 'expand_augmented', True)
    
    # ËÆæÁΩÆÂ§öGPUÂèÇÊï∞
    config.devices = devices
    config.strategy = strategy
    config.sync_batchnorm = sync_batchnorm
    
    # ËÆæÁΩÆÊó∂Èó¥Êà≥ÂíåÈÖçÁΩÆË∑ØÂæÑ
    config.GENERAL.current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    config.config = 'built-in'  # Ê†áËÆ∞‰∏∫ÂÜÖÁΩÆÈÖçÁΩÆ
    
    print(f"‚úÖ ÈÖçÁΩÆÊûÑÂª∫ÂÆåÊàê:")
    print(f"   - Êï∞ÊçÆÈõÜ: {args.dataset} ({dataset_info['path']})")
    print(f"   - Ê®°Âûã: {args.model}")
    print(f"   - ÁºñÁ†ÅÂô®: {encoder_name} (ÁâπÂæÅÁª¥Â∫¶: {ENCODER_FEATURE_DIMS[encoder_name]})")
    print(f"   - GPU: {devices}‰∏™ (Á≠ñÁï•: {strategy})")
    print(f"   - ËÆ≠ÁªÉËΩÆÊï∞: {config.TRAINING.num_epochs}")
    print(f"   - ÊâπÊ¨°Â§ßÂ∞è: {config.DATA.train_dataloader.batch_size}")
    print(f"   - Â≠¶‰π†Áéá: {config.TRAINING.learning_rate}")
    
    return config








def main(config):
    print("begin main.py")
    print("config_infomation")
    print(config)
     
    seed = config.GENERAL.seed
    print(f"---seed: {seed}---")
    fix_seed(seed)

    print(f'mode: {config.mode}')
    print(f'expr_name: {config.expr_name}')
    print(f'data_path: {config.data_path}')
    print(f'encoder_name: {config.encoder_name}')

    # initialize dataset
    print(f'intializing dataset...')
    dataset = DataInterface(config)
    print(f'dataset: {dataset}')

    print(f'intializing model...')
    model = ModelInterface(config)
    print(f'model: {model}')

    print(f'intializing logger...')
    logger = load_loggers(config)
    print(f'logger: {logger}')

    print(f'intializing callbacks...')
    callbacks = load_callbacks(config)
    print(f'callbacks: {callbacks}')

    print(f'intializing trainer...')
    
    # Configure multi-GPU training strategy based on user selection
    # DDP (DistributedDataParallel) is the recommended strategy for multi-GPU training
    # as it provides better performance and memory efficiency compared to DataParallel
    strategy_config = config.strategy
    if config.devices > 1 and config.strategy == 'ddp':
        # Configure DDP strategy with advanced optimization parameters
        from pytorch_lightning.strategies import DDPStrategy
        strategy_config = DDPStrategy(
            # find_unused_parameters: Set to False for better performance when all parameters are used
            # Setting to True can help with debugging but may slow down training
            find_unused_parameters=getattr(getattr(config, 'MULTI_GPU', None), 'find_unused_parameters', False),
            # gradient_as_bucket_view: Memory optimization that reduces GPU memory usage
            # by storing gradients as views into buckets rather than separate tensors
            gradient_as_bucket_view=True,
            # static_graph: Set to False to support dynamic computational graphs
            # Required for models with conditional execution paths
            static_graph=False
        )
        print(f"ÈÖçÁΩÆDDPÁ≠ñÁï•: find_unused_parameters={getattr(getattr(config, 'MULTI_GPU', None), 'find_unused_parameters', False)}")
    
    # Configure gradient accumulation for handling large effective batch sizes
    # When GPU memory is limited, gradient accumulation allows training with larger
    # effective batch sizes by accumulating gradients over multiple mini-batches
    accumulate_grad_batches = 1
    if hasattr(config, 'MULTI_GPU') and hasattr(config.MULTI_GPU, 'accumulate_grad_batches'):
        accumulate_grad_batches = config.MULTI_GPU.accumulate_grad_batches
    
    # Initialize PyTorch Lightning Trainer with multi-GPU optimizations
    trainer = pl.Trainer(
        # Hardware configuration
        accelerator='gpu',  # Use GPU acceleration
        devices=config.devices,  # Number of GPUs to use
        max_epochs=config.TRAINING.num_epochs,  # Maximum training epochs
        
        # Logging and monitoring
        logger=logger,  # TensorBoard/WandB logger for experiment tracking
        check_val_every_n_epoch=1,  # Validate after every epoch
        callbacks=callbacks,  # Early stopping, checkpointing, etc.
        
        # Training optimizations
        precision='16-mixed',  # Mixed precision training for faster training and reduced memory
        strategy=strategy_config,  # Multi-GPU training strategy (DDP/DP)
        sync_batchnorm=config.sync_batchnorm,  # Synchronize BatchNorm statistics across GPUs
        accumulate_grad_batches=accumulate_grad_batches,  # Gradient accumulation steps
        
        # Progress monitoring and debugging options
        enable_progress_bar=True,  # Show training progress bar
        log_every_n_steps=50,  # Log metrics every N training steps
        gradient_clip_val=getattr(config.TRAINING, 'gradient_clip_val', 1.0),  # Gradient clipping for stability
        
        # Reproducibility settings
        deterministic=False,  # Set to True for fully deterministic training (may impact performance)
    )

    print(f'trainer: {trainer}')

    print(f'training...')
    if config.mode == 'train':
        trainer.fit(model, datamodule=dataset)

    return model

if __name__ == '__main__':
    parser = get_parse()
    args = parser.parse_args()
    
    # ÊûÑÂª∫ÈÖçÁΩÆÂπ∂ËøêË°åËÆ≠ÁªÉ
    config = build_config_from_args(args)

    main(config)
