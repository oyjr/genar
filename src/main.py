import os
import sys
import argparse
import logging
from datetime import datetime

# ç¡®ä¿å¯¼å…¥é¡¹ç›®ç›®å½•ä¸‹çš„æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import torch
import pytorch_lightning as pl

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from dataset.hest_dataset import STDataset
from model import ModelInterface
from utils import (
    load_callbacks,
    load_loggers,
    fix_seed
)
from torch.utils.data import DataLoader

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

torch.set_float32_matmul_precision('high')


# ç¼–ç å™¨ç‰¹å¾ç»´åº¦æ˜ å°„
ENCODER_FEATURE_DIMS = {
    'uni': 1024,
    'conch': 512
}

# æ•°æ®é›†é…ç½® - åŒ…å«è·¯å¾„å’Œslideåˆ’åˆ†ä¿¡æ¯
DATASETS = {
    'PRAD': {
        'path': '/data/ouyangjiarui/stem/hest1k_datasets/PRAD/',
        'val_slides': 'MEND139',
        'test_slides': 'MEND140',
        'recommended_encoder': 'uni'
    },
    'her2st': {
        'path': '/data/ouyangjiarui/stem/hest1k_datasets/her2st/',
        'val_slides': 'SPA148',
        'test_slides': 'SPA148', 
        'recommended_encoder': 'conch'
    }
}

# Multi-Scale Gene VAR æ¨¡å‹é…ç½®
VAR_ST_CONFIG = {
        'model_name': 'VAR_ST',
        'num_genes': 200,
        'histology_feature_dim': 1024,  # ä¾èµ–ç¼–ç å™¨
        'spatial_coord_dim': 2,
        
        # Multi-Scale VAR é…ç½® (å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬)
        'gene_patch_nums': (1, 2, 4, 6, 8, 10, 15),  # 7ä¸ªå°ºåº¦ï¼Œæœ€åä¸€ä¸ªæ”¹ä¸º14å‡å°‘åºåˆ—é•¿åº¦
        # vocab_size å°†æ ¹æ® max_gene_count åŠ¨æ€è®¡ç®— (max_gene_count + 1)
        'embed_dim': 512,  # å‡å°‘åµŒå…¥ç»´åº¦ 768->512
        'num_heads': 8,    # å‡å°‘æ³¨æ„åŠ›å¤´æ•° 12->8
        'num_layers': 8,   # å‡å°‘å±‚æ•° 12->8
        'mlp_ratio': 3.0,  # å‡å°‘MLPå€æ•° 4.0->3.0
        
        # Dropout å‚æ•°
        'drop_rate': 0.0,
        'attn_drop_rate': 0.0,
        'drop_path_rate': 0.1,
        
        # æ¡ä»¶ç›¸å…³å‚æ•°
        'condition_embed_dim': 512,  # åŒ¹é…embed_dim
        'cond_drop_rate': 0.1,
        
        # å…¶ä»–å‚æ•°
        'norm_eps': 1e-6,
        'shared_aln': False,
        'attn_l2_norm': True
}

# é»˜è®¤è®­ç»ƒé…ç½® 
DEFAULT_CONFIG = {
    'GENERAL': {
        'seed': 2021,
        'log_path': './logs', 
        'debug': False
    },
    'DATA': {
        'normalize': True,  # ä¿ç•™å‚æ•°å…¼å®¹æ€§ï¼Œå®é™…ä½¿ç”¨åŸå§‹åŸºå› è®¡æ•°
        'train_dataloader': {
            'batch_size': 256,
            'num_workers': 4,
            'pin_memory': True,
            'shuffle': True,
            'persistent_workers': True
        },
        'val_dataloader': {
            'batch_size': 64,  # ğŸ”§ è¿›ä¸€æ­¥å¢åŠ éªŒè¯æ‰¹æ¬¡å¤§å°åˆ°64ï¼Œæ˜¾è‘—åŠ é€ŸéªŒè¯
            'num_workers': 4,
            'pin_memory': True,
            'shuffle': False,
            'persistent_workers': True
        },
        'test_dataloader': {
            'batch_size': 64,  # ğŸ”§ åŒæ­¥å¢åŠ æµ‹è¯•æ‰¹æ¬¡å¤§å°åˆ°64
            'num_workers': 4,
            'pin_memory': True,
            'shuffle': False,
            'persistent_workers': True
        }
    },
    'TRAINING': {
        'num_epochs': 200,
        'learning_rate': 1.0e-4,
        'weight_decay': 1.0e-4,
        'mode': 'min',
        'monitor': 'val_loss',
        'lr_scheduler': {
            'patience': 0,  # é»˜è®¤ç¦ç”¨ï¼Œåªæœ‰å‘½ä»¤è¡ŒæŒ‡å®šæ—¶æ‰å¯ç”¨
            'factor': 0.5
        },
        'gradient_clip_val': 1.0
    },
    'CALLBACKS': {
        'early_stopping': {
            'monitor': 'val_loss',  # åŠ¨æ€æ›´æ–°ï¼šStage1ç”¨val_mse, Stage2ç”¨val_accuracy
            'patience': 10000,  # é»˜è®¤è®¾ç½®å¾ˆå¤§å€¼ï¼Œå®é™…ç¦ç”¨æ—©åœ
            'mode': 'min',      # åŠ¨æ€æ›´æ–°ï¼šStage1ç”¨min, Stage2ç”¨max
            'min_delta': 0.0
        },
        'model_checkpoint': {
            'monitor': 'val_loss',  # åŠ¨æ€æ›´æ–°ï¼šStage1ç”¨val_mse, Stage2ç”¨val_accuracy  
            'save_top_k': 1,
            'mode': 'min',          # åŠ¨æ€æ›´æ–°ï¼šStage1ç”¨min, Stage2ç”¨max
            'filename': 'best-epoch={epoch:02d}-{val_mse:.4f}'  # åŠ¨æ€æ›´æ–°ï¼šStage1å’ŒStage2ä½¿ç”¨ä¸åŒå‘½å
        },
        'learning_rate_monitor': {
            'logging_interval': 'epoch'
        }
    },
    'MULTI_GPU': {
        'find_unused_parameters': True,  # ğŸ”§ å¯ç”¨æœªä½¿ç”¨å‚æ•°æ£€æµ‹ï¼šVARæ¨¡å‹å¯èƒ½æœ‰æœªä½¿ç”¨å‚æ•°
        'accumulate_grad_batches': 1
    }
}


def get_parse():
    """
    Parse command line arguments for VAR_ST training.
    
    Returns:
        argparse.ArgumentParser: Configured argument parser with simplified parameters
    """
    parser = argparse.ArgumentParser(
        description='Simplified Training for Spatial Transcriptomics Models',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage
  python src/main.py --dataset PRAD --gpus 4
  
  # With custom parameters
  python src/main.py --dataset PRAD --encoder uni \\
      --gpus 4 --epochs 200 --batch_size 256 --lr 1e-4
  
  # Single GPU training
  python src/main.py --dataset her2st --gpus 1
  
  # Test mode with checkpoint
  python src/main.py --dataset her2st --mode test --gpus 1 \\
      --ckpt_path logs/her2st/VAR_ST/best-epoch=epoch=02-pcc50=val_pcc_50=0.7688.ckpt
        """
    )
    
    # === æ ¸å¿ƒå‚æ•° ===
    parser.add_argument('--dataset', type=str, choices=list(DATASETS.keys()),
                        help='æ•°æ®é›†åç§° (PRAD, her2st)')
    parser.add_argument('--encoder', type=str, choices=list(ENCODER_FEATURE_DIMS.keys()),
                        help='ç¼–ç å™¨ç±»å‹ (uni, conch)ï¼Œé»˜è®¤ä½¿ç”¨æ•°æ®é›†æ¨èç¼–ç å™¨')
    
    # === è®­ç»ƒå‚æ•° ===
    parser.add_argument('--gpus', type=int, default=1,
                        help='GPUæ•°é‡ (é»˜è®¤: 1)')
    parser.add_argument('--epochs', type=int,
                        help='è®­ç»ƒè½®æ•° (é»˜è®¤: 200)')
    parser.add_argument('--batch_size', type=int,
                        help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 256)')
    parser.add_argument('--lr', type=float,
                        help='å­¦ä¹ ç‡ (é»˜è®¤: 1e-4)')
    parser.add_argument('--weight-decay', type=float,
                        help='æƒé‡è¡°å‡ (é»˜è®¤: 1e-4)')
    parser.add_argument('--patience', type=int,
                        help='å­¦ä¹ ç‡è°ƒåº¦å™¨è€å¿ƒå€¼ (é»˜è®¤: ç¦ç”¨, åªæœ‰æŒ‡å®šæ—¶æ‰å¯ç”¨patienceæœºåˆ¶)')
    
    # === å¤šGPUå‚æ•° ===
    parser.add_argument('--strategy', type=str, default='auto',
                        choices=['auto', 'ddp', 'ddp_spawn', 'dp'],
                        help='å¤šGPUç­–ç•¥ (é»˜è®¤: autoï¼Œå¤šGPUæ—¶ä½¿ç”¨ddp)')
    parser.add_argument('--sync-batchnorm', action='store_true',
                        help='å¯ç”¨åŒæ­¥BatchNorm (å¤šGPUè®­ç»ƒæ¨è)')
    
    # === æ•°æ®å¢å¼ºå‚æ•° ===
    parser.add_argument('--use-augmented', action='store_true', default=True,
                        help='ä½¿ç”¨æ•°æ®å¢å¼º (é»˜è®¤: True)')
    parser.add_argument('--expand-augmented', action='store_true', default=True,
                        help='å±•å¼€å¢å¼ºæ•°æ®ä¸º7å€æ ·æœ¬ (é»˜è®¤: True)')
    
    # === ğŸ†• åŸºå› è®¡æ•°å‚æ•° ===
    parser.add_argument('--max-gene-count', type=int, default=500,
                        help='æœ€å¤§åŸºå› è®¡æ•°å€¼ (é»˜è®¤: 500)')
    
    # === å…¶ä»–å‚æ•° ===
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'],
                        help='è¿è¡Œæ¨¡å¼ (é»˜è®¤: train)')
    parser.add_argument('--seed', type=int,
                        help='éšæœºç§å­ (é»˜è®¤: 2021)')
    
    # === ğŸ†• æµ‹è¯•æ¨¡å¼å‚æ•° ===
    parser.add_argument('--ckpt_path', type=str,
                        help='æµ‹è¯•æ¨¡å¼æ—¶ä½¿ç”¨çš„checkpointè·¯å¾„ (å¿…é¡»åœ¨--mode testæ—¶æŒ‡å®š)')
    
    # === å‘åå…¼å®¹å‚æ•° (ä¿ç•™æœ€å°‘å¿…è¦çš„) ===
    parser.add_argument('--config', type=str,
                        help='[å·²å¼ƒç”¨] è¯·ä½¿ç”¨ --dataset å‚æ•°æ›¿ä»£')
    
    return parser


def build_config_from_args(args):
    """
    ä»ç®€åŒ–çš„å‘½ä»¤è¡Œå‚æ•°æ„å»ºå®Œæ•´é…ç½®
    
    Args:
        args: è§£æåçš„å‘½ä»¤è¡Œå‚æ•°
        
    Returns:
        å®Œæ•´çš„é…ç½®å¯¹è±¡
    """
    from addict import Dict
    
    # å¦‚æœä½¿ç”¨äº†åŸæœ‰çš„configå‚æ•°ï¼Œåˆ™ä½¿ç”¨åŸæœ‰é€»è¾‘
    if args.config:
        print("ğŸ”„ ä½¿ç”¨åŸæœ‰é…ç½®æ–‡ä»¶æ¨¡å¼")
        return None  # è¿”å›Noneè¡¨ç¤ºä½¿ç”¨åŸæœ‰é€»è¾‘
    
    # æ£€æŸ¥å¿…éœ€å‚æ•°
    if not args.dataset:
        raise ValueError("å¿…é¡»æŒ‡å®š --dataset å‚æ•°")
    
    if args.dataset not in DATASETS:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {args.dataset}ï¼Œæ”¯æŒçš„æ•°æ®é›†: {list(DATASETS.keys())}")
    
    # ğŸ†• æ£€æŸ¥æµ‹è¯•æ¨¡å¼å‚æ•°
    if args.mode == 'test' and not args.ckpt_path:
        raise ValueError("æµ‹è¯•æ¨¡å¼å¿…é¡»æŒ‡å®š --ckpt_path å‚æ•°")
    
    if args.ckpt_path and not os.path.exists(args.ckpt_path):
        raise ValueError(f"Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {args.ckpt_path}")
    
    print(f"ğŸš€ ä½¿ç”¨ç®€åŒ–é…ç½®æ¨¡å¼: æ•°æ®é›†={args.dataset}, æ¨¡å‹=VAR_ST, æ¨¡å¼={args.mode}")
    
    # è·å–æ•°æ®é›†ä¿¡æ¯
    dataset_info = DATASETS[args.dataset]
    
    # è·å–æ¨¡å‹ä¿¡æ¯
    model_info = VAR_ST_CONFIG
    
    # ç¡®å®šç¼–ç å™¨
    encoder_name = args.encoder or dataset_info['recommended_encoder']
    
    # ç¡®å®šGPUç›¸å…³å‚æ•°
    devices = args.gpus
    strategy = 'ddp' if devices > 1 and args.strategy == 'auto' else args.strategy
    sync_batchnorm = getattr(args, 'sync_batchnorm', False) or (devices > 1)
    
    # æ„å»ºå®Œæ•´é…ç½®
    config = Dict(DEFAULT_CONFIG)
    
    # æ›´æ–°æ—¥å¿—è·¯å¾„ä¸ºæ•°æ®é›†åç§°å’Œæ¨¡å‹åç§°
    config.GENERAL.log_path = f'./logs/{args.dataset}/VAR_ST'
    
    # æ›´æ–°æ¨¡å‹é…ç½®
    config.MODEL = Dict(model_info)
    config.MODEL.feature_dim = ENCODER_FEATURE_DIMS[encoder_name]
    # ğŸ”§ æ ¹æ®å‘½ä»¤è¡Œå‚æ•°æ›´æ–°åŸºå› æ•°é‡
    max_gene_count = getattr(args, 'max_gene_count', 500)
    # num_genesä¿æŒ200ä¸å˜ï¼Œä¸è¢«max_gene_countå½±å“
    
    # ğŸ†• åŠ¨æ€è®¡ç®—vocab_size = max_gene_count + 1 (å¯¹åº”0åˆ°max_gene_countçš„è®¡æ•°èŒƒå›´)
    vocab_size = max_gene_count + 1
    config.MODEL.vocab_size = vocab_size
    config.MODEL.max_gene_count = max_gene_count
    
    # æ›´æ–°è®­ç»ƒå‚æ•°
    if args.epochs:
        config.TRAINING.num_epochs = args.epochs
    if args.lr:
        config.TRAINING.learning_rate = args.lr
    if args.weight_decay:
        config.TRAINING.weight_decay = args.weight_decay
    batch_size = getattr(args, 'batch_size', None)
    if batch_size:
        config.DATA.train_dataloader.batch_size = batch_size
    if args.patience is not None:
        # åªæœ‰æ˜ç¡®æŒ‡å®špatienceæ—¶æ‰å¯ç”¨patienceæœºåˆ¶
        config.TRAINING.lr_scheduler.patience = args.patience
        # è®¾ç½®æ—©åœçš„patienceï¼ˆé€šå¸¸è®¾ä¸ºlr_scheduler patienceçš„2å€ï¼‰
        if args.patience == 0:
            # å¦‚æœæ˜ç¡®è®¾ä¸º0ï¼Œç¦ç”¨æ—©åœ
            config.CALLBACKS.early_stopping.patience = 10000
        else:
            # å¯ç”¨æ—©åœï¼Œè®¾ä¸ºpatienceçš„2å€
            config.CALLBACKS.early_stopping.patience = max(10, args.patience * 2)
    # å¦‚æœæ²¡æœ‰æŒ‡å®špatienceï¼Œä¿æŒé»˜è®¤çš„ç¦ç”¨çŠ¶æ€ï¼ˆpatience=0å’Œearly_stopping=10000ï¼‰
    
    # æ›´æ–°ç§å­
    if args.seed:
        config.GENERAL.seed = args.seed
    
    # è®¾ç½®æ•°æ®é›†ç›¸å…³å‚æ•°
    config.mode = args.mode
    config.expr_name = args.dataset
    config.data_path = dataset_info['path']
    config.slide_val = dataset_info['val_slides']
    config.slide_test = dataset_info['test_slides']
    config.encoder_name = encoder_name
    config.use_augmented = getattr(args, 'use_augmented', True)
    config.expand_augmented = getattr(args, 'expand_augmented', True)
    config.gene_count_mode = 'discrete_tokens'  # å›ºå®šä¸ºç¦»æ•£tokenæ¨¡å¼
    config.max_gene_count = getattr(args, 'max_gene_count', 500)
    
    # ğŸ†• è®¾ç½®checkpointè·¯å¾„
    if args.ckpt_path:
        config.ckpt_path = args.ckpt_path
    
    # è®¾ç½®å¤šGPUå‚æ•°
    config.devices = devices
    config.strategy = strategy
    config.sync_batchnorm = sync_batchnorm
    
    # è®¾ç½®æ—¶é—´æˆ³å’Œé…ç½®è·¯å¾„
    config.GENERAL.current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    config.config = 'built-in'  # æ ‡è®°ä¸ºå†…ç½®é…ç½®
    
    # æ£€æŸ¥patienceçŠ¶æ€
    lr_patience = config.TRAINING.lr_scheduler.patience
    early_patience = config.CALLBACKS.early_stopping.patience
    patience_status = "ç¦ç”¨" if lr_patience == 0 else f"å¯ç”¨ (LRè°ƒåº¦å™¨: {lr_patience}, æ—©åœ: {early_patience})"
    
    # VAR-STæ¨¡å‹é…ç½®
    config.MODEL.histology_feature_dim = ENCODER_FEATURE_DIMS[encoder_name]
    config.MODEL.gene_count_mode = config.gene_count_mode
    config.MODEL.max_gene_count = config.max_gene_count
    # ğŸ”§ æš‚æ—¶ä½¿ç”¨val_lossä½œä¸ºç›‘æ§æŒ‡æ ‡ï¼Œé¿å…ç¬¬ä¸€ä¸ªepochçš„EarlyStoppingé”™è¯¯
    # TODO: åç»­å¯ä»¥æ”¹å›val_pcc_50ï¼Œä½†éœ€è¦ç¡®ä¿ç¬¬ä¸€ä¸ªepochéªŒè¯å®Œæˆåæ‰æ£€æŸ¥
    config.TRAINING.monitor = 'val_loss'
    config.TRAINING.mode = 'min'
    config.CALLBACKS.early_stopping.monitor = 'val_loss'
    config.CALLBACKS.early_stopping.mode = 'min'
    config.CALLBACKS.model_checkpoint.monitor = 'val_loss'
    config.CALLBACKS.model_checkpoint.mode = 'min'
    config.CALLBACKS.model_checkpoint.filename = 'best-epoch={epoch:02d}-loss={val_loss:.6f}'
    print(f"   - VAR-STç›‘æ§æŒ‡æ ‡: val_loss (æœ€å°åŒ–) - ä¸´æ—¶ä½¿ç”¨ï¼Œé¿å…ç¬¬ä¸€ä¸ªepoché”™è¯¯")
    print(f"   - Checkpointæ–‡ä»¶åæ¨¡æ¿: best-epoch={{epoch:02d}}-loss={{val_loss:.6f}}")
    print(f"   - åŸºå› è®¡æ•°æ¨¡å¼: discrete_tokens (ä¿æŒåŸå§‹è®¡æ•°)")
    print(f"   - æœ€å¤§åŸºå› è®¡æ•°: {config.max_gene_count}")
    print(f"   - è¯æ±‡è¡¨å¤§å°: {vocab_size} (åŠ¨æ€è®¡ç®—: {max_gene_count} + 1)")
    
    print(f"âœ… é…ç½®æ„å»ºå®Œæˆ:")
    print(f"   - æ•°æ®é›†: {args.dataset} ({dataset_info['path']})")
    print(f"   - æ¨¡å‹: VAR_ST")
    print(f"   - ç¼–ç å™¨: {encoder_name} (ç‰¹å¾ç»´åº¦: {ENCODER_FEATURE_DIMS[encoder_name]})")
    print(f"   - GPU: {devices}ä¸ª (ç­–ç•¥: {strategy})")
    print(f"   - è®­ç»ƒè½®æ•°: {config.TRAINING.num_epochs}")
    print(f"   - æ‰¹æ¬¡å¤§å°: {config.DATA.train_dataloader.batch_size}")
    print(f"   - å­¦ä¹ ç‡: {config.TRAINING.learning_rate}")
    print(f"   - Patienceæœºåˆ¶: {patience_status}")
    print(f"   - åŸºå› è®¡æ•°èŒƒå›´: 0-{max_gene_count} (vocab_size: {vocab_size})")
    
    return config


def create_dataloaders(config):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    # åŸºç¡€å‚æ•°
    base_params = {
        'data_path': config.data_path,
        'expr_name': config.expr_name,
        'slide_val': config.slide_val,
        'slide_test': config.slide_test,
        'encoder_name': config.encoder_name,
        'use_augmented': config.use_augmented,
        'max_gene_count': getattr(config, 'max_gene_count', 500),
    }
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = STDataset(mode='train', expand_augmented=config.expand_augmented, **base_params)
    val_dataset = STDataset(mode='val', expand_augmented=False, **base_params)
    test_dataset = STDataset(mode='test', expand_augmented=False, **base_params)
    
    # åˆ›å»ºDataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.DATA.train_dataloader.batch_size,
        shuffle=config.DATA.train_dataloader.shuffle,
        num_workers=config.DATA.train_dataloader.num_workers,
        pin_memory=config.DATA.train_dataloader.pin_memory
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.DATA.val_dataloader.batch_size,
        shuffle=config.DATA.val_dataloader.shuffle,
        num_workers=config.DATA.val_dataloader.num_workers,
        pin_memory=config.DATA.val_dataloader.pin_memory
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.DATA.val_dataloader.batch_size,
        shuffle=False,
        num_workers=config.DATA.val_dataloader.num_workers,
        pin_memory=config.DATA.val_dataloader.pin_memory
    )
    
    return train_loader, val_loader, test_loader


def main(config):
    if config.mode == 'train':
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    else:
        print("ğŸ§ª å¼€å§‹æµ‹è¯•...")
    
    # è®¾ç½®éšæœºç§å­
    fix_seed(config.GENERAL.seed)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader = create_dataloaders(config)
    
    # åˆå§‹åŒ–ç»„ä»¶
    model = ModelInterface(config)
    logger = load_loggers(config)
    callbacks = load_callbacks(config)

    # é…ç½®å¤šGPUç­–ç•¥
    strategy_config = config.strategy
    if config.devices > 1 and config.strategy == 'ddp':
        from pytorch_lightning.strategies import DDPStrategy
        strategy_config = DDPStrategy(
            find_unused_parameters=config.MULTI_GPU.find_unused_parameters,
            gradient_as_bucket_view=True,
            static_graph=False
        )
    
    # é…ç½®æ¢¯åº¦ç´¯ç§¯
    accumulate_grad_batches = getattr(config.MULTI_GPU, 'accumulate_grad_batches', 1)
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = pl.Trainer(
        accelerator='gpu',
        devices=config.devices,
        max_epochs=config.TRAINING.num_epochs,
        logger=logger,
        callbacks=callbacks,
        precision=32,
        strategy=strategy_config,
        sync_batchnorm=config.sync_batchnorm,
        accumulate_grad_batches=accumulate_grad_batches,
        enable_progress_bar=True,
        log_every_n_steps=50,
        gradient_clip_val=config.TRAINING.gradient_clip_val,
        deterministic=False,
    )

    # æ ¹æ®æ¨¡å¼æ‰§è¡Œä¸åŒçš„æ“ä½œ
    if config.mode == 'train':
        trainer.fit(model, train_loader, val_loader)
    elif config.mode == 'test':
        print(f"ğŸ“‚ ä»checkpointåŠ è½½æ¨¡å‹: {config.ckpt_path}")
        trainer.test(model, test_loader, ckpt_path=config.ckpt_path)
        print("âœ… æµ‹è¯•å®Œæˆï¼")

    return model

if __name__ == '__main__':
    parser = get_parse()
    args = parser.parse_args()
    
    # æ„å»ºé…ç½®å¹¶è¿è¡Œè®­ç»ƒ
    config = build_config_from_args(args)

    main(config)
