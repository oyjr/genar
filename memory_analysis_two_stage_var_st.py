"""
Two-Stage VAR-ST å†…å­˜å ç”¨åˆ†æ
è¯¦ç»†åˆ†æStage 1å’ŒStage 2çš„å†…å­˜å ç”¨å·®å¼‚

è¿è¡Œæ­¤è„šæœ¬æ¥æµ‹é‡ï¼š
1. æ¨¡å‹å‚æ•°æ•°é‡
2. è®­ç»ƒæ—¶çš„GPUå†…å­˜å ç”¨
3. å‰å‘ä¼ æ’­çš„å†…å­˜å³°å€¼
4. æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜å ç”¨
"""

import sys
import os
sys.path.insert(0, 'src')

import torch
import torch.nn as nn
import psutil
import gc
from typing import Dict, Tuple
import tempfile

from model.VAR.two_stage_var_st import TwoStageVARST
from model.VAR.multi_scale_gene_vqvae import MultiScaleGeneVQVAE
from model.VAR.gene_var_transformer import GeneVARTransformer, ConditionProcessor


def get_gpu_memory_usage():
    """è·å–å½“å‰GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / (1024**3)  # GB
        reserved = torch.cuda.memory_reserved() / (1024**3)    # GB
        return allocated, reserved
    return 0, 0


def get_model_parameters(model: nn.Module) -> Dict[str, int]:
    """è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    frozen_params = total_params - trainable_params
    
    return {
        'total': total_params,
        'trainable': trainable_params,
        'frozen': frozen_params
    }


def analyze_stage1_memory():
    """åˆ†æStage 1 (VQVAE) çš„å†…å­˜å ç”¨"""
    print("ğŸ” Stage 1 (VQVAE) å†…å­˜åˆ†æ")
    print("=" * 50)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
    
    # åˆ›å»ºStage 1æ¨¡å‹
    stage1_model = TwoStageVARST(
        num_genes=200,
        current_stage=1,
        device=device
    )
    stage1_model = stage1_model.to(device)
    
    # åˆ†ææ¨¡å‹å‚æ•°
    stage1_params = get_model_parameters(stage1_model)
    vqvae_params = get_model_parameters(stage1_model.stage1_vqvae)
    
    print(f"ğŸ“Š Stage 1 æ¨¡å‹å‚æ•°:")
    print(f"   å®Œæ•´æ¨¡å‹å‚æ•°: {stage1_params['total']:,}")
    print(f"   VQVAEå‚æ•°: {vqvae_params['total']:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {stage1_params['trainable']:,}")
    
    # åˆ†æVQVAEå„ç»„ä»¶å‚æ•°
    components_params = {}
    components_params['decomposer'] = get_model_parameters(stage1_model.stage1_vqvae.decomposer)['total']
    components_params['encoders'] = get_model_parameters(stage1_model.stage1_vqvae.encoders)['total']
    components_params['quantizer'] = get_model_parameters(stage1_model.stage1_vqvae.shared_quantizer)['total']
    components_params['decoders'] = get_model_parameters(stage1_model.stage1_vqvae.decoders)['total']
    components_params['reconstructor'] = get_model_parameters(stage1_model.stage1_vqvae.reconstructor)['total']
    
    print(f"\nğŸ”§ VQVAEç»„ä»¶å‚æ•°åˆ†å¸ƒ:")
    for component, params in components_params.items():
        percentage = (params / vqvae_params['total']) * 100
        print(f"   {component}: {params:,} ({percentage:.1f}%)")
    
    if torch.cuda.is_available():
        allocated_after_model, reserved_after_model = get_gpu_memory_usage()
        print(f"\nğŸ’¾ æ¨¡å‹åŠ è½½åGPUå†…å­˜:")
        print(f"   å·²åˆ†é…: {allocated_after_model:.2f} GB")
        print(f"   å·²ä¿ç•™: {reserved_after_model:.2f} GB")
    
    # æ¨¡æ‹Ÿè®­ç»ƒ
    batch_sizes = [8, 16, 32, 64]
    print(f"\nğŸ‹ï¸ Stage 1 è®­ç»ƒå†…å­˜å ç”¨ (ä¸åŒbatch size):")
    
    for batch_size in batch_sizes:
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.reset_peak_memory_stats()
            
            # åˆ›å»ºè®­ç»ƒæ•°æ®
            gene_expression = torch.randn(batch_size, 200, device=device)
            
            # å‰å‘ä¼ æ’­
            stage1_model.train()
            output = stage1_model(gene_expression)
            loss = output['loss']
            
            if torch.cuda.is_available():
                allocated_forward, _ = get_gpu_memory_usage()
                peak_forward = torch.cuda.max_memory_allocated() / (1024**3)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            if torch.cuda.is_available():
                allocated_backward, _ = get_gpu_memory_usage()
                peak_backward = torch.cuda.max_memory_allocated() / (1024**3)
                
                print(f"   Batch {batch_size:2d}: å‰å‘ {peak_forward:.2f}GB, åå‘ {peak_backward:.2f}GB")
            else:
                print(f"   Batch {batch_size:2d}: CPUæ¨¡å¼ï¼Œæ— GPUå†…å­˜ç»Ÿè®¡")
            
            # æ¸…ç†
            del gene_expression, output, loss
            stage1_model.zero_grad()
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"   Batch {batch_size:2d}: âŒ GPUå†…å­˜ä¸è¶³")
                break
            else:
                raise
    
    return stage1_params, components_params


def analyze_stage2_memory():
    """åˆ†æStage 2 (VAR Transformer) çš„å†…å­˜å ç”¨"""
    print("\nğŸ” Stage 2 (VAR Transformer) å†…å­˜åˆ†æ")
    print("=" * 50)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
    
    # é¦–å…ˆåˆ›å»ºå¹¶ä¿å­˜Stage 1æ¨¡å‹
    with tempfile.TemporaryDirectory() as tmp_dir:
        ckpt_path = os.path.join(tmp_dir, "stage1_for_memory_test.ckpt")
        
        stage1_model = TwoStageVARST(num_genes=200, current_stage=1, device=device)
        stage1_model = stage1_model.to(device)
        stage1_model.save_stage_checkpoint(ckpt_path, stage=1)
        
        # åˆ›å»ºStage 2æ¨¡å‹
        stage2_model = TwoStageVARST(
            num_genes=200,
            histology_feature_dim=1024,
            spatial_coord_dim=2,
            current_stage=2,
            stage1_ckpt_path=ckpt_path,
            device=device
        )
        stage2_model = stage2_model.to(device)
        
        # åˆ†ææ¨¡å‹å‚æ•°
        stage2_params = get_model_parameters(stage2_model)
        vqvae_params = get_model_parameters(stage2_model.stage1_vqvae)
        var_params = get_model_parameters(stage2_model.stage2_var)
        condition_params = get_model_parameters(stage2_model.condition_processor)
        
        print(f"ğŸ“Š Stage 2 æ¨¡å‹å‚æ•°:")
        print(f"   å®Œæ•´æ¨¡å‹å‚æ•°: {stage2_params['total']:,}")
        print(f"   VQVAEå‚æ•° (å†»ç»“): {vqvae_params['total']:,}")
        print(f"   VAR Transformerå‚æ•°: {var_params['total']:,}")
        print(f"   æ¡ä»¶å¤„ç†å™¨å‚æ•°: {condition_params['total']:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {stage2_params['trainable']:,}")
        print(f"   å†»ç»“å‚æ•°: {stage2_params['frozen']:,}")
        
        # åˆ†æVAR Transformerç»„ä»¶å‚æ•°
        var_components = {}
        var_components['token_embedding'] = get_model_parameters(stage2_model.stage2_var.token_embedding)['total']
        var_components['transformer_decoder'] = get_model_parameters(stage2_model.stage2_var.transformer_decoder)['total']
        var_components['output_projection'] = get_model_parameters(stage2_model.stage2_var.output_projection)['total']
        var_components['condition_projection'] = get_model_parameters(stage2_model.stage2_var.condition_projection)['total']
        
        print(f"\nğŸ”§ VAR Transformerç»„ä»¶å‚æ•°åˆ†å¸ƒ:")
        for component, params in var_components.items():
            percentage = (params / var_params['total']) * 100
            print(f"   {component}: {params:,} ({percentage:.1f}%)")
        
        if torch.cuda.is_available():
            allocated_after_model, reserved_after_model = get_gpu_memory_usage()
            print(f"\nğŸ’¾ æ¨¡å‹åŠ è½½åGPUå†…å­˜:")
            print(f"   å·²åˆ†é…: {allocated_after_model:.2f} GB")
            print(f"   å·²ä¿ç•™: {reserved_after_model:.2f} GB")
        
        # æ¨¡æ‹Ÿè®­ç»ƒ
        batch_sizes = [4, 8, 16, 32]  # Stage 2é€šå¸¸éœ€è¦æ›´å°çš„batch size
        print(f"\nğŸ‹ï¸ Stage 2 è®­ç»ƒå†…å­˜å ç”¨ (ä¸åŒbatch size):")
        
        for batch_size in batch_sizes:
            try:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.reset_peak_memory_stats()
                
                # åˆ›å»ºè®­ç»ƒæ•°æ®
                gene_expression = torch.randn(batch_size, 200, device=device)
                histology_features = torch.randn(batch_size, 1024, device=device)
                spatial_coords = torch.randn(batch_size, 2, device=device)
                
                # å‰å‘ä¼ æ’­
                stage2_model.train()
                output = stage2_model(
                    gene_expression=gene_expression,
                    histology_features=histology_features,
                    spatial_coords=spatial_coords
                )
                loss = output['loss']
                
                if torch.cuda.is_available():
                    allocated_forward, _ = get_gpu_memory_usage()
                    peak_forward = torch.cuda.max_memory_allocated() / (1024**3)
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                if torch.cuda.is_available():
                    allocated_backward, _ = get_gpu_memory_usage()
                    peak_backward = torch.cuda.max_memory_allocated() / (1024**3)
                    
                    print(f"   Batch {batch_size:2d}: å‰å‘ {peak_forward:.2f}GB, åå‘ {peak_backward:.2f}GB")
                else:
                    print(f"   Batch {batch_size:2d}: CPUæ¨¡å¼ï¼Œæ— GPUå†…å­˜ç»Ÿè®¡")
                
                # æ¸…ç†
                del gene_expression, histology_features, spatial_coords, output, loss
                stage2_model.zero_grad()
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"   Batch {batch_size:2d}: âŒ GPUå†…å­˜ä¸è¶³")
                    break
                else:
                    raise
        
        return stage2_params, var_components


def compare_memory_usage():
    """å¯¹æ¯”Stage 1å’ŒStage 2çš„å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    print("\n" + "=" * 70)
    print("ğŸ“Š å†…å­˜å ç”¨å¯¹æ¯”åˆ†æ")
    print("=" * 70)
    
    # åˆ†æStage 1
    stage1_params, stage1_components = analyze_stage1_memory()
    
    # åˆ†æStage 2  
    stage2_params, stage2_components = analyze_stage2_memory()
    
    # å¯¹æ¯”åˆ†æ
    print("\nğŸ“ˆ å†…å­˜å ç”¨å¯¹æ¯”:")
    print("=" * 50)
    
    print(f"ğŸ”¸ æ¨¡å‹å‚æ•°å¯¹æ¯”:")
    print(f"   Stage 1æ€»å‚æ•°: {stage1_params['total']:,}")
    print(f"   Stage 2æ€»å‚æ•°: {stage2_params['total']:,}")
    param_ratio = stage2_params['total'] / stage1_params['total']
    print(f"   Stage 2 / Stage 1: {param_ratio:.2f}x")
    
    print(f"\nğŸ”¸ å¯è®­ç»ƒå‚æ•°å¯¹æ¯”:")
    print(f"   Stage 1å¯è®­ç»ƒ: {stage1_params['trainable']:,}")
    print(f"   Stage 2å¯è®­ç»ƒ: {stage2_params['trainable']:,}")
    trainable_ratio = stage2_params['trainable'] / stage1_params['trainable']
    print(f"   Stage 2 / Stage 1: {trainable_ratio:.2f}x")
    
    print(f"\nğŸ”¸ å…³é”®è§‚å¯Ÿ:")
    print(f"   â€¢ Stage 2åŒ…å«å®Œæ•´çš„Stage 1æ¨¡å‹(å†»ç»“)")
    print(f"   â€¢ Stage 2é¢å¤–å¢åŠ VAR Transformer: {stage2_params['total'] - stage1_params['total']:,} å‚æ•°")
    print(f"   â€¢ VAR Transformerå‚æ•°å Stage 2æ€»å‚æ•°çš„ {((stage2_params['total'] - stage1_params['total']) / stage2_params['total'] * 100):.1f}%")
    
    # å†…å­˜ä½¿ç”¨å»ºè®®
    print(f"\nğŸ’¡ å†…å­˜ä½¿ç”¨å»ºè®®:")
    print(f"   ğŸ”¹ Stage 1è®­ç»ƒ:")
    print(f"      - æ¨èbatch size: 16-64")
    print(f"      - ç›¸å¯¹å†…å­˜å‹å¥½ï¼Œä¸»è¦æ˜¯VQVAEè®¡ç®—")
    print(f"      - é€‚åˆè¾ƒå¤§çš„batch sizeè¿›è¡Œç¨³å®šè®­ç»ƒ")
    
    print(f"   ğŸ”¹ Stage 2è®­ç»ƒ:")
    print(f"      - æ¨èbatch size: 4-16")
    print(f"      - å†…å­˜éœ€æ±‚æ›´é«˜ ({param_ratio:.1f}xå‚æ•°é‡)")
    print(f"      - Transformerè‡ªæ³¨æ„åŠ›æœºåˆ¶å†…å­˜å¤æ‚åº¦é«˜")
    print(f"      - éœ€è¦è½½å…¥å†»ç»“çš„Stage 1æ¨¡å‹")
    
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"\nğŸ–¥ï¸  å½“å‰GPUå†…å­˜: {total_memory:.1f} GB")
        
        if total_memory < 16:
            print("   âš ï¸  GPUå†…å­˜è¾ƒå°ï¼Œå»ºè®®ï¼š")
            print("      - Stage 1: batch_size â‰¤ 32")
            print("      - Stage 2: batch_size â‰¤ 8")
        elif total_memory < 24:
            print("   âœ… GPUå†…å­˜ä¸­ç­‰ï¼Œå»ºè®®ï¼š")
            print("      - Stage 1: batch_size â‰¤ 64") 
            print("      - Stage 2: batch_size â‰¤ 16")
        else:
            print("   ğŸš€ GPUå†…å­˜å……è¶³ï¼Œå»ºè®®ï¼š")
            print("      - Stage 1: batch_size â‰¤ 128")
            print("      - Stage 2: batch_size â‰¤ 32")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ Two-Stage VAR-ST å†…å­˜å ç”¨åˆ†æ")
    print("=" * 70)
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"ğŸ–¥ï¸  GPU: {gpu_name}")
        print(f"ğŸ’¾ GPUå†…å­˜: {gpu_memory:.1f} GB")
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œåˆ†æ")
    
    try:
        compare_memory_usage()
        
        print("\n" + "=" * 70)
        print("âœ… å†…å­˜åˆ†æå®Œæˆï¼")
        print("=" * 70)
        
    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 