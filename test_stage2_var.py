"""
Stage 2åŸºå› VAR Transformeræµ‹è¯•è„šæœ¬

æµ‹è¯•ä»¥ä¸‹åŠŸèƒ½ï¼š
1. ConditionProcessoræ¡ä»¶å¤„ç†å™¨
2. GeneVARTransformeræ¨¡å‹æ¶æ„
3. Stage2Trainerè®­ç»ƒæµç¨‹
4. æ¡ä»¶ç”ŸæˆåŠŸèƒ½
5. ä¸Stage 1 VQVAEçš„é›†æˆ

éªŒè¯Stage 2çš„å‡†å¤‡æƒ…å†µ
"""

import sys
import os
sys.path.append('src')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from model.VAR.gene_var_transformer import (
    ConditionProcessor, 
    GeneVARTransformer, 
    Stage2Trainer
)
from model.VAR.multi_scale_gene_vqvae import MultiScaleGeneVQVAE


def test_condition_processor():
    """æµ‹è¯•æ¡ä»¶å¤„ç†å™¨"""
    print("ğŸ¯ æµ‹è¯• ConditionProcessor...")
    
    # åˆå§‹åŒ–æ¡ä»¶å¤„ç†å™¨
    processor = ConditionProcessor(
        histology_dim=1024,
        spatial_dim=2,
        histology_hidden_dim=512,
        spatial_hidden_dim=128,
        condition_embed_dim=640
    )
    
    print(f"   å‚æ•°æ•°é‡: {sum(p.numel() for p in processor.parameters())}")
    
    # æµ‹è¯•æ•°æ®
    B = 8
    histology_features = torch.randn(B, 1024)
    spatial_coords = torch.rand(B, 2)
    
    print(f"   è¾“å…¥ - ç»„ç»‡å­¦ç‰¹å¾: {histology_features.shape}")
    print(f"   è¾“å…¥ - ç©ºé—´åæ ‡: {spatial_coords.shape}")
    
    # å‰å‘ä¼ æ’­
    condition_embed = processor(histology_features, spatial_coords)
    
    print(f"   è¾“å‡º - æ¡ä»¶åµŒå…¥: {condition_embed.shape}")
    assert condition_embed.shape == (B, 640), f"æ¡ä»¶åµŒå…¥å½¢çŠ¶é”™è¯¯: {condition_embed.shape}"
    
    # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°
    for batch_size in [1, 4, 16, 32]:
        hist_feat = torch.randn(batch_size, 1024)
        spatial = torch.rand(batch_size, 2)
        embed = processor(hist_feat, spatial)
        assert embed.shape == (batch_size, 640), f"æ‰¹æ¬¡å¤§å°{batch_size}æµ‹è¯•å¤±è´¥"
    
    print("   âœ… ConditionProcessoræµ‹è¯•é€šè¿‡ï¼")


def test_gene_var_transformer():
    """æµ‹è¯•åŸºå› VAR Transformer"""
    print("ğŸ¤– æµ‹è¯• GeneVARTransformer...")
    
    # åˆå§‹åŒ–æ¨¡å‹
    transformer = GeneVARTransformer(
        vocab_size=4096,
        embed_dim=640,
        num_heads=8,
        num_layers=6,  # å‡å°‘å±‚æ•°ç”¨äºæµ‹è¯•
        feedforward_dim=2560,
        dropout=0.1,
        max_sequence_length=1500,
        condition_embed_dim=640
    )
    
    print(f"   å‚æ•°æ•°é‡: {sum(p.numel() for p in transformer.parameters())}")
    
    # æµ‹è¯•æ•°æ®
    B = 4
    seq_len = 1446  # å¤šå°ºåº¦tokenæ€»æ•°: 1+8+32+32*32*200 = 1446
    vocab_size = 4096
    
    input_tokens = torch.randint(0, vocab_size, (B, seq_len))
    condition_embed = torch.randn(B, 640)
    target_tokens = torch.randint(0, vocab_size, (B, seq_len))
    
    print(f"   è¾“å…¥ - Tokenåºåˆ—: {input_tokens.shape}")
    print(f"   è¾“å…¥ - æ¡ä»¶åµŒå…¥: {condition_embed.shape}")
    print(f"   è¾“å…¥ - ç›®æ ‡Token: {target_tokens.shape}")
    
    # å‰å‘ä¼ æ’­ (è®­ç»ƒæ¨¡å¼)
    outputs = transformer(input_tokens, condition_embed, target_tokens)
    
    print(f"   è¾“å‡º - Logits: {outputs['logits'].shape}")
    print(f"   è¾“å‡º - Loss: {outputs['loss'].item():.4f}")
    print(f"   è¾“å‡º - Accuracy: {outputs['accuracy'].item():.4f}")
    
    assert outputs['logits'].shape == (B, seq_len, vocab_size), f"Logitså½¢çŠ¶é”™è¯¯: {outputs['logits'].shape}"
    assert 'loss' in outputs, "ç¼ºå°‘lossè¾“å‡º"
    assert 'accuracy' in outputs, "ç¼ºå°‘accuracyè¾“å‡º"
    
    # å‰å‘ä¼ æ’­ (æ¨ç†æ¨¡å¼)
    outputs_inference = transformer(input_tokens, condition_embed)
    assert 'loss' not in outputs_inference, "æ¨ç†æ¨¡å¼ä¸åº”è¯¥æœ‰loss"
    assert outputs_inference['logits'].shape == (B, seq_len, vocab_size)
    
    print("   âœ… GeneVARTransformerå‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡ï¼")
    
    # æµ‹è¯•ç”ŸæˆåŠŸèƒ½
    print("   ğŸ² æµ‹è¯•ç”ŸæˆåŠŸèƒ½...")
    generated_tokens = transformer.generate(
        condition_embed=condition_embed,
        max_length=100,  # è¾ƒçŸ­é•¿åº¦ç”¨äºæµ‹è¯•
        temperature=1.0
    )
    
    print(f"   ç”Ÿæˆçš„Tokenåºåˆ—: {generated_tokens.shape}")
    assert generated_tokens.shape == (B, 100), f"ç”Ÿæˆåºåˆ—å½¢çŠ¶é”™è¯¯: {generated_tokens.shape}"
    print("   âœ… ç”ŸæˆåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")


def test_stage2_trainer():
    """æµ‹è¯•Stage 2è®­ç»ƒå™¨"""
    print("ğŸ‹ï¸ æµ‹è¯• Stage2Trainer...")
    
    # åˆ›å»ºæµ‹è¯•ç”¨çš„Stage 1 VQVAEæ¨¡å‹
    vqvae_model = MultiScaleGeneVQVAE(
        vocab_size=256,  # è¾ƒå°çš„è¯æ±‡è¡¨ç”¨äºæµ‹è¯•
        embed_dim=64,
        beta=0.25
    )
    
    # åˆ›å»ºæ¡ä»¶å¤„ç†å™¨
    condition_processor = ConditionProcessor(
        histology_dim=1024,
        spatial_dim=2,
        condition_embed_dim=512  # è¾ƒå°ç»´åº¦ç”¨äºæµ‹è¯•
    )
    
    # åˆ›å»ºVAR Transformer
    var_transformer = GeneVARTransformer(
        vocab_size=256,
        embed_dim=512,
        num_heads=4,
        num_layers=2,  # æå°‘å±‚æ•°ç”¨äºå¿«é€Ÿæµ‹è¯•
        feedforward_dim=1024,
        condition_embed_dim=512
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Stage2Trainer(
        vqvae_model=vqvae_model,
        var_transformer=var_transformer,
        condition_processor=condition_processor,
        device=torch.device('cpu'),  # ä½¿ç”¨CPUæµ‹è¯•
        learning_rate=1e-4,
        print_freq=1
    )
    
    print("   âœ… Stage2Traineråˆå§‹åŒ–æˆåŠŸï¼")
    
    # éªŒè¯VQVAEè¢«å†»ç»“
    vqvae_trainable = sum(p.requires_grad for p in trainer.vqvae_model.parameters())
    transformer_trainable = sum(p.requires_grad for p in trainer.var_transformer.parameters())
    condition_trainable = sum(p.requires_grad for p in trainer.condition_processor.parameters())
    
    print(f"   VQVAEå¯è®­ç»ƒå‚æ•°: {vqvae_trainable}")
    print(f"   Transformerå¯è®­ç»ƒå‚æ•°: {transformer_trainable}")
    print(f"   æ¡ä»¶å¤„ç†å™¨å¯è®­ç»ƒå‚æ•°: {condition_trainable}")
    
    assert vqvae_trainable == 0, "VQVAEåº”è¯¥è¢«å†»ç»“"
    assert transformer_trainable > 0, "Transformeråº”è¯¥å¯è®­ç»ƒ"
    assert condition_trainable > 0, "æ¡ä»¶å¤„ç†å™¨åº”è¯¥å¯è®­ç»ƒ"
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    B, gene_dim = 16, 200
    gene_expressions = torch.randn(B, gene_dim)
    histology_features = torch.randn(B, 1024)
    spatial_coords = torch.rand(B, 2)
    
    train_dataset = TensorDataset(gene_expressions, histology_features, spatial_coords)
    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    
    print(f"   è®­ç»ƒæ•°æ®: {len(train_dataset)}ä¸ªæ ·æœ¬, æ‰¹æ¬¡å¤§å°: 8")
    
    # æµ‹è¯•è®­ç»ƒä¸€ä¸ªepoch
    print("   å¼€å§‹è®­ç»ƒepoch...")
    train_stats = trainer.train_epoch(train_dataloader, epoch=1)
    
    print(f"   âœ… è®­ç»ƒLoss: {train_stats['loss']:.4f}")
    print(f"   âœ… è®­ç»ƒAccuracy: {train_stats['accuracy']:.4f}")
    
    # æµ‹è¯•éªŒè¯
    print("   å¼€å§‹éªŒè¯epoch...")
    val_stats = trainer.validate_epoch(train_dataloader, epoch=1)
    
    print(f"   âœ… éªŒè¯Loss: {val_stats['loss']:.4f}")
    print(f"   âœ… éªŒè¯Accuracy: {val_stats['accuracy']:.4f}")
    
    print("   âœ… Stage2Traineræµ‹è¯•é€šè¿‡ï¼")


def test_checkpoint_functionality():
    """æµ‹è¯•checkpointåŠŸèƒ½"""
    print("ğŸ’¾ æµ‹è¯• CheckpointåŠŸèƒ½...")
    
    # åˆ›å»ºç®€åŒ–æ¨¡å‹
    vqvae_model = MultiScaleGeneVQVAE(vocab_size=256, embed_dim=64)
    condition_processor = ConditionProcessor(condition_embed_dim=256)
    var_transformer = GeneVARTransformer(
        vocab_size=256, 
        embed_dim=256, 
        num_heads=2, 
        num_layers=1,
        condition_embed_dim=256
    )
    
    trainer = Stage2Trainer(
        vqvae_model=vqvae_model,
        var_transformer=var_transformer,
        condition_processor=condition_processor,
        device=torch.device('cpu')
    )
    
    # ä¿å­˜checkpoint
    checkpoint_path = "test_stage2_checkpoint.pth"
    trainer.save_checkpoint(checkpoint_path, epoch=5, metadata={"test": "stage2"})
    
    # ä¿®æ”¹ä¸€äº›å‚æ•°
    original_weight = trainer.var_transformer.token_embedding.weight.clone()
    trainer.var_transformer.token_embedding.weight.data.fill_(0.5)
    
    # åŠ è½½checkpoint
    checkpoint = trainer.load_checkpoint(checkpoint_path)
    loaded_weight = trainer.var_transformer.token_embedding.weight.clone()
    
    # éªŒè¯å‚æ•°æ¢å¤
    assert torch.allclose(original_weight, loaded_weight), "CheckpointåŠ è½½å¤±è´¥"
    assert checkpoint['epoch'] == 5, f"Epochä¸åŒ¹é…: {checkpoint['epoch']}"
    assert checkpoint['metadata']['test'] == "stage2", "Metadataä¸åŒ¹é…"
    
    # æ¸…ç†
    os.remove(checkpoint_path)
    
    print("   âœ… Checkpointæµ‹è¯•é€šè¿‡ï¼")


def test_integration_with_stage1():
    """æµ‹è¯•ä¸Stage 1çš„é›†æˆ"""
    print("ğŸ”— æµ‹è¯•ä¸Stage 1é›†æˆ...")
    
    # åˆ›å»ºStage 1æ¨¡å‹å¹¶è¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­
    vqvae_model = MultiScaleGeneVQVAE(vocab_size=4096, embed_dim=128)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    B = 4
    gene_expressions = torch.randn(B, 200)
    
    # Stage 1ç¼–ç 
    with torch.no_grad():
        vqvae_result = vqvae_model(gene_expressions)
        tokens = vqvae_result['tokens']
    
    print(f"   Stage 1è¾“å‡ºtokens:")
    total_tokens = 0
    for scale, scale_tokens in tokens.items():
        print(f"     {scale}: {scale_tokens.shape}")
        total_tokens += scale_tokens.numel() // B
    
    print(f"   æ¯ä¸ªæ ·æœ¬çš„æ€»tokenæ•°: {total_tokens}")
    
    # å±•å¹³tokensä¸ºåºåˆ—
    token_sequence = []
    for scale in ['global', 'pathway', 'module', 'individual']:
        scale_tokens = tokens[scale].view(B, -1)
        token_sequence.append(scale_tokens)
    
    full_token_sequence = torch.cat(token_sequence, dim=1)
    print(f"   å±•å¹³åçš„tokenåºåˆ—: {full_token_sequence.shape}")
    
    # åˆ›å»ºStage 2æ¨¡å‹
    condition_processor = ConditionProcessor()
    var_transformer = GeneVARTransformer(
        vocab_size=4096,
        max_sequence_length=full_token_sequence.shape[1] + 100
    )
    
    # åˆ›å»ºæ¡ä»¶ä¿¡æ¯
    histology_features = torch.randn(B, 1024)
    spatial_coords = torch.rand(B, 2)
    condition_embed = condition_processor(histology_features, spatial_coords)
    
    # Stage 2å‰å‘ä¼ æ’­
    outputs = var_transformer(full_token_sequence, condition_embed, full_token_sequence)
    
    print(f"   Stage 2è¾“å‡º:")
    print(f"     Logits: {outputs['logits'].shape}")
    print(f"     Loss: {outputs['loss'].item():.4f}")
    
    # æµ‹è¯•ç”Ÿæˆ
    generated = var_transformer.generate(
        condition_embed=condition_embed,
        max_length=full_token_sequence.shape[1],
        temperature=1.0
    )
    
    print(f"   ç”Ÿæˆçš„tokenåºåˆ—: {generated.shape}")
    
    print("   âœ… ä¸Stage 1é›†æˆæµ‹è¯•é€šè¿‡ï¼")


def test_different_input_formats():
    """æµ‹è¯•ä¸åŒè¾“å…¥æ ¼å¼"""
    print("ğŸ“ æµ‹è¯•ä¸åŒè¾“å…¥æ ¼å¼...")
    
    condition_processor = ConditionProcessor()
    
    # æµ‹è¯•ä¸åŒçš„è¾“å…¥æ ¼å¼
    test_cases = [
        ("æ ‡å‡†è¾“å…¥", torch.randn(4, 1024), torch.rand(4, 2)),
        ("å•æ ·æœ¬", torch.randn(1, 1024), torch.rand(1, 2)),
        ("å¤§æ‰¹æ¬¡", torch.randn(32, 1024), torch.rand(32, 2))
    ]
    
    for name, hist_feat, spatial in test_cases:
        embed = condition_processor(hist_feat, spatial)
        print(f"   {name}: {hist_feat.shape} + {spatial.shape} â†’ {embed.shape}")
        assert embed.shape[0] == hist_feat.shape[0], f"{name}æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…"
        assert embed.shape[1] == 640, f"{name}åµŒå…¥ç»´åº¦é”™è¯¯"
    
    print("   âœ… ä¸åŒè¾“å…¥æ ¼å¼æµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    print("ğŸ§ª å¼€å§‹Stage 2åŸºå› VAR Transformeræµ‹è¯•\n")
    
    try:
        test_condition_processor()
        print()
        
        test_gene_var_transformer()
        print()
        
        test_stage2_trainer()
        print()
        
        test_checkpoint_functionality()
        print()
        
        test_integration_with_stage1()
        print()
        
        test_different_input_formats()
        print()
        
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ“Š Stage 2 VAR Transformeræµ‹è¯•æ‘˜è¦:")
        print("   - æ¡ä»¶å¤„ç†å™¨: âœ… æ­£å¸¸")
        print("   - VAR Transformer: âœ… æ­£å¸¸")
        print("   - è®­ç»ƒå™¨åŠŸèƒ½: âœ… æ­£å¸¸")
        print("   - CheckpointåŠŸèƒ½: âœ… æ­£å¸¸")
        print("   - Stage 1é›†æˆ: âœ… æ­£å¸¸")
        print("   - ç”ŸæˆåŠŸèƒ½: âœ… æ­£å¸¸")
        print()
        print("âœ… Step 3 åŸºå› VAR Transformeråˆ›å»ºå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc() 