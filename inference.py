#!/usr/bin/env python3
"""
VAR_STæ¨¡å‹æ¨ç†è„šæœ¬
ç”¨äºåŠ è½½è®­ç»ƒå¥½çš„checkpointå¹¶åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°
"""

import os
import sys
import argparse
import logging
import numpy as np
import torch
import pytorch_lightning as pl
from typing import Dict, List, Tuple
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from dataset.data_interface import DataInterface
from model import ModelInterface

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æœ€å°æ–¹å·®é˜ˆå€¼
MIN_VARIANCE_THRESHOLD = 1e-8

# æ•°æ®é›†é…ç½®
DATASETS = {
    'PRAD': {
        'path': '/data/ouyangjiarui/stem/hest1k_datasets/PRAD/',
        'val_slides': 'MEND139',
        'test_slides': 'MEND140',
        'recommended_encoder': 'uni'
    },
    'her2st': {
        'path': '/data/ouyangjiarui/stem/hest1k_datasets/her2st/',
        'val_slides': 'SPA148',
        'test_slides': 'SPA148', 
        'recommended_encoder': 'conch'
    }
}

# ç¼–ç å™¨ç‰¹å¾ç»´åº¦æ˜ å°„
ENCODER_FEATURE_DIMS = {
    'uni': 1024,
    'conch': 512
}


def calculate_gene_correlations(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
    """è®¡ç®—åŸºå› çº§åˆ«çš„ç›¸å…³ç³»æ•°"""
    num_genes = y_true.shape[1]
    correlations = np.zeros(num_genes)
    
    for i in range(num_genes):
        true_gene = y_true[:, i]
        pred_gene = y_pred[:, i]
        
        # å¤„ç†å¸¸æ•°å€¼
        if np.std(true_gene) == 0 or np.std(pred_gene) == 0:
            correlations[i] = 0.0
        else:
            corr = np.corrcoef(true_gene, pred_gene)[0, 1]
            correlations[i] = 0.0 if np.isnan(corr) else corr
    
    return correlations


def calculate_evaluation_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
    """è®¡ç®—ç»¼åˆè¯„ä¼°æŒ‡æ ‡"""
    # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„
    if torch.is_tensor(y_true):
        y_true = y_true.cpu().numpy()
    if torch.is_tensor(y_pred):
        y_pred = y_pred.cpu().numpy()
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šåº”ç”¨log2(x+1)å˜æ¢ç”¨äºè¯„ä¼°æŒ‡æ ‡è®¡ç®—
    # è¿™ä¸è®­ç»ƒæ—¶çš„è¯„ä¼°ä¿æŒä¸€è‡´
    logger.info("ğŸ“Š åº”ç”¨log2(x+1)å˜æ¢ç”¨äºæŒ‡æ ‡è®¡ç®—...")
    y_true_log2 = np.log2(y_true + 1.0)
    y_pred_log2 = np.log2(y_pred + 1.0)
    
    # æ£€æŸ¥NaNå€¼
    if np.isnan(y_true_log2).any() or np.isnan(y_pred_log2).any():
        logger.warning("âš ï¸ Log2å˜æ¢åå‘ç°NaNå€¼ï¼Œå°†ä½¿ç”¨åŸå§‹å€¼")
        y_true_log2 = y_true
        y_pred_log2 = y_pred
    
    # è®¡ç®—åŸºå› ç›¸å…³æ€§ï¼ˆä½¿ç”¨log2å˜æ¢åçš„å€¼ï¼‰
    correlations = calculate_gene_correlations(y_true_log2, y_pred_log2)
    
    # æ’åºç›¸å…³æ€§
    sorted_corr = np.sort(correlations)[::-1]
    
    # è®¡ç®—PCCæŒ‡æ ‡
    pcc_10 = np.mean(sorted_corr[:10]) if len(sorted_corr) >= 10 else np.mean(sorted_corr)
    pcc_50 = np.mean(sorted_corr[:50]) if len(sorted_corr) >= 50 else np.mean(sorted_corr)
    pcc_200 = np.mean(sorted_corr[:200]) if len(sorted_corr) >= 200 else np.mean(sorted_corr)
    
    # è®¡ç®—MSEå’ŒMAEï¼ˆä½¿ç”¨log2å˜æ¢åçš„å€¼ï¼‰
    mse = np.mean((y_true_log2 - y_pred_log2) ** 2)
    mae = np.mean(np.abs(y_true_log2 - y_pred_log2))
    
    # è®¡ç®—RVD (Relative Variance Difference)ï¼ˆä½¿ç”¨log2å˜æ¢åçš„å€¼ï¼‰
    pred_var = np.var(y_pred_log2, axis=0)
    true_var = np.var(y_true_log2, axis=0)
    
    valid_mask = true_var > MIN_VARIANCE_THRESHOLD
    if np.sum(valid_mask) > 0:
        rvd = np.mean(((pred_var[valid_mask] - true_var[valid_mask]) ** 2) / (true_var[valid_mask] ** 2))
    else:
        rvd = 0.0
    
    return {
        'PCC-10': float(pcc_10),
        'PCC-50': float(pcc_50), 
        'PCC-200': float(pcc_200),
        'MSE': float(mse),
        'MAE': float(mae),
        'RVD': float(rvd),
        'correlations': correlations
    }


def load_model_from_checkpoint(checkpoint_path: str, dataset_name: str, encoder_name: str = None) -> ModelInterface:
    """ä»checkpointåŠ è½½æ¨¡å‹ - ä¸¥æ ¼éªŒè¯é…ç½®ä¸€è‡´æ€§"""
    
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
    
    # è·å–æ•°æ®é›†ä¿¡æ¯
    dataset_info = DATASETS[dataset_name]
    encoder_name = encoder_name or dataset_info['recommended_encoder']
    
    logger.info(f"ğŸ”„ åŠ è½½æ¨¡å‹ä»: {checkpoint_path}")
    logger.info(f"ğŸ“Š æ•°æ®é›†: {dataset_name}")
    logger.info(f"ğŸ”§ ç¼–ç å™¨: {encoder_name}")
    
    # ä»checkpointåŠ è½½æ¨¡å‹ - ä¸¥æ ¼æ¨¡å¼ï¼Œä¸å…è®¸å‚æ•°ä¸åŒ¹é…
    model = ModelInterface.load_from_checkpoint(
        checkpoint_path=checkpoint_path,
        strict=True  # ğŸ”§ ä¸¥æ ¼æ¨¡å¼ï¼šé…ç½®å¿…é¡»å®Œå…¨åŒ¹é…
    )
    
    logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    return model


def create_config_for_inference(dataset_name: str, encoder_name: str = None):
    """ä¸ºæ¨ç†åˆ›å»ºé…ç½®"""
    from addict import Dict
    
    # è·å–æ•°æ®é›†ä¿¡æ¯
    dataset_info = DATASETS[dataset_name]
    encoder_name = encoder_name or dataset_info['recommended_encoder']
    
    # åˆ›å»ºåŸºç¡€é…ç½®
    config = Dict({
        'GENERAL': {
            'seed': 2021,
            'log_path': f'./logs/{dataset_name}/VAR_ST',
            'debug': False
        },
        'DATA': {
            'normalize': True,
            'test_dataloader': {
                'batch_size': 128,  # å•æ ·æœ¬æ¨ç†ä»¥æœ€å¤§åŒ–èŠ‚çœå†…å­˜
                'num_workers': 1,  # æœ€å°‘workeræ•°é‡
                'pin_memory': False,  # å…³é—­pin_memoryèŠ‚çœå†…å­˜
                'shuffle': False,
                'persistent_workers': False
            }
        },
        'MODEL': {
            'model_name': 'VAR_ST',
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šåˆ é™¤æ‰€æœ‰ç¡¬ç¼–ç çš„æ¨¡å‹é…ç½®
            # æ¨¡å‹é…ç½®å°†ä»checkpointä¸­è‡ªåŠ¨è¯»å–ï¼Œç¡®ä¿è®­ç»ƒå’Œæ¨ç†å®Œå…¨ä¸€è‡´
            'gene_count_mode': 'discrete_tokens',
            'max_gene_count': 200
        }
    })
    
    # è®¾ç½®æ•°æ®é›†ç›¸å…³å‚æ•°
    config.mode = 'test'
    config.expr_name = dataset_name
    config.data_path = dataset_info['path']
    config.slide_val = dataset_info['val_slides']
    config.slide_test = dataset_info['test_slides']
    config.encoder_name = encoder_name
    config.use_augmented = True
    config.expand_augmented = True
    config.gene_count_mode = 'discrete_tokens'
    config.max_gene_count = 200
    
    return config


def run_inference(model: ModelInterface, dataloader, args, device: str = 'cuda') -> Tuple[np.ndarray, np.ndarray]:
    """è¿è¡Œæ¨ç†å¹¶æ”¶é›†é¢„æµ‹ç»“æœ"""
    
    model.eval()
    model = model.to(device)
    
    all_predictions = []
    all_targets = []
    
    logger.info(f"ğŸ”® å¼€å§‹æ¨ç†ï¼Œå…± {len(dataloader)} ä¸ªæ‰¹æ¬¡")
    logger.info(f"ğŸ’¾ ä½¿ç”¨æ‰¹æ¬¡å¤§å°: {dataloader.batch_size}")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc="æ¨ç†ä¸­")):
            try:
                # æ¯ä¸ªæ‰¹æ¬¡å‰éƒ½æ¸…ç†GPUç¼“å­˜
                torch.cuda.empty_cache()
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šå…ˆå°†æ•´ä¸ªbatchç§»åŠ¨åˆ°è®¾å¤‡ï¼Œç„¶åå†é¢„å¤„ç†
                # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š
                batch_on_device = {}
                for key, value in batch.items():
                    if torch.is_tensor(value):
                        batch_on_device[key] = value.to(device)
                    else:
                        batch_on_device[key] = value
                
                # ä½¿ç”¨ModelInterfaceçš„é¢„å¤„ç†é€»è¾‘
                processed_batch = model._preprocess_inputs(batch_on_device)
                
                # ä¸¥æ ¼éªŒè¯é¢„å¤„ç†ç»“æœ
                required_keys = ['histology_features', 'spatial_coords']
                for key in required_keys:
                    if key not in processed_batch:
                        raise ValueError(f"é¢„å¤„ç†åç¼ºå°‘å¿…éœ€çš„é”®: {key}")
                
                # ç¡®ä¿é¢„å¤„ç†åçš„æ•°æ®ä¹Ÿåœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                for key in required_keys:
                    if torch.is_tensor(processed_batch[key]):
                        processed_batch[key] = processed_batch[key].to(device)
                
                # è°ƒç”¨åº•å±‚æ¨¡å‹çš„inferenceæ–¹æ³•ï¼Œæ”¯æŒé‡‡æ ·å‚æ•°
                outputs = model.model.inference(
                    histology_features=processed_batch['histology_features'],
                    spatial_coords=processed_batch['spatial_coords'],
                    temperature=args.temperature,
                    top_k=args.top_k,
                    top_p=args.top_p,
                    seed=args.seed
                )
                
                # ä¸¥æ ¼éªŒè¯è¾“å‡ºæ ¼å¼
                if not isinstance(outputs, dict):
                    raise ValueError(f"æ¨¡å‹è¾“å‡ºå¿…é¡»æ˜¯å­—å…¸æ ¼å¼ï¼Œå®é™…å¾—åˆ°: {type(outputs)}")
                
                if 'predictions' not in outputs:
                    raise ValueError(f"æ¨¡å‹è¾“å‡ºä¸­ç¼ºå°‘'predictions'é”®ï¼Œå¯ç”¨é”®: {list(outputs.keys())}")
                
                predictions = outputs['predictions']
                gene_expression = batch_on_device['target_genes']
                
                # ç«‹å³ç§»åŠ¨åˆ°CPUå¹¶æ”¶é›†ç»“æœ
                all_predictions.append(predictions.cpu())
                all_targets.append(gene_expression.cpu())
                
                # åˆ é™¤GPUä¸Šçš„ä¸´æ—¶å˜é‡
                del batch_on_device, processed_batch, predictions, gene_expression
                if 'outputs' in locals():
                    del outputs
                
            except torch.cuda.OutOfMemoryError as e:
                logger.error(f"âŒ æ‰¹æ¬¡ {batch_idx} GPUå†…å­˜ä¸è¶³: {e}")
                logger.info("ğŸ”„ æ¸…ç†GPUç¼“å­˜å¹¶è·³è¿‡æ­¤æ‰¹æ¬¡...")
                torch.cuda.empty_cache()
                continue
            except Exception as e:
                logger.error(f"âŒ æ‰¹æ¬¡ {batch_idx} æ¨ç†å¤±è´¥: {e}")
                continue
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
    all_predictions = torch.cat(all_predictions, dim=0).numpy()
    all_targets = torch.cat(all_targets, dim=0).numpy()
    
    logger.info(f"âœ… æ¨ç†å®Œæˆ")
    logger.info(f"ğŸ“Š é¢„æµ‹å½¢çŠ¶: {all_predictions.shape}")
    logger.info(f"ğŸ“Š ç›®æ ‡å½¢çŠ¶: {all_targets.shape}")
    
    return all_targets, all_predictions


def main():
    parser = argparse.ArgumentParser(description='VAR_STæ¨¡å‹æ¨ç†è„šæœ¬')
    parser.add_argument('--checkpoint', type=str, required=True,
                        help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--dataset', type=str, choices=list(DATASETS.keys()), required=True,
                        help='æ•°æ®é›†åç§°')
    parser.add_argument('--encoder', type=str, choices=list(ENCODER_FEATURE_DIMS.keys()),
                        help='ç¼–ç å™¨ç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨æ•°æ®é›†æ¨èç¼–ç å™¨')
    parser.add_argument('--device', type=str, default='cuda',
                        help='æ¨ç†è®¾å¤‡ (é»˜è®¤: cuda)')
    parser.add_argument('--output', type=str, default='inference_results.txt',
                        help='ç»“æœè¾“å‡ºæ–‡ä»¶ (é»˜è®¤: inference_results.txt)')
    
    # ğŸ”§ æ–°å¢ï¼šé‡‡æ ·å‚æ•°æ§åˆ¶
    parser.add_argument('--temperature', type=float, default=1.0,
                        help='é‡‡æ ·æ¸©åº¦ (é»˜è®¤: 1.0)')
    parser.add_argument('--top_k', type=int, default=None,
                        help='Top-ké‡‡æ ·å‚æ•° (é»˜è®¤: None)')
    parser.add_argument('--top_p', type=float, default=None,
                        help='Top-pé‡‡æ ·å‚æ•° (é»˜è®¤: None)')
    parser.add_argument('--seed', type=int, default=None,
                        help='éšæœºç§å­ï¼Œç”¨äºå¯é‡ç°ç»“æœ (é»˜è®¤: None)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥checkpointæ–‡ä»¶
    if not os.path.exists(args.checkpoint):
        logger.error(f"âŒ Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {args.checkpoint}")
        return
    
    # åˆ›å»ºæ¨ç†é…ç½®
    config = create_config_for_inference(args.dataset, args.encoder)
    
    # åŠ è½½æ¨¡å‹
    try:
        model = load_model_from_checkpoint(args.checkpoint, args.dataset, args.encoder)
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    logger.info("ğŸ“‚ å‡†å¤‡æµ‹è¯•æ•°æ®...")
    dataset = DataInterface(config)
    dataset.setup('test')
    test_dataloader = dataset.test_dataloader()
    
    logger.info(f"ğŸ“Š æµ‹è¯•æ•°æ®é›†å¤§å°: {len(test_dataloader.dataset)}")
    
    # ğŸ”§ æ–°å¢ï¼šè¾“å‡ºé‡‡æ ·å‚æ•°é…ç½®
    logger.info("ğŸ¯ é‡‡æ ·å‚æ•°é…ç½®:")
    logger.info(f"  - Temperature: {args.temperature}")
    logger.info(f"  - Top-k: {args.top_k}")
    logger.info(f"  - Top-p: {args.top_p}")
    logger.info(f"  - Seed: {args.seed}")
    
    # è¿è¡Œæ¨ç†
    try:
        y_true, y_pred = run_inference(model, test_dataloader, args, args.device)
    except Exception as e:
        logger.error(f"âŒ æ¨ç†å¤±è´¥: {e}")
        return
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    logger.info("ğŸ“ˆ è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    metrics = calculate_evaluation_metrics(y_true, y_pred)
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*60)
    print("ğŸ¯ VAR_STæ¨¡å‹æ¨ç†ç»“æœ")
    print("="*60)
    print(f"ğŸ“ Checkpoint: {args.checkpoint}")
    print(f"ğŸ“Š æ•°æ®é›†: {args.dataset}")
    print(f"ğŸ”§ ç¼–ç å™¨: {config.encoder_name}")
    print(f"ğŸ“ æµ‹è¯•æ ·æœ¬æ•°: {y_true.shape[0]}")
    print(f"ğŸ§¬ åŸºå› æ•°é‡: {y_true.shape[1]}")
    print("-"*60)
    print("ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡:")
    print(f"   PCC-10:  {metrics['PCC-10']:.4f}")
    print(f"   PCC-50:  {metrics['PCC-50']:.4f}")
    print(f"   PCC-200: {metrics['PCC-200']:.4f}")
    print(f"   MSE:     {metrics['MSE']:.6f}")
    print(f"   MAE:     {metrics['MAE']:.6f}")
    print(f"   RVD:     {metrics['RVD']:.6f}")
    print("="*60)
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    with open(args.output, 'w') as f:
        f.write("VAR_STæ¨¡å‹æ¨ç†ç»“æœ\n")
        f.write("="*60 + "\n")
        f.write(f"Checkpoint: {args.checkpoint}\n")
        f.write(f"æ•°æ®é›†: {args.dataset}\n")
        f.write(f"ç¼–ç å™¨: {config.encoder_name}\n")
        f.write(f"æµ‹è¯•æ ·æœ¬æ•°: {y_true.shape[0]}\n")
        f.write(f"åŸºå› æ•°é‡: {y_true.shape[1]}\n")
        f.write("-"*60 + "\n")
        f.write("è¯„ä¼°æŒ‡æ ‡:\n")
        f.write(f"PCC-10:  {metrics['PCC-10']:.4f}\n")
        f.write(f"PCC-50:  {metrics['PCC-50']:.4f}\n")
        f.write(f"PCC-200: {metrics['PCC-200']:.4f}\n")
        f.write(f"MSE:     {metrics['MSE']:.6f}\n")
        f.write(f"MAE:     {metrics['MAE']:.6f}\n")
        f.write(f"RVD:     {metrics['RVD']:.6f}\n")
        
        # ä¿å­˜åŸºå› çº§åˆ«çš„ç›¸å…³æ€§
        f.write("\nåŸºå› çº§åˆ«ç›¸å…³æ€§ç»Ÿè®¡:\n")
        correlations = metrics['correlations']
        f.write(f"å¹³å‡ç›¸å…³æ€§: {np.mean(correlations):.4f}\n")
        f.write(f"ä¸­ä½æ•°ç›¸å…³æ€§: {np.median(correlations):.4f}\n")
        f.write(f"æ ‡å‡†å·®: {np.std(correlations):.4f}\n")
        f.write(f"æœ€å¤§ç›¸å…³æ€§: {np.max(correlations):.4f}\n")
        f.write(f"æœ€å°ç›¸å…³æ€§: {np.min(correlations):.4f}\n")
        
        # ç›¸å…³æ€§åˆ†å¸ƒ
        high_corr = np.sum(correlations > 0.5)
        medium_corr = np.sum((correlations > 0.3) & (correlations <= 0.5))
        low_corr = np.sum(correlations <= 0.3)
        
        f.write(f"\nç›¸å…³æ€§åˆ†å¸ƒ:\n")
        f.write(f"é«˜ç›¸å…³æ€§ (>0.5): {high_corr} ä¸ªåŸºå›  ({high_corr/len(correlations)*100:.1f}%)\n")
        f.write(f"ä¸­ç­‰ç›¸å…³æ€§ (0.3-0.5): {medium_corr} ä¸ªåŸºå›  ({medium_corr/len(correlations)*100:.1f}%)\n")
        f.write(f"ä½ç›¸å…³æ€§ (â‰¤0.3): {low_corr} ä¸ªåŸºå›  ({low_corr/len(correlations)*100:.1f}%)\n")
    
    logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")


if __name__ == '__main__':
    main() 